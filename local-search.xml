<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Hive常用函数（二）</title>
    <link href="/2020/08/21/Hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2020/08/21/Hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="case-when-窗口函数-时间函数-列转行"><a href="#case-when-窗口函数-时间函数-列转行" class="headerlink" title="case when/ 窗口函数/时间函数/列转行"></a>case when/ 窗口函数/时间函数/列转行</h2><h3 id="1-case-when"><a href="#1-case-when" class="headerlink" title="1. case when"></a>1. case when</h3><p><strong>CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END</strong></p><ul><li>When a = true, returns b; when c = true, returns d; else returns e.</li><li>类似java中的switch case语句和scala中的模式匹配</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age &lt; <span class="hljs-number">18</span> <span class="hljs-keyword">then</span> <span class="hljs-string">"少年"</span> <span class="hljs-keyword">when</span> age &lt; <span class="hljs-number">60</span> <span class="hljs-keyword">then</span> <span class="hljs-string">"成年人"</span>  <span class="hljs-keyword">else</span> <span class="hljs-string">"老年人"</span> <span class="hljs-keyword">end</span> <span class="hljs-keyword">from</span> student;</code></pre><h3 id="2-row-number"><a href="#2-row-number" class="headerlink" title="2. row_number"></a>2. row_number</h3><p><strong>排序窗口函数：</strong></p><ul><li>根据具体的分组和排序，为每行数据生成一个起始值等于1的唯一序列数。</li><li>常用于分组取TopN需求</li></ul><h3 id="3-rank"><a href="#3-rank" class="headerlink" title="3. rank"></a>3. rank</h3><p><strong>排序窗口函数：</strong></p><ul><li>对组中的数据进行排名，如果名次相同，则排名也相同，但是下一个名次的排名序号会出现不连续，序号总数不会变。</li></ul><h3 id="4-dense-rank"><a href="#4-dense-rank" class="headerlink" title="4. dense_rank"></a>4. dense_rank</h3><p><strong>排序窗口函数：</strong></p><ul><li>dense_rank与rank类似，dense_rank在生成序号时是连续的，而rank生成的序号有可能不连续。当出现名次相同时，则排名序号也相同。下一个排名的序号与上一个排名序号是连续的，所以序号总数可能会减少。</li></ul><p><strong>排序窗口函数示例：</strong></p><p><strong>建表：</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> score(<span class="hljs-keyword">name</span> <span class="hljs-keyword">string</span>,score <span class="hljs-built_in">int</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">"\t"</span>;</code></pre><p><strong>插入数据：</strong></p><pre><code class="hljs crystal">insert into scoreselect <span class="hljs-string">"Michael"</span>,<span class="hljs-string">"87"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Will</span>","95"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Wendy</span>","64"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Steven</span>","94"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Lucy</span>","56"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Lily</span>","84"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Jess</span>","64"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Mike</span>","87"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Wei</span>","84"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Yun</span>","60"</span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">all</span> <span class="hljs-title">select</span> "<span class="hljs-title">Richard</span>","87";</span></code></pre><p><strong>按照成绩排名：</strong></p><pre><code class="hljs pgsql"><span class="hljs-keyword">select</span> <span class="hljs-type">name</span>,score,rank() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score <span class="hljs-keyword">desc</span>) rp,dense_rank() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score <span class="hljs-keyword">desc</span>) drp,row_number() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score <span class="hljs-keyword">desc</span>) rmp <span class="hljs-keyword">from</span> score;</code></pre><p><strong>查询结果：</strong></p><pre><code class="hljs angelscript">name    score   rp   drp   rmp  Will      <span class="hljs-number">95</span>      <span class="hljs-number">1</span>    <span class="hljs-number">1</span>     <span class="hljs-number">1</span>    Steven    <span class="hljs-number">94</span>      <span class="hljs-number">2</span>    <span class="hljs-number">2</span>     <span class="hljs-number">2</span>    Richard   <span class="hljs-number">87</span>      <span class="hljs-number">3</span>    <span class="hljs-number">3</span>     <span class="hljs-number">3</span>    Mike      <span class="hljs-number">87</span>      <span class="hljs-number">3</span>    <span class="hljs-number">3</span>     <span class="hljs-number">4</span>    Michael   <span class="hljs-number">87</span>      <span class="hljs-number">3</span>    <span class="hljs-number">3</span>     <span class="hljs-number">5</span>    Wei       <span class="hljs-number">84</span>      <span class="hljs-number">6</span>    <span class="hljs-number">4</span>     <span class="hljs-number">6</span>    Lily      <span class="hljs-number">84</span>      <span class="hljs-number">6</span>    <span class="hljs-number">4</span>     <span class="hljs-number">7</span>    Jess      <span class="hljs-number">64</span>      <span class="hljs-number">8</span>    <span class="hljs-number">5</span>     <span class="hljs-number">8</span>    Wendy     <span class="hljs-number">64</span>      <span class="hljs-number">8</span>    <span class="hljs-number">5</span>     <span class="hljs-number">9</span>    Yun       <span class="hljs-number">60</span>      <span class="hljs-number">10</span>   <span class="hljs-number">6</span>     <span class="hljs-number">10</span>   Lucy      <span class="hljs-number">56</span>      <span class="hljs-number">11</span>   <span class="hljs-number">7</span>     <span class="hljs-number">11</span></code></pre><h3 id="6-date-format"><a href="#6-date-format" class="headerlink" title="6.date_format"></a>6.date_format</h3><p><strong>date_format(date/timestamp/string ts, string fmt)</strong></p><ul><li>将日期/时间戳/字符串转换为fmt指定格式的字符串值，第二个参数fmt应该是常量。</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">date_format</span>(<span class="hljs-string">'2020-08-21'</span>,<span class="hljs-string">'yyyy-MM'</span>);<span class="hljs-comment">--2020-08</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">date_format</span>(<span class="hljs-string">'2020-08-21'</span>,<span class="hljs-string">'MM'</span>);<span class="hljs-comment">--08</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">date_format</span>(<span class="hljs-string">'2020-08-21'</span>,<span class="hljs-string">'M'</span>);<span class="hljs-comment">--8</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">date_format</span>(<span class="hljs-string">'2020-11-21'</span>,<span class="hljs-string">'M'</span>);<span class="hljs-comment">--11</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">date_format</span>(<span class="hljs-string">'2020-08-21'</span>,<span class="hljs-string">'y'</span>);<span class="hljs-comment">--2020</span></code></pre><h3 id="7-current-date"><a href="#7-current-date" class="headerlink" title="7.current_date"></a>7.current_date</h3><ul><li>返回当前日期，格式yyyy-MM-dd, 使用时候带不带括号都可以。</li><li>同一查询中对current_date的所有调用均返回相同的值。</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">current_date</span>();  <span class="hljs-comment">-- 2020-08-21</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">current_date</span>;<span class="hljs-comment">-- 2020-08-21</span></code></pre><h3 id="8-current-timestamp"><a href="#8-current-timestamp" class="headerlink" title="8.current_timestamp"></a>8.current_timestamp</h3><ul><li>返回当前时刻，格式yyyy-MM-dd HH:mm:ss,使用时候带不带括号都可以。</li><li>同一查询中对current_timestamp的所有调用均返回相同的值。</li></ul><pre><code class="hljs angelscript">select current_timestamp(); -<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-21</span> <span class="hljs-number">14</span>:<span class="hljs-number">31</span>:<span class="hljs-number">29.438</span>select current_timestamp;-<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-21</span> <span class="hljs-number">14</span>:<span class="hljs-number">31</span>:<span class="hljs-number">34.354</span></code></pre><h3 id="9-date-add"><a href="#9-date-add" class="headerlink" title="9.date_add"></a>9.date_add</h3><p><strong>date_add(date/timestamp/string startdate, tinyint/smallint/int days)</strong></p><ul><li>加减startdate的天数</li></ul><pre><code class="hljs lsl">select date_add('<span class="hljs-number">2020</span><span class="hljs-number">-08</span><span class="hljs-number">-21</span>',<span class="hljs-number">1</span>);-<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-22</span>select date_add('<span class="hljs-number">2020</span><span class="hljs-number">-08</span><span class="hljs-number">-21</span>',<span class="hljs-number">-1</span>);-<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-20</span>select date_add(current_date,<span class="hljs-number">1</span>);-<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-22</span>select date_add(current_date,<span class="hljs-number">-1</span>);-<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-20</span></code></pre><h3 id="10-next-day"><a href="#10-next-day" class="headerlink" title="10.next_day"></a>10.next_day</h3><p><strong>next_day(string start_date, string day_of_week)</strong></p><ul><li>返回start_date下周日期中的day_of_week代表的当天日期。</li><li>start_date是字符串/日期/时间戳。day_of_week是2个字母，3个字母或一周中某天的全名。</li></ul><pre><code class="hljs lsl">select next_day('<span class="hljs-number">2020</span><span class="hljs-number">-08</span><span class="hljs-number">-21</span>','mo');-<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-24</span>select next_day('<span class="hljs-number">2020</span><span class="hljs-number">-08</span><span class="hljs-number">-21</span>','Tuesday');-<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-25</span></code></pre><h3 id="11-last-day"><a href="#11-last-day" class="headerlink" title="11. last_day"></a>11. last_day</h3><p><strong>last_day(string date)</strong></p><ul><li>返回date所属月份的最后一天。</li><li>date格式为“ yyyy-MM-dd HH：mm：ss”或“ yyyy-MM-dd”的字符串。</li></ul><pre><code class="hljs lsl">select last_day('<span class="hljs-number">2020</span><span class="hljs-number">-08</span><span class="hljs-number">-21</span>');-<span class="hljs-number">-2020</span><span class="hljs-number">-08</span><span class="hljs-number">-31</span>select last_day('<span class="hljs-number">2020</span><span class="hljs-number">-09</span><span class="hljs-number">-21</span>');-<span class="hljs-number">-2020</span><span class="hljs-number">-09</span><span class="hljs-number">-30</span></code></pre><h3 id="12-year"><a href="#12-year" class="headerlink" title="12. year"></a>12. year</h3><p><strong>year(string date)</strong></p><ul><li>返回日期或时间戳的年份部分</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">year</span>(<span class="hljs-string">'2020-08-21'</span>);<span class="hljs-comment">--2020</span></code></pre><h3 id="13-month"><a href="#13-month" class="headerlink" title="13. month"></a>13. month</h3><p><strong>month(string date)</strong></p><ul><li>返回日期或时间戳的月份部分。</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">month</span>(<span class="hljs-string">'2020-08-21'</span>);<span class="hljs-comment">--8</span></code></pre><h3 id="14-day"><a href="#14-day" class="headerlink" title="14. day"></a>14. day</h3><p><strong>day(string date)</strong></p><ul><li>返回日期或时间戳的日部分。</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">day</span>(<span class="hljs-string">'2020-08-21'</span>);<span class="hljs-comment">--21</span></code></pre><h3 id="15-列转行"><a href="#15-列转行" class="headerlink" title="15. 列转行"></a>15. 列转行</h3><p><strong>EXPLODE(col)</strong></p><ul><li>将hive一列中复杂的array或者map结构拆分成多行。</li></ul><p><strong>LATERAL VIEW</strong></p><ul><li>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li><li>用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">select</span> tf.*  <span class="hljs-keyword">from</span> ( <span class="hljs-keyword">select</span> <span class="hljs-number">0</span>) t <span class="hljs-keyword">lateral</span> <span class="hljs-keyword">view</span> <span class="hljs-keyword">explode</span>(<span class="hljs-built_in">array</span>( <span class="hljs-string">'A'</span> , <span class="hljs-string">'B'</span> , <span class="hljs-string">'C'</span> )) tf  <span class="hljs-keyword">as</span>   <span class="hljs-keyword">col</span>;<span class="hljs-comment">--查询结果：</span><span class="hljs-comment">--col</span><span class="hljs-comment">--A    </span><span class="hljs-comment">--B    </span><span class="hljs-comment">--C</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>函数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive SQL练习</title>
    <link href="/2020/08/20/Hive-SQL%E7%BB%83%E4%B9%A0/"/>
    <url>/2020/08/20/Hive-SQL%E7%BB%83%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-窗口函数——练习一"><a href="#1-窗口函数——练习一" class="headerlink" title="1. 窗口函数——练习一"></a>1. 窗口函数——练习一</h2><pre><code class="hljs mysql">Function (arg1,..., argn) OVER ([PARTITION BY &lt;...&gt;] [ORDER BY &lt;....&gt;][&lt;window_expression&gt;])</code></pre><p>Function (arg1,…, argn) 可以是下面的函数：</p><ul><li>Aggregate Functions: 聚合函数,比如：sum(…)、 max(…)、min(…)、avg(…)等.</li><li>Sort Functions: 数据排序函数, 比如 ：rank(…)、row_number(…)等.</li><li>Analytics Functions: 统计和比较函数, 比如：lead(…)、lag(…)、 first_value(…)等.</li></ul><h3 id="1-1-数据准备"><a href="#1-1-数据准备" class="headerlink" title="1.1 数据准备"></a>1.1 数据准备</h3><h4 id="1-1-1-样例数据"><a href="#1-1-1-样例数据" class="headerlink" title="1.1.1 样例数据"></a>1.1.1 样例数据</h4><pre><code class="hljs angelscript">[职工姓名|部门编号|职工ID|工资|岗位类型|入职时间]Michael|<span class="hljs-number">1000</span>|<span class="hljs-number">100</span>|<span class="hljs-number">5000</span>|full|<span class="hljs-number">2014</span><span class="hljs-number">-01</span><span class="hljs-number">-29</span>Will|<span class="hljs-number">1000</span>|<span class="hljs-number">101</span>|<span class="hljs-number">4000</span>|full|<span class="hljs-number">2013</span><span class="hljs-number">-10</span><span class="hljs-number">-02</span>Wendy|<span class="hljs-number">1000</span>|<span class="hljs-number">101</span>|<span class="hljs-number">4000</span>|part|<span class="hljs-number">2014</span><span class="hljs-number">-10</span><span class="hljs-number">-02</span>Steven|<span class="hljs-number">1000</span>|<span class="hljs-number">102</span>|<span class="hljs-number">6400</span>|part|<span class="hljs-number">2012</span><span class="hljs-number">-11</span><span class="hljs-number">-03</span>Lucy|<span class="hljs-number">1000</span>|<span class="hljs-number">103</span>|<span class="hljs-number">5500</span>|full|<span class="hljs-number">2010</span><span class="hljs-number">-01</span><span class="hljs-number">-03</span>Lily|<span class="hljs-number">1001</span>|<span class="hljs-number">104</span>|<span class="hljs-number">5000</span>|part|<span class="hljs-number">2014</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span>Jess|<span class="hljs-number">1001</span>|<span class="hljs-number">105</span>|<span class="hljs-number">6000</span>|part|<span class="hljs-number">2014</span><span class="hljs-number">-12</span><span class="hljs-number">-02</span>Mike|<span class="hljs-number">1001</span>|<span class="hljs-number">106</span>|<span class="hljs-number">6400</span>|part|<span class="hljs-number">2013</span><span class="hljs-number">-11</span><span class="hljs-number">-03</span>Wei|<span class="hljs-number">1002</span>|<span class="hljs-number">107</span>|<span class="hljs-number">7000</span>|part|<span class="hljs-number">2010</span><span class="hljs-number">-04</span><span class="hljs-number">-03</span>Yun|<span class="hljs-number">1002</span>|<span class="hljs-number">108</span>|<span class="hljs-number">5500</span>|full|<span class="hljs-number">2014</span><span class="hljs-number">-01</span><span class="hljs-number">-29</span>Richard|<span class="hljs-number">1002</span>|<span class="hljs-number">109</span>|<span class="hljs-number">8000</span>|full|<span class="hljs-number">2013</span><span class="hljs-number">-09</span><span class="hljs-number">-01</span></code></pre><h4 id="1-1-2-建表语句"><a href="#1-1-2-建表语句" class="headerlink" title="1.1.2 建表语句"></a>1.1.2 建表语句</h4><pre><code class="hljs mysql">CREATE TABLE IF NOT EXISTS employee (                name string,                dept_num int,                employee_id int,                salary int,                type string,                start_date date)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &#39;|&#39;STORED as TEXTFILE;</code></pre><h4 id="1-1-3-导入数据"><a href="#1-1-3-导入数据" class="headerlink" title="1.1.3 导入数据"></a>1.1.3 导入数据</h4><pre><code class="hljs mysql">load data local inpath &#39;&#x2F;opt&#x2F;datas&#x2F;data&#x2F;employee_contract.txt&#39; into table employee;</code></pre><h3 id="1-2-窗口聚合函数"><a href="#1-2-窗口聚合函数" class="headerlink" title="1.2 窗口聚合函数"></a>1.2 窗口聚合函数</h3><h4 id="1-2-1-查询姓名、部门编号、工资以及部门人数"><a href="#1-2-1-查询姓名、部门编号、工资以及部门人数" class="headerlink" title="1.2.1 查询姓名、部门编号、工资以及部门人数"></a>1.2.1 查询姓名、部门编号、工资以及部门人数</h4><pre><code class="hljs angelscript">select     name,    dept_num as deptno ,    salary,    count(*) over (partition by dept_num) as cnt <span class="hljs-keyword">from</span> employee ;查询结果：name    deptno  salary  cntLucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">5</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">5</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">5</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">5</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">5</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">3</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">3</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">3</span>Richard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">3</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">3</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">3</span></code></pre><h4 id="1-2-2-查询姓名、部门编号、工资以及每个部门的总工资，部门总工资按照降序输出"><a href="#1-2-2-查询姓名、部门编号、工资以及每个部门的总工资，部门总工资按照降序输出" class="headerlink" title="1.2.2 查询姓名、部门编号、工资以及每个部门的总工资，部门总工资按照降序输出"></a>1.2.2 查询姓名、部门编号、工资以及每个部门的总工资，部门总工资按照降序输出</h4><pre><code class="hljs angelscript">select     name ,    dept_num as deptno,    salary,    sum(salary) over (partition by dept_num order by dept_num) as sum_dept_salary <span class="hljs-keyword">from</span> employee order by sum_dept_salary desc;name    deptno  salary  sum_dept_salaryMichael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">24900</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">24900</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">24900</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">24900</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">24900</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">20500</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">20500</span>Richard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">20500</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">17400</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">17400</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">17400</span></code></pre><h3 id="1-3-窗口排序函数"><a href="#1-3-窗口排序函数" class="headerlink" title="1.3 窗口排序函数"></a>1.3 窗口排序函数</h3><p>窗口排序函数提供了数据的排序信息，比如行号和排名。在一个分组的内部将行号或者排名作为数据的一部分进行返回，最常用的排序函数主要包括：</p><ul><li><strong>row_number</strong></li></ul><p>根据具体的分组和排序，为每行数据生成一个起始值等于1的唯一序列数</p><ul><li><strong>rank</strong></li></ul><p>对组中的数据进行排名，如果名次相同，则排名也相同，但是下一个名次的排名序号会出现不连续。比如查找具体条件的topN行</p><ul><li><strong>dense_rank</strong></li></ul><p>dense_rank函数的功能与rank函数类似，dense_rank函数在生成序号时是连续的，而rank函数生成的序号有可能不连续。当出现名次相同时，则排名序号也相同。而下一个排名的序号与上一个排名序号是连续的。</p><ul><li><strong>percent_rank</strong></li></ul><p>排名计算公式为：(current rank - 1)/(total number of rows - 1)</p><ul><li><strong>ntile</strong></li></ul><p>将一个有序的数据集划分为多个桶(bucket)，并为每行分配一个适当的桶数。它可用于将数据划分为相等的小切片，为每一行分配该小切片的数字序号。</p><h4 id="1-3-1-查询姓名、部门编号、工资、排名编号-按工资的多少排名"><a href="#1-3-1-查询姓名、部门编号、工资、排名编号-按工资的多少排名" class="headerlink" title="1.3.1 查询姓名、部门编号、工资、排名编号(按工资的多少排名)"></a>1.3.1 查询姓名、部门编号、工资、排名编号(按工资的多少排名)</h4><pre><code class="hljs angelscript">select    name ,   dept_num as dept_no ,   salary,   row_number() over (order by salary desc ) rnum <span class="hljs-keyword">from</span> employee;name    dept_no salary  rnumRichard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">1</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">2</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">3</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">4</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">5</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">6</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">7</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">8</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">9</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">10</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">11</span></code></pre><h4 id="1-3-2-查询每个部门工资最高的两个人的信息-姓名、部门、薪水"><a href="#1-3-2-查询每个部门工资最高的两个人的信息-姓名、部门、薪水" class="headerlink" title="1.3.2 查询每个部门工资最高的两个人的信息(姓名、部门、薪水)"></a>1.3.2 查询每个部门工资最高的两个人的信息(姓名、部门、薪水)</h4><pre><code class="hljs angelscript">select    name,   dept_num,   salary <span class="hljs-keyword">from</span>( select name ,   dept_num ,   salary,   row_number() over (partition by dept_num order by salary desc ) rnum  <span class="hljs-keyword">from</span> employee) t1 where rnum &lt;= <span class="hljs-number">2</span>;  name    dept_num        salarySteven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>Richard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span></code></pre><h4 id="1-3-3-查询每个部门的员工工资排名信息"><a href="#1-3-3-查询每个部门的员工工资排名信息" class="headerlink" title="1.3.3 查询每个部门的员工工资排名信息"></a>1.3.3 查询每个部门的员工工资排名信息</h4><pre><code class="hljs angelscript">select name , dept_num as dept_no , salary,row_number() over (partition by dept_num order by salary desc ) rnum <span class="hljs-keyword">from</span> employee;name    dept_no salary  rnumSteven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">1</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">2</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">3</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">4</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">5</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">1</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">2</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">3</span>Richard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">1</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">2</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">3</span></code></pre><h4 id="1-3-4-使用rank函数进行排名"><a href="#1-3-4-使用rank函数进行排名" class="headerlink" title="1.3.4 使用rank函数进行排名"></a>1.3.4 使用rank函数进行排名</h4><pre><code class="hljs angelscript">select  name,  dept_num,  salary,  rank() over (order by salary desc) rank<span class="hljs-keyword">from</span> employee;name    dept_num        salary  rankRichard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">1</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">2</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">3</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">3</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">5</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">6</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">6</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">8</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">8</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">10</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">10</span></code></pre><h4 id="1-3-4-使用dense-rank进行排名"><a href="#1-3-4-使用dense-rank进行排名" class="headerlink" title="1.3.4 使用dense_rank进行排名"></a>1.3.4 使用dense_rank进行排名</h4><pre><code class="hljs angelscript">select  name,  dept_num,  salary,  dense_rank() over (order by salary desc) rank<span class="hljs-keyword">from</span> employee;name    dept_num        salary  rankRichard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">1</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">2</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">3</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">3</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">4</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">5</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">5</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">6</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">6</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">7</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">7</span></code></pre><h4 id="1-3-5-使用percent-rank-进行排名"><a href="#1-3-5-使用percent-rank-进行排名" class="headerlink" title="1.3.5 使用percent_rank()进行排名"></a>1.3.5 使用percent_rank()进行排名</h4><pre><code class="hljs angelscript">select  name,  dept_num,  salary,  percent_rank() over (order by salary desc) rank<span class="hljs-keyword">from</span> employee;name    dept_num        salary  rankRichard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">0.0</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">0.1</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">0.2</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">0.2</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">0.4</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">0.5</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">0.5</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">0.7</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">0.7</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">0.9</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">0.9</span></code></pre><h4 id="1-3-6-使用ntile进行数据分片排名"><a href="#1-3-6-使用ntile进行数据分片排名" class="headerlink" title="1.3.6 使用ntile进行数据分片排名"></a>1.3.6 使用ntile进行数据分片排名</h4><pre><code class="hljs angelscript">SELECTname,dept_num as deptno,salary,ntile(<span class="hljs-number">4</span>) OVER(ORDER BY salary desc) as ntileFROM employee;name    deptno  salary  ntileRichard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">1</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">1</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">1</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">2</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">2</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">2</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">3</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">3</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">3</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">4</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">4</span></code></pre><p><strong>尖叫提示</strong>：从 Hive v2.1.0开始, 支持在OVER语句里使用聚集函数，比如：</p><pre><code class="hljs n1ql"><span class="hljs-keyword">SELECT</span>  dept_num,  row_number() <span class="hljs-keyword">OVER</span> (<span class="hljs-keyword">PARTITION</span> <span class="hljs-keyword">BY</span> dept_num <span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> <span class="hljs-built_in">sum</span>(salary)) <span class="hljs-keyword">as</span> rk<span class="hljs-keyword">FROM</span> employee<span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> dept_num;dept_num        rk1000    11001    11002    1</code></pre><h3 id="1-4-窗口分析函数"><a href="#1-4-窗口分析函数" class="headerlink" title="1.4 窗口分析函数"></a>1.4 窗口分析函数</h3><p>常用的分析函数主要包括：</p><ul><li><strong>cume_dist</strong></li></ul><p>如果按升序排列，则统计：小于等于当前值的行数/总行数(number of rows ≤ current row)/(total number of  rows）。如果是降序排列，则统计：大于等于当前值的行数/总行数。比如，统计小于等于当前工资的人数占总人数的比例 ，用于累计统计.</p><ul><li><strong>lead(value_expr[,offset[,default]])</strong></li></ul><p>用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL.</p><ul><li><strong>lag(value_expr[,offset[,default]])</strong></li></ul><p>与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL.</p><ul><li><strong>first_value</strong></li></ul><p>取分组内排序后，截止到当前行，第一个值</p><ul><li><strong>last_value</strong></li></ul><p>取分组内排序后，截止到当前行，最后一个值</p><h4 id="1-4-1-统计小于等于当前工资的人数占总人数的比例"><a href="#1-4-1-统计小于等于当前工资的人数占总人数的比例" class="headerlink" title="1.4.1 统计小于等于当前工资的人数占总人数的比例"></a>1.4.1 统计小于等于当前工资的人数占总人数的比例</h4><pre><code class="hljs angelscript">SELECT name, dept_num as deptno, salary, cume_dist() OVER (ORDER BY salary) as cumeFROM employee;name    deptno  salary  cumeWendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">0.18181818181818182</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">0.18181818181818182</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">0.36363636363636365</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">0.36363636363636365</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">0.5454545454545454</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">0.5454545454545454</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">0.6363636363636364</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">0.8181818181818182</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">0.8181818181818182</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">0.9090909090909091</span>Richard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">1.0</span></code></pre><h4 id="1-4-2-统计大于等于当前工资的人数占总人数的比例"><a href="#1-4-2-统计大于等于当前工资的人数占总人数的比例" class="headerlink" title="1.4.2 统计大于等于当前工资的人数占总人数的比例"></a>1.4.2 统计大于等于当前工资的人数占总人数的比例</h4><pre><code class="hljs angelscript">SELECT name, dept_num as deptno, salary, cume_dist() OVER (ORDER BY salary desc) as cumeFROM employee;name    deptno  salary  cumeRichard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">0.09090909090909091</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">0.18181818181818182</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">0.36363636363636365</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">0.36363636363636365</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">0.45454545454545453</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">0.6363636363636364</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">0.6363636363636364</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">0.8181818181818182</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">0.8181818181818182</span>Wendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">1.0</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">1.0</span></code></pre><h4 id="1-4-3-按照部门统计小于等于当前工资的人数占部门总人数的比例"><a href="#1-4-3-按照部门统计小于等于当前工资的人数占部门总人数的比例" class="headerlink" title="1.4.3 按照部门统计小于等于当前工资的人数占部门总人数的比例"></a>1.4.3 按照部门统计小于等于当前工资的人数占部门总人数的比例</h4><pre><code class="hljs angelscript">SELECT name, dept_num as deptno, salary, cume_dist() OVER (PARTITION BY dept_num ORDER BY salary) as cumeFROM employee;name    deptno  salary  cumeWendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">0.4</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">0.4</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">0.6</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">0.8</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">1.0</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">0.3333333333333333</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">0.6666666666666666</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">1.0</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">0.3333333333333333</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">0.6666666666666666</span>Richard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">1.0</span></code></pre><h4 id="1-4-4-按部门分组，统计每个部门员工的工资以及大于等于该员工工资的下一个员工的工资"><a href="#1-4-4-按部门分组，统计每个部门员工的工资以及大于等于该员工工资的下一个员工的工资" class="headerlink" title="1.4.4 按部门分组，统计每个部门员工的工资以及大于等于该员工工资的下一个员工的工资"></a>1.4.4 按部门分组，统计每个部门员工的工资以及大于等于该员工工资的下一个员工的工资</h4><pre><code class="hljs yaml"><span class="hljs-string">SELECT</span> <span class="hljs-string">name,</span> <span class="hljs-string">dept_num</span> <span class="hljs-string">as</span> <span class="hljs-string">deptno,</span> <span class="hljs-string">salary,</span> <span class="hljs-string">lead(salary,1)</span> <span class="hljs-string">OVER</span> <span class="hljs-string">(PARTITION</span> <span class="hljs-string">BY</span> <span class="hljs-string">dept_num</span> <span class="hljs-string">ORDER</span> <span class="hljs-string">BY</span> <span class="hljs-string">salary)</span> <span class="hljs-string">as</span> <span class="hljs-string">lead</span><span class="hljs-string">FROM</span> <span class="hljs-string">employee;</span><span class="hljs-string">name</span>    <span class="hljs-string">deptno</span>  <span class="hljs-string">salary</span>  <span class="hljs-string">lead</span><span class="hljs-string">Wendy</span>   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">4000</span><span class="hljs-string">Will</span>    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">5000</span><span class="hljs-string">Michael</span> <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">5500</span><span class="hljs-string">Lucy</span>    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">6400</span><span class="hljs-string">Steven</span>  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-literal">NULL</span><span class="hljs-string">Lily</span>    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">6000</span><span class="hljs-string">Jess</span>    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">6400</span><span class="hljs-string">Mike</span>    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-literal">NULL</span><span class="hljs-string">Yun</span>     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">7000</span><span class="hljs-string">Wei</span>     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">8000</span><span class="hljs-string">Richard</span> <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-literal">NULL</span></code></pre><h4 id="1-4-5-按部门分组，统计每个部门员工的工资以及小于等于该员工工资的上一个员工的工资"><a href="#1-4-5-按部门分组，统计每个部门员工的工资以及小于等于该员工工资的上一个员工的工资" class="headerlink" title="1.4.5 按部门分组，统计每个部门员工的工资以及小于等于该员工工资的上一个员工的工资"></a>1.4.5 按部门分组，统计每个部门员工的工资以及小于等于该员工工资的上一个员工的工资</h4><pre><code class="hljs yaml"><span class="hljs-string">SELECT</span> <span class="hljs-string">name,</span> <span class="hljs-string">dept_num</span> <span class="hljs-string">as</span> <span class="hljs-string">deptno,</span> <span class="hljs-string">salary,</span> <span class="hljs-string">lag(salary,1)</span> <span class="hljs-string">OVER</span> <span class="hljs-string">(PARTITION</span> <span class="hljs-string">BY</span> <span class="hljs-string">dept_num</span> <span class="hljs-string">ORDER</span> <span class="hljs-string">BY</span> <span class="hljs-string">salary)</span> <span class="hljs-string">as</span> <span class="hljs-string">lead</span><span class="hljs-string">FROM</span> <span class="hljs-string">employee;</span><span class="hljs-string">name</span>    <span class="hljs-string">deptno</span>  <span class="hljs-string">salary</span>  <span class="hljs-string">lead</span><span class="hljs-string">Wendy</span>   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-literal">NULL</span><span class="hljs-string">Will</span>    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">4000</span><span class="hljs-string">Michael</span> <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">4000</span><span class="hljs-string">Lucy</span>    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">5000</span><span class="hljs-string">Steven</span>  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">5500</span><span class="hljs-string">Lily</span>    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-literal">NULL</span><span class="hljs-string">Jess</span>    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">5000</span><span class="hljs-string">Mike</span>    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">6000</span><span class="hljs-string">Yun</span>     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-literal">NULL</span><span class="hljs-string">Wei</span>     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">5500</span><span class="hljs-string">Richard</span> <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">7000</span></code></pre><h4 id="1-4-6-按部门分组，统计每个部门员工工资以及该部门最低的员工工资"><a href="#1-4-6-按部门分组，统计每个部门员工工资以及该部门最低的员工工资" class="headerlink" title="1.4.6 按部门分组，统计每个部门员工工资以及该部门最低的员工工资"></a>1.4.6 按部门分组，统计每个部门员工工资以及该部门最低的员工工资</h4><pre><code class="hljs angelscript">SELECT name, dept_num as deptno, salary, first_value(salary) OVER (PARTITION BY dept_num ORDER BY salary) as fvalFROM employee;name    deptno  salary  fvalWendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">4000</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">4000</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">4000</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">4000</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">4000</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">5000</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">5000</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">5000</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">5500</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">5500</span>Richard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">5500</span></code></pre><h4 id="1-4-7-按部门分组，统计每个部门员工工资以及该部门最高的员工工资"><a href="#1-4-7-按部门分组，统计每个部门员工工资以及该部门最高的员工工资" class="headerlink" title="1.4.7 按部门分组，统计每个部门员工工资以及该部门最高的员工工资"></a>1.4.7 按部门分组，统计每个部门员工工资以及该部门最高的员工工资</h4><pre><code class="hljs angelscript">SELECT name, dept_num as deptno, salary, last_value(salary) OVER (PARTITION BY dept_num ORDER BY salary RANGEBETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as lvalFROM employee;name    deptno  salary  lvalWendy   <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">6400</span>Will    <span class="hljs-number">1000</span>    <span class="hljs-number">4000</span>    <span class="hljs-number">6400</span>Michael <span class="hljs-number">1000</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">6400</span>Lucy    <span class="hljs-number">1000</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">6400</span>Steven  <span class="hljs-number">1000</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">6400</span>Lily    <span class="hljs-number">1001</span>    <span class="hljs-number">5000</span>    <span class="hljs-number">6400</span>Jess    <span class="hljs-number">1001</span>    <span class="hljs-number">6000</span>    <span class="hljs-number">6400</span>Mike    <span class="hljs-number">1001</span>    <span class="hljs-number">6400</span>    <span class="hljs-number">6400</span>Yun     <span class="hljs-number">1002</span>    <span class="hljs-number">5500</span>    <span class="hljs-number">8000</span>Wei     <span class="hljs-number">1002</span>    <span class="hljs-number">7000</span>    <span class="hljs-number">8000</span>Richard <span class="hljs-number">1002</span>    <span class="hljs-number">8000</span>    <span class="hljs-number">8000</span></code></pre><p><strong>注意</strong>: last_value默认的窗口是RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT  ROW，表示当前行永远是最后一个值，需改成RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED  FOLLOWING。</p><p><img src="https://s1.ax1x.com/2020/08/20/d8xdVU.png" srcset="/img/loading.gif" alt=""></p><ul><li>RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</li></ul><p>为默认值，即当指定了ORDER BY从句，而省略了window从句 ，表示从开始到当前行。</p><ul><li>RANGE BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING</li></ul><p>表示从当前行到最后一行</p><ul><li>RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING</li></ul><p>表示所有行</p><ul><li>n PRECEDING  m FOLLOWING</li></ul><p>表示窗口的范围是：[（当前行的行数）- n, （当前行的行数）+m] row.</p><h2 id="2-窗口函数——练习二"><a href="#2-窗口函数——练习二" class="headerlink" title="2. 窗口函数——练习二"></a>2. 窗口函数——练习二</h2><h3 id="2-1-数据准备"><a href="#2-1-数据准备" class="headerlink" title="2.1 数据准备"></a>2.1 数据准备</h3><h4 id="2-1-1-样例数据"><a href="#2-1-1-样例数据" class="headerlink" title="2.1.1 样例数据"></a>2.1.1 样例数据</h4><pre><code class="hljs angelscript">name，orderdate，costjack,<span class="hljs-number">2017</span><span class="hljs-number">-01</span><span class="hljs-number">-01</span>,<span class="hljs-number">10</span>tony,<span class="hljs-number">2017</span><span class="hljs-number">-01</span><span class="hljs-number">-02</span>,<span class="hljs-number">15</span>jack,<span class="hljs-number">2017</span><span class="hljs-number">-02</span><span class="hljs-number">-03</span>,<span class="hljs-number">23</span>tony,<span class="hljs-number">2017</span><span class="hljs-number">-01</span><span class="hljs-number">-04</span>,<span class="hljs-number">29</span>jack,<span class="hljs-number">2017</span><span class="hljs-number">-01</span><span class="hljs-number">-05</span>,<span class="hljs-number">46</span>jack,<span class="hljs-number">2017</span><span class="hljs-number">-04</span><span class="hljs-number">-06</span>,<span class="hljs-number">42</span>tony,<span class="hljs-number">2017</span><span class="hljs-number">-01</span><span class="hljs-number">-07</span>,<span class="hljs-number">50</span>jack,<span class="hljs-number">2017</span><span class="hljs-number">-01</span><span class="hljs-number">-08</span>,<span class="hljs-number">55</span>mart,<span class="hljs-number">2017</span><span class="hljs-number">-04</span><span class="hljs-number">-08</span>,<span class="hljs-number">62</span>mart,<span class="hljs-number">2017</span><span class="hljs-number">-04</span><span class="hljs-number">-09</span>,<span class="hljs-number">68</span>neil,<span class="hljs-number">2017</span><span class="hljs-number">-05</span><span class="hljs-number">-10</span>,<span class="hljs-number">12</span>mart,<span class="hljs-number">2017</span><span class="hljs-number">-04</span><span class="hljs-number">-11</span>,<span class="hljs-number">75</span>neil,<span class="hljs-number">2017</span><span class="hljs-number">-06</span><span class="hljs-number">-12</span>,<span class="hljs-number">80</span>mart,<span class="hljs-number">2017</span><span class="hljs-number">-04</span><span class="hljs-number">-13</span>,<span class="hljs-number">94</span></code></pre><h4 id="2-1-2-建表语句"><a href="#2-1-2-建表语句" class="headerlink" title="2.1.2 建表语句"></a>2.1.2 建表语句</h4><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> business(<span class="hljs-keyword">name</span> <span class="hljs-keyword">string</span>, orderdate <span class="hljs-keyword">string</span>,<span class="hljs-keyword">cost</span> <span class="hljs-built_in">int</span>) <span class="hljs-keyword">ROW</span> <span class="hljs-keyword">FORMAT</span> <span class="hljs-keyword">DELIMITED</span> <span class="hljs-keyword">FIELDS</span> <span class="hljs-keyword">TERMINATED</span> <span class="hljs-keyword">BY</span> <span class="hljs-string">','</span>;</code></pre><h4 id="2-1-3-导入数据"><a href="#2-1-3-导入数据" class="headerlink" title="2.1.3 导入数据"></a>2.1.3 导入数据</h4><pre><code class="hljs sql"><span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">"/opt/module/datas/business.txt"</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> business;</code></pre><h3 id="2-2-相关说明"><a href="#2-2-相关说明" class="headerlink" title="2.2 相关说明"></a>2.2 相关说明</h3><p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化</p><p>CURRENT ROW：当前行</p><p>n PRECEDING：往前n行数据</p><p>n FOLLOWING：往后n行数据</p><p>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p><p>LAG(col,n)：往前第n行数据</p><p>LEAD(col,n)：往后第n行数据</p><p>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</p><h3 id="2-3-练习题"><a href="#2-3-练习题" class="headerlink" title="2.3 练习题"></a>2.3 练习题</h3><h4 id="2-3-1-查询在2017年4月份购买过的顾客及总人数"><a href="#2-3-1-查询在2017年4月份购买过的顾客及总人数" class="headerlink" title="2.3.1 查询在2017年4月份购买过的顾客及总人数"></a>2.3.1 查询在2017年4月份购买过的顾客及总人数</h4><pre><code class="hljs routeros">select name,count(*) over () <span class="hljs-keyword">from</span> business where substring(orderdate,1,7) = <span class="hljs-string">'2017-04'</span> group by name;</code></pre><h4 id="2-3-2-查询顾客的购买明细及月购买总额"><a href="#2-3-2-查询顾客的购买明细及月购买总额" class="headerlink" title="2.3.2 查询顾客的购买明细及月购买总额"></a>2.3.2 查询顾客的购买明细及月购买总额</h4><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">name</span>,orderdate,<span class="hljs-keyword">cost</span>,<span class="hljs-keyword">sum</span>(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">month</span>(orderdate)) <span class="hljs-keyword">from</span> business;</code></pre><h4 id="2-3-3-上述的场景-要将cost按照日期进行累加"><a href="#2-3-3-上述的场景-要将cost按照日期进行累加" class="headerlink" title="2.3.3 上述的场景,要将cost按照日期进行累加"></a>2.3.3 上述的场景,要将cost按照日期进行累加</h4><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">name</span>,orderdate,<span class="hljs-keyword">cost</span>, <span class="hljs-keyword">sum</span>(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>() <span class="hljs-keyword">as</span> sample1,<span class="hljs-comment">--所有行相加 </span><span class="hljs-keyword">sum</span>(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span>) <span class="hljs-keyword">as</span> sample2,<span class="hljs-comment">--按name分组，组内数据相加 </span><span class="hljs-keyword">sum</span>(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate) <span class="hljs-keyword">as</span> sample3,<span class="hljs-comment">--按name分组，组内数据累加 </span><span class="hljs-keyword">sum</span>(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> <span class="hljs-keyword">UNBOUNDED</span> <span class="hljs-keyword">PRECEDING</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">current</span> <span class="hljs-keyword">row</span> ) <span class="hljs-keyword">as</span> sample4 ,<span class="hljs-comment">--和sample3一样,由起点到当前行的聚合 </span><span class="hljs-keyword">sum</span>(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> <span class="hljs-number">1</span> <span class="hljs-keyword">PRECEDING</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">current</span> <span class="hljs-keyword">row</span>) <span class="hljs-keyword">as</span> sample5, <span class="hljs-comment">--当前行和前面一行做聚合 </span><span class="hljs-keyword">sum</span>(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> <span class="hljs-number">1</span> <span class="hljs-keyword">PRECEDING</span> <span class="hljs-keyword">AND</span> <span class="hljs-number">1</span> <span class="hljs-keyword">FOLLOWING</span> ) <span class="hljs-keyword">as</span> sample6,<span class="hljs-comment">--当前行和前边一行及后面一行 </span><span class="hljs-keyword">sum</span>(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> <span class="hljs-keyword">current</span> <span class="hljs-keyword">row</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">UNBOUNDED</span> <span class="hljs-keyword">FOLLOWING</span> ) <span class="hljs-keyword">as</span> sample7 <span class="hljs-comment">--当前行及后面所有行 </span><span class="hljs-keyword">from</span> business;</code></pre><h4 id="2-3-4-查看顾客上次的购买时间"><a href="#2-3-4-查看顾客上次的购买时间" class="headerlink" title="2.3.4 查看顾客上次的购买时间"></a>2.3.4 查看顾客上次的购买时间</h4><pre><code class="hljs pgsql"><span class="hljs-keyword">select</span> <span class="hljs-type">name</span>,orderdate,<span class="hljs-keyword">cost</span>, lag(orderdate,<span class="hljs-number">1</span>,<span class="hljs-string">'1900-01-01'</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate ) <span class="hljs-keyword">as</span> time1, lag(orderdate,<span class="hljs-number">2</span>) <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate) <span class="hljs-keyword">as</span> time2 <span class="hljs-keyword">from</span> business;</code></pre><h4 id="2-3-5-查询前20-时间的订单信息"><a href="#2-3-5-查询前20-时间的订单信息" class="headerlink" title="2.3.5 查询前20%时间的订单信息"></a>2.3.5 查询前20%时间的订单信息</h4><pre><code class="hljs pgsql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> (    <span class="hljs-keyword">select</span> <span class="hljs-type">name</span>,orderdate,<span class="hljs-keyword">cost</span>, ntile(<span class="hljs-number">5</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate) sorted    <span class="hljs-keyword">from</span> business) t<span class="hljs-keyword">where</span> sorted = <span class="hljs-number">1</span>;</code></pre><blockquote><p>参考资料：</p><p><a href="https://mp.weixin.qq.com/s/K2TA_PhNzGEkucYxBXqhLw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/K2TA_PhNzGEkucYxBXqhLw</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>练习题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark定义UDF函数，UDAF函数</title>
    <link href="/2020/08/04/Spark%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0%EF%BC%8CUDAF%E5%87%BD%E6%95%B0/"/>
    <url>/2020/08/04/Spark%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0%EF%BC%8CUDAF%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-自定义UDF函数"><a href="#1-自定义UDF函数" class="headerlink" title="1. 自定义UDF函数"></a>1. 自定义UDF函数</h2><p>​    使用<code>sparkSession.udf.register()</code>定义UDF函数</p><p>示例：</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> net.sf.json.<span class="hljs-type">JSONObject</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">UDF</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;          <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"udf"</span>).setMaster(<span class="hljs-string">"local"</span>)    <span class="hljs-keyword">val</span> sparkContext = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()    <span class="hljs-keyword">import</span> sparkSession.implicits._          <span class="hljs-comment">//student.txt</span>    <span class="hljs-comment">//张三_2班_&#123;"math":90,"english":100&#125;</span>    <span class="hljs-comment">//李四_3班_&#123;"math":80,"english":50&#125;</span>    <span class="hljs-comment">//王五_2班_&#123;"math":60,"english":70&#125;</span>    <span class="hljs-keyword">val</span> stuDF = sparkContext.textFile(<span class="hljs-string">"src\\main\\resources\\student.txt"</span>)      .map(_.split(<span class="hljs-string">"_"</span>))      .map(array =&gt; (array(<span class="hljs-number">0</span>),array(<span class="hljs-number">1</span>),array(<span class="hljs-number">2</span>)))      .toDF(<span class="hljs-string">"name"</span>, <span class="hljs-string">"class"</span>, <span class="hljs-string">"gradeJson"</span>)      .createOrReplaceTempView(<span class="hljs-string">"stu"</span>)    <span class="hljs-comment">//自定义udf拼接两个列</span>    sparkSession.udf.register(<span class="hljs-string">"concat"</span>,(v1:<span class="hljs-type">String</span>,v2:<span class="hljs-type">String</span>,split:<span class="hljs-type">String</span>) =&gt;&#123;      v1 + split + v2    &#125;)    <span class="hljs-keyword">val</span> sql = <span class="hljs-string">"select concat(name,class,',') from stu"</span>    sparkSession.sql(sql).show()<span class="hljs-comment">//    +--------------------------+</span><span class="hljs-comment">//    |UDF:concat(name, class, ,)|</span><span class="hljs-comment">//    +--------------------------+</span><span class="hljs-comment">//    |                     张三,2班|</span><span class="hljs-comment">//    |                     李四,3班|</span><span class="hljs-comment">//    |                     王五,2班|</span><span class="hljs-comment">//    +--------------------------+</span>    <span class="hljs-comment">//自定义udf：解析Json</span>    sparkSession.udf.register(<span class="hljs-string">"get_json_field"</span>, (json: <span class="hljs-type">String</span>, field: <span class="hljs-type">String</span>) =&gt; &#123;      <span class="hljs-keyword">val</span> jSONObject = <span class="hljs-type">JSONObject</span>.fromObject(json)      jSONObject.getString(field)    &#125;)          <span class="hljs-keyword">val</span> sql1 = <span class="hljs-string">"select get_json_field(gradeJson,'math') from stu"</span>    sparkSession.sql(sql1).show()    <span class="hljs-comment">//+-----------------------------------+</span>    <span class="hljs-comment">//|UDF:get_json_field(gradeJson, math)|</span>    <span class="hljs-comment">//+-----------------------------------+</span>    <span class="hljs-comment">//|                                 90|</span>    <span class="hljs-comment">//|                                 70|</span>    <span class="hljs-comment">//|                                 50|</span>    <span class="hljs-comment">//+-----------------------------------+</span>  &#125;&#125;</code></pre><h2 id="2-自定义UDAF函数"><a href="#2-自定义UDAF函数" class="headerlink" title="2. 自定义UDAF函数"></a>2. 自定义UDAF函数</h2><p>​    强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。</p><p>​    注意：自定义的UDAF函数需要先注册后再使用</p><h3 id="2-1-弱类型用户自定义聚合函数"><a href="#2-1-弱类型用户自定义聚合函数" class="headerlink" title="2.1 弱类型用户自定义聚合函数"></a>2.1 弱类型用户自定义聚合函数</h3><p>通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。</p><p>示例：现类似Hive中的collect_set函数</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Row</span><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="hljs-type">MutableAggregationBuffer</span>, <span class="hljs-type">UserDefinedAggregateFunction</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.sql.types.&#123;<span class="hljs-type">DataType</span>, <span class="hljs-type">StructField</span>, <span class="hljs-type">StructType</span>,<span class="hljs-type">StringType</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * 实现类似Hive中的collect_set</span><span class="hljs-comment"> * 列转行</span><span class="hljs-comment"> * 它们都是将分组中的某列转为一个数组返回，</span><span class="hljs-comment"> * 不同的是collect_list不去重而collect_set去重。</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Collect_set</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">UserDefinedAggregateFunction</span></span>&#123;  <span class="hljs-comment">// UDAF:输入数据类型为String</span>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inputSchema</span></span>: <span class="hljs-type">StructType</span> = <span class="hljs-type">StructType</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">"col"</span>, <span class="hljs-type">StringType</span>)::<span class="hljs-type">Nil</span>)  <span class="hljs-comment">// 缓冲区类型</span>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferSchema</span></span>: <span class="hljs-type">StructType</span> = <span class="hljs-type">StructType</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">"bufferCol"</span>, <span class="hljs-type">StringType</span>)::<span class="hljs-type">Nil</span>)  <span class="hljs-comment">// 输出数据类型</span>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataType</span></span>: <span class="hljs-type">DataType</span> = <span class="hljs-type">StringType</span>  <span class="hljs-comment">// 对于相同的输入是否一直返回相同的输出</span>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deterministic</span></span>: <span class="hljs-type">Boolean</span> = <span class="hljs-literal">true</span>  <span class="hljs-comment">//初始化buffer</span>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>): <span class="hljs-type">Unit</span> = &#123;    buffer(<span class="hljs-number">0</span>) = <span class="hljs-string">""</span>  &#125;  <span class="hljs-comment">// 相同Execute间的数据合并</span>  <span class="hljs-comment">// input更新进buffer(0)</span>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>, input: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">var</span> bufferInfo = buffer.getString(<span class="hljs-number">0</span>)    <span class="hljs-keyword">val</span> col = input.getString(<span class="hljs-number">0</span>)    <span class="hljs-comment">//collect_set需要去重，如果col已经在buffer中，就不拼接</span>    <span class="hljs-keyword">if</span>(!bufferInfo.contains(col))&#123;      <span class="hljs-keyword">if</span>(<span class="hljs-string">""</span>.equals(bufferInfo))&#123;        bufferInfo += col      &#125;<span class="hljs-keyword">else</span>&#123;        bufferInfo += <span class="hljs-string">","</span> + col      &#125;      buffer.update(<span class="hljs-number">0</span>,bufferInfo)    &#125;  &#125;  <span class="hljs-comment">// 不同Execute间的数据合并</span>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(buffer1: <span class="hljs-type">MutableAggregationBuffer</span>, buffer2: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">var</span> bufferInfo1 = buffer1.getString(<span class="hljs-number">0</span>)    <span class="hljs-keyword">val</span> bufferInfo2 = buffer2.getString(<span class="hljs-number">0</span>)    <span class="hljs-keyword">for</span> (col &lt;- bufferInfo2.split(<span class="hljs-string">","</span>)) &#123;      <span class="hljs-keyword">if</span> (!bufferInfo1.contains(col)) &#123;        <span class="hljs-keyword">if</span> (<span class="hljs-string">""</span>.equals(bufferInfo1)) &#123;          bufferInfo1 += col        &#125; <span class="hljs-keyword">else</span> &#123;          bufferInfo1 += <span class="hljs-string">","</span> + col        &#125;        buffer1.update(<span class="hljs-number">0</span>, bufferInfo1)      &#125;    &#125;  &#125;  <span class="hljs-comment">// 计算最终结果</span>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span></span>(buffer: <span class="hljs-type">Row</span>): <span class="hljs-type">Any</span> = &#123;    buffer.getString(<span class="hljs-number">0</span>)  &#125;&#125;</code></pre><p>测试：</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-type">SparkConf</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TestUDAF</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"udf"</span>).setMaster(<span class="hljs-string">"local"</span>)    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()    sparkSession.udf.register(<span class="hljs-string">"collect_set"</span>,<span class="hljs-keyword">new</span> <span class="hljs-type">Collect_set</span>)        <span class="hljs-keyword">val</span> dataArray = <span class="hljs-type">Array</span>((<span class="hljs-string">"张三"</span>,<span class="hljs-string">"red"</span>),          (<span class="hljs-string">"李四"</span>,<span class="hljs-string">"yellow"</span>),(<span class="hljs-string">"张三"</span>,<span class="hljs-string">"green"</span>),(<span class="hljs-string">"张三"</span>,<span class="hljs-string">"green"</span>))    <span class="hljs-keyword">import</span> sparkSession.implicits._    <span class="hljs-keyword">val</span> dataFrame = sparkSession.sparkContext.makeRDD(dataArray)      .toDF(<span class="hljs-string">"name"</span>, <span class="hljs-string">"color"</span>).createOrReplaceTempView(<span class="hljs-string">"stu"</span>)    <span class="hljs-keyword">val</span> sql = <span class="hljs-string">"select name,collect_set(color) from stu group by name"</span>    sparkSession.sql(sql).show()    <span class="hljs-comment">//+----+------------------+</span>    <span class="hljs-comment">//|name|collect_set(color)|</span>    <span class="hljs-comment">//+----+------------------+</span>    <span class="hljs-comment">//|  李四|            yellow|</span>    <span class="hljs-comment">//|  张三|         red,green|</span>    <span class="hljs-comment">//+----+------------------+</span>  &#125;&#125;</code></pre><h3 id="2-2-强类型用户自定义聚合函数"><a href="#2-2-强类型用户自定义聚合函数" class="headerlink" title="2.2 强类型用户自定义聚合函数"></a>2.2 强类型用户自定义聚合函数</h3><p>通过继承Aggregator来实现强类型自定义聚合函数，求平均工资</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">Aggregator</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoder</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoders</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><span class="hljs-comment">// 既然是强类型，可能有case类</span><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Employee</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, salary: <span class="hljs-type">Long</span></span>)</span><span class="hljs-class"><span class="hljs-title">case</span> <span class="hljs-title">class</span> <span class="hljs-title">Average</span>(<span class="hljs-params">var sum: <span class="hljs-type">Long</span>, var count: <span class="hljs-type">Long</span></span>)</span><span class="hljs-class"></span><span class="hljs-class"><span class="hljs-title">object</span> <span class="hljs-title">MyAverage</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Aggregator</span>[<span class="hljs-type">Employee</span>, <span class="hljs-type">Average</span>, <span class="hljs-type">Double</span>] </span>&#123;<span class="hljs-comment">// 定义一个数据结构，保存工资总数和工资总个数，初始都为0</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zero</span></span>: <span class="hljs-type">Average</span> = <span class="hljs-type">Average</span>(<span class="hljs-number">0</span>L, <span class="hljs-number">0</span>L)<span class="hljs-comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span><span class="hljs-comment">// and return it instead of constructing a new object</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reduce</span></span>(buffer: <span class="hljs-type">Average</span>, employee: <span class="hljs-type">Employee</span>): <span class="hljs-type">Average</span> = &#123;buffer.sum += employee.salarybuffer.count += <span class="hljs-number">1</span>buffer&#125;<span class="hljs-comment">// 聚合不同execute的结果</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(b1: <span class="hljs-type">Average</span>, b2: <span class="hljs-type">Average</span>): <span class="hljs-type">Average</span> = &#123;b1.sum += b2.sumb1.count += b2.countb1&#125;<span class="hljs-comment">// 计算输出</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">finish</span></span>(reduction: <span class="hljs-type">Average</span>): <span class="hljs-type">Double</span> = reduction.sum.toDouble / reduction.count<span class="hljs-comment">// 设定之间值类型的编码器，要转换成case类</span><span class="hljs-comment">// Encoders.product是进行scala元组和case类转换的编码器 </span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Average</span>] = <span class="hljs-type">Encoders</span>.product<span class="hljs-comment">// 设定最终输出值的编码器</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outputEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Double</span>] = <span class="hljs-type">Encoders</span>.scalaDouble&#125;<span class="hljs-keyword">import</span> spark.implicits._<span class="hljs-keyword">val</span> ds = spark.read.json(<span class="hljs-string">"examples/src/main/resources/employees.json"</span>).as[<span class="hljs-type">Employee</span>]ds.show()<span class="hljs-comment">// +-------+------+</span><span class="hljs-comment">// |   name|salary|</span><span class="hljs-comment">// +-------+------+</span><span class="hljs-comment">// |Michael|  3000|</span><span class="hljs-comment">// |   Andy|  4500|</span><span class="hljs-comment">// | Justin|  3500|</span><span class="hljs-comment">// |  Berta|  4000|</span><span class="hljs-comment">// +-------+------+</span><span class="hljs-comment">// Convert the function to a `TypedColumn` and give it a name</span><span class="hljs-keyword">val</span> averageSalary = <span class="hljs-type">MyAverage</span>.toColumn.name(<span class="hljs-string">"average_salary"</span>)<span class="hljs-keyword">val</span> result = ds.select(averageSalary)result.show()<span class="hljs-comment">// +--------------+</span><span class="hljs-comment">// |average_salary|</span><span class="hljs-comment">// +--------------+</span><span class="hljs-comment">// |        3750.0|</span><span class="hljs-comment">// +--------------+</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>scala读取配置文件</title>
    <link href="/2020/07/27/scala%E8%AF%BB%E5%8F%96%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"/>
    <url>/2020/07/27/scala%E8%AF%BB%E5%8F%96%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="scala读取配置文件"><a href="#scala读取配置文件" class="headerlink" title="scala读取配置文件"></a>scala读取配置文件</h2><p><strong>项目内的配置文件</strong></p><h3 id="方法一：使用FileBasedConfigurationBuilder"><a href="#方法一：使用FileBasedConfigurationBuilder" class="headerlink" title="方法一：使用FileBasedConfigurationBuilder"></a>方法一：使用FileBasedConfigurationBuilder</h3><pre><code class="hljs scala"><span class="hljs-keyword">package</span> conf<span class="hljs-keyword">import</span> org.apache.commons.configuration2.&#123;<span class="hljs-type">FileBasedConfiguration</span>, <span class="hljs-type">PropertiesConfiguration</span>&#125;<span class="hljs-keyword">import</span> org.apache.commons.configuration2.builder.<span class="hljs-type">FileBasedConfigurationBuilder</span><span class="hljs-keyword">import</span> org.apache.commons.configuration2.builder.fluent.<span class="hljs-type">Parameters</span><span class="hljs-comment">/**</span><span class="hljs-comment"> * 读取配置文件</span><span class="hljs-comment"> * 方法一：</span><span class="hljs-comment"> * 使用FileBasedConfigurationBuilder,基于文件配置生成器</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">ConfigManagerFileBased</span> </span>&#123;  <span class="hljs-comment">// 创建用于初始化配置生成器实例的参数对象</span>  <span class="hljs-keyword">private</span> <span class="hljs-keyword">val</span> params = <span class="hljs-keyword">new</span> <span class="hljs-type">Parameters</span>()  <span class="hljs-comment">// FileBasedConfigurationBuilder:产生一个传入的类的实例对象</span>  <span class="hljs-comment">// FileBasedConfiguration:融合FileBased与Configuration的接口</span>  <span class="hljs-comment">// PropertiesConfiguration:从一个或者多个文件读取配置的标准配置加载器</span>  <span class="hljs-comment">// configure():通过params实例初始化配置生成器</span>  <span class="hljs-comment">// 向FileBasedConfigurationBuilder()中传入一个标准配置加载器类，生成一个加载器类的实例对象，然后通过params参数对其初始化</span>  <span class="hljs-keyword">private</span> <span class="hljs-keyword">val</span> builder: <span class="hljs-type">FileBasedConfigurationBuilder</span>[<span class="hljs-type">FileBasedConfiguration</span>] = <span class="hljs-keyword">new</span> <span class="hljs-type">FileBasedConfigurationBuilder</span>[<span class="hljs-type">FileBasedConfiguration</span>](classOf[<span class="hljs-type">PropertiesConfiguration</span>])    .configure(params.properties().setFileName(<span class="hljs-string">"commerce.properties"</span>))  <span class="hljs-comment">// 通过getConfiguration获取配置对</span>  <span class="hljs-keyword">val</span> config: <span class="hljs-type">FileBasedConfiguration</span> = builder.getConfiguration&#125;</code></pre><h3 id="方法二：使用Properties"><a href="#方法二：使用Properties" class="headerlink" title="方法二：使用Properties"></a>方法二：使用Properties</h3><pre><code class="hljs scala"><span class="hljs-keyword">package</span> conf<span class="hljs-keyword">import</span> java.io.&#123;<span class="hljs-type">FileInputStream</span>, <span class="hljs-type">InputStream</span>&#125;<span class="hljs-keyword">import</span> java.util.<span class="hljs-type">Properties</span><span class="hljs-comment">/**</span><span class="hljs-comment"> * 读取配置文件</span><span class="hljs-comment"> * 使用Properties</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">ConfigManagerProperties</span> </span>&#123;  <span class="hljs-keyword">val</span> props = <span class="hljs-keyword">new</span> <span class="hljs-type">Properties</span><span class="hljs-comment">/*  方法一：FileInputStream方式，传入配置文件所在位置</span><span class="hljs-comment">  val path = "src\\main\\resources\\commerce.properties"</span><span class="hljs-comment">  private val inputStream = new FileInputStream(path)*/</span>  <span class="hljs-comment">//方法二：用getResourceAsStream获取输入流</span>  <span class="hljs-comment">//getResourceAsStreampath以'/'开头时,</span>  <span class="hljs-comment">//从项目的ClassPath根下获取资源，就是要写相对于classpath根下的绝对路径。</span>  <span class="hljs-keyword">private</span> <span class="hljs-keyword">val</span> stream: <span class="hljs-type">InputStream</span> = <span class="hljs-keyword">this</span>.getClass().getResourceAsStream(<span class="hljs-string">"/commerce.properties"</span>)  props.load(stream)&#125;</code></pre><h3 id="测试："><a href="#测试：" class="headerlink" title="测试："></a>测试：</h3><pre><code class="hljs scala"><span class="hljs-keyword">package</span> conf<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TestConfiguration</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> user = <span class="hljs-type">ConfigManagerFileBased</span>.config.getString(<span class="hljs-string">"jdbc.user"</span>)    println(user) <span class="hljs-comment">//root</span>    <span class="hljs-keyword">val</span> password = <span class="hljs-type">ConfigManagerProperties</span>.props.getProperty(<span class="hljs-string">"jdbc.password"</span>)    println(password) <span class="hljs-comment">//123456</span>  &#125;&#125;</code></pre><blockquote><h4 id="扩展："><a href="#扩展：" class="headerlink" title="扩展："></a><strong>扩展：</strong></h4><p><a href="https://www.cnblogs.com/starwater/p/6703702.html" target="_blank" rel="noopener">scala读取jar包外配置文件的方式</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>工具类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark实现WordCount的4种方式</title>
    <link href="/2020/07/23/Spark%E5%AE%9E%E7%8E%B0WordCount%E7%9A%844%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
    <url>/2020/07/23/Spark%E5%AE%9E%E7%8E%B0WordCount%E7%9A%844%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark实现WordCount的4种方式"><a href="#Spark实现WordCount的4种方式" class="headerlink" title="Spark实现WordCount的4种方式"></a>Spark实现WordCount的4种方式</h1><p><strong>people.txt文件内容：</strong></p><pre><code class="hljs angelscript">Michael, <span class="hljs-number">29</span>Andy, <span class="hljs-number">30</span>Justin, <span class="hljs-number">29</span></code></pre><h2 id="1-RDD操作实现"><a href="#1-RDD操作实现" class="headerlink" title="1.RDD操作实现"></a>1.RDD操作实现</h2><pre><code class="hljs scala"><span class="hljs-comment">/**</span><span class="hljs-comment"> * rdd操作实现wordcount</span><span class="hljs-comment"> * 1.切割，转元组，reduceByKey根据key进行统计</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WC5</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"wordcount"</span>).setMaster(<span class="hljs-string">"local"</span>)    <span class="hljs-keyword">val</span> sparkContext = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)    <span class="hljs-keyword">val</span> dataRdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sparkContext.textFile(<span class="hljs-string">"src\\main\\resources\\people.txt"</span>)    <span class="hljs-keyword">val</span> res = dataRdd.flatMap(_.split(<span class="hljs-string">","</span>)).map((_, <span class="hljs-number">1</span>)).reduceByKey(_ + _)    res.foreach(println)<span class="hljs-comment">//    ( 30,1)</span><span class="hljs-comment">//    (Michael,1)</span><span class="hljs-comment">//    (Andy,1)</span><span class="hljs-comment">//    ( 29,2)</span><span class="hljs-comment">//    (Justin,1)</span>  &#125;&#125;</code></pre><h2 id="2-SparkSQL实现（一）"><a href="#2-SparkSQL实现（一）" class="headerlink" title="2.SparkSQL实现（一）"></a>2.SparkSQL实现（一）</h2><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WC4</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">"local[*]"</span>)    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()    <span class="hljs-keyword">import</span> sparkSession.implicits._    <span class="hljs-keyword">val</span> dataRdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sparkSession.sparkContext.textFile(<span class="hljs-string">"src\\main\\resources\\people.txt"</span>)    <span class="hljs-keyword">val</span> dataFrame = dataRdd.flatMap(_.split(<span class="hljs-string">","</span>)).toDF()    dataFrame.createOrReplaceTempView(<span class="hljs-string">"people"</span>)    <span class="hljs-comment">// 未指定schema，默认的列名是value</span>    <span class="hljs-keyword">val</span> res = sparkSession.sql(<span class="hljs-string">"select value,count(1) from people group by value"</span>)    res.show()<span class="hljs-comment">//    +-------+--------+</span><span class="hljs-comment">//    |  value|count(1)|</span><span class="hljs-comment">//    +-------+--------+</span><span class="hljs-comment">//    |     29|       2|</span><span class="hljs-comment">//    |Michael|       1|</span><span class="hljs-comment">//    |   Andy|       1|</span><span class="hljs-comment">//    |     30|       1|</span><span class="hljs-comment">//    | Justin|       1|</span><span class="hljs-comment">//    +-------+--------+</span>  &#125;&#125;</code></pre><h2 id="3-SparkSQL实现（三）"><a href="#3-SparkSQL实现（三）" class="headerlink" title="3.SparkSQL实现（三）"></a>3.SparkSQL实现（三）</h2><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WC2</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">"local[*]"</span>)    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()    <span class="hljs-keyword">val</span> dataRdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sparkSession.sparkContext.textFile(<span class="hljs-string">"src\\main\\resources\\people.txt"</span>)    <span class="hljs-keyword">import</span> sparkSession.implicits._    <span class="hljs-keyword">val</span> dataRow: <span class="hljs-type">RDD</span>[<span class="hljs-type">Row</span>] = dataRdd.flatMap(_.split(<span class="hljs-string">","</span>)).map(attributes =&gt; <span class="hljs-type">Row</span>(attributes.trim))    <span class="hljs-keyword">val</span> word: <span class="hljs-type">StructField</span> = <span class="hljs-type">StructField</span>(<span class="hljs-string">"word"</span>, <span class="hljs-type">StringType</span>, nullable = <span class="hljs-literal">true</span>)    <span class="hljs-keyword">val</span> wordStructType = <span class="hljs-type">StructType</span>(<span class="hljs-type">Seq</span>(word))    <span class="hljs-comment">// 手动创建DataFrame，指定列名叫word</span>    <span class="hljs-keyword">val</span> wordDataFrame = sparkSession.createDataFrame(dataRow, wordStructType)    wordDataFrame.createTempView(<span class="hljs-string">"wordCount"</span>)    <span class="hljs-keyword">val</span> res = sparkSession.sql(<span class="hljs-string">"select word, count(1) from wordCount group by word"</span>)    res.show()<span class="hljs-comment">//    +-------+--------+</span><span class="hljs-comment">//    |   word|count(1)|</span><span class="hljs-comment">//    +-------+--------+</span><span class="hljs-comment">//    |     29|       2|</span><span class="hljs-comment">//    |     30|       1|</span><span class="hljs-comment">//    |Michael|       1|</span><span class="hljs-comment">//    |   Andy|       1|</span><span class="hljs-comment">//    | Justin|       1|</span><span class="hljs-comment">//    +-------+--------+</span>  &#125;&#125;</code></pre><h2 id="4-SparkSQL实现（四）"><a href="#4-SparkSQL实现（四）" class="headerlink" title="4.SparkSQL实现（四）"></a>4.SparkSQL实现（四）</h2><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WC1</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">"local[*]"</span>)    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()    <span class="hljs-keyword">import</span> sparkSession.implicits._    <span class="hljs-keyword">val</span> dataRdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sparkSession.sparkContext.textFile(<span class="hljs-string">"src\\main\\resources\\people.txt"</span>)    <span class="hljs-comment">// 使用样例类，指定schema</span>    <span class="hljs-keyword">val</span> dataFrame = dataRdd.flatMap(_.split(<span class="hljs-string">","</span>)).map(attributes =&gt; <span class="hljs-type">WordCount</span>(attributes)).toDF()    dataFrame.show()    dataFrame.createOrReplaceTempView(<span class="hljs-string">"wordCount"</span>)    <span class="hljs-keyword">val</span> res = sparkSession.sql(<span class="hljs-string">"select word, count(1) from wordCount group by word"</span>)    res.show()<span class="hljs-comment">//    +-------+--------+</span><span class="hljs-comment">//    |   word|count(1)|</span><span class="hljs-comment">//    +-------+--------+</span><span class="hljs-comment">//    |     29|       2|</span><span class="hljs-comment">//    |Michael|       1|</span><span class="hljs-comment">//    |   Andy|       1|</span><span class="hljs-comment">//    |     30|       1|</span><span class="hljs-comment">//    | Justin|       1|</span><span class="hljs-comment">//    +-------+--------+</span>  &#125;&#125;<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCount</span>(<span class="hljs-params">word: <span class="hljs-type">String</span></span>)</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark系列（五）—— Tuning Spark</title>
    <link href="/2020/07/23/Tuning-Spark/"/>
    <url>/2020/07/23/Tuning-Spark/</url>
    
    <content type="html"><![CDATA[<h1 id="Tuning-Spark"><a href="#Tuning-Spark" class="headerlink" title="Tuning Spark"></a>Tuning Spark</h1><p>Because of the in-memory nature of most Spark computations, Spark programs can be bottlenecked by any resource in the cluster: CPU, network bandwidth, or memory. Most often, if the data fits in memory, the bottleneck is network bandwidth, but sometimes, you also need to do some tuning, such as <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">storing RDDs in serialized form</a>, to decrease memory usage. This guide will cover two main topics: data serialization, which is crucial for good network performance and can also reduce memory use, and memory tuning. We also sketch several smaller topics.</p><h2 id="1-Data-Serialization"><a href="#1-Data-Serialization" class="headerlink" title="1. Data Serialization"></a>1. Data Serialization</h2><p>Serialization plays an important role in the performance of any distributed application. Formats that are slow to serialize objects into, or consume a large number of bytes, will greatly slow down the computation. Often, this will be the first thing you should tune to optimize a Spark application. Spark aims to strike a balance between convenience (allowing you to work with any Java type in your operations) and performance. It provides two serialization libraries:</p><p>序列化在任何分布式应用程序的性能中都起着重要作用。将对象序列化为慢速格式或占用大量字节的格式将大大减慢计算速度。通常，这是您应该优化Spark应用程序的第一件事。Spark旨在在便利性（允许您在操作中使用任何Java类型）和性能之间取得平衡。它提供了两个序列化库：</p><ul><li><a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html" target="_blank" rel="noopener">Java serialization</a>: By default, Spark serializes objects using Java’s <code>ObjectOutputStream</code> framework, and can work with any class you create that implements <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html" target="_blank" rel="noopener"><code>java.io.Serializable</code></a>. You can also control the performance of your serialization more closely by extending <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Externalizable.html" target="_blank" rel="noopener"><code>java.io.Externalizable</code></a>. Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.</li><li><a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html" target="_blank" rel="noopener">Java序列化</a>：默认情况下，Spark使用Java的<code>ObjectOutputStream</code>框架对对象进行序列化，并且可以与您创建的实现的任何类一起使用 <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html" target="_blank" rel="noopener"><code>java.io.Serializable</code></a>。您还可以通过扩展来更紧密地控制序列化的性能 <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Externalizable.html" target="_blank" rel="noopener"><code>java.io.Externalizable</code></a>。Java序列化很灵活，但是通常很慢，并且导致许多类的序列化格式很大。</li><li><a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">Kryo serialization</a>: Spark can also use the Kryo library (version 2) to serialize objects more quickly. Kryo is significantly faster and more compact than Java serialization (often as much as 10x), but does not support all <code>Serializable</code> types and requires you to <em>register</em> the classes you’ll use in the program in advance for best performance.</li><li><a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">Kryo序列化</a>：Spark还可以使用Kryo库（版本2）更快地序列化对象。与Java序列化（通常多达10倍）相比，Kryo显着更快，更紧凑，但是Kryo不支持所有 <code>Serializable</code>类型，并且需要您预先<em>注册</em>要在程序中使用的类才能获得最佳性能。</li></ul><p>You can switch to using Kryo by initializing your job with a <a href="http://spark.apache.org/docs/2.1.2/configuration.html#spark-properties" target="_blank" rel="noopener">SparkConf</a> and calling <code>conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</code>. This setting configures the serializer used for not only shuffling data between worker nodes but also when serializing RDDs to disk. The only reason Kryo is not the default is because of the custom registration requirement, but we recommend trying it in any network-intensive application. Since Spark 2.0.0, we internally use Kryo serializer when shuffling RDDs with simple types, arrays of simple types, or string type.</p><p>Spark automatically includes Kryo serializers for the many commonly-used core Scala classes covered in the AllScalaRegistrar from the <a href="https://github.com/twitter/chill" target="_blank" rel="noopener">Twitter chill</a> library.</p><p>您可以通过使用<a href="http://spark.apache.org/docs/2.1.2/configuration.html#spark-properties" target="_blank" rel="noopener">SparkConf</a>初始化作业 并调用来切换为使用Kryo <code>conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</code>。此设置配置了串行器，该串行器不仅用于在工作程序节点之间改组数据，而且还用于将RDD序列化到磁盘时。Kryo不是默认值的唯一原因是由于自定义注册要求，但是我们建议在任何网络密集型应用程序中尝试使用它。从Spark 2.0.0开始，在将RDD与简单类型，简单类型的数组或字符串类型进行混洗时，我们在内部使用Kryo序列化器。</p><p>Spark自动为<a href="https://github.com/twitter/chill" target="_blank" rel="noopener">Twitter chill</a>库的AllScalaRegistrar中涵盖的许多常用Scala核心类包括Kryo序列化器。</p><p>To register your own custom classes with Kryo, use the <code>registerKryoClasses</code> method.</p><p>要向Kryo注册您自己的自定义类，请使用<code>registerKryoClasses</code>方法。</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(...).setAppName(...)conf.registerKryoClasses(<span class="hljs-type">Array</span>(classOf[<span class="hljs-type">MyClass1</span>], classOf[<span class="hljs-type">MyClass2</span>]))<span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)</code></pre><p>The <a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">Kryo documentation</a> describes more advanced registration options, such as adding custom serialization code.</p><p>If your objects are large, you may also need to increase the <code>spark.kryoserializer.buffer</code> <a href="http://spark.apache.org/docs/2.1.2/configuration.html#compression-and-serialization" target="_blank" rel="noopener">config</a>. This value needs to be large enough to hold the <em>largest</em> object you will serialize.</p><p>Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.</p><p>所述<a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">KRYO文档</a>描述了更先进的注册选项，如添加自定义序列的代码。</p><p>如果对象很大，则可能还需要增加<code>spark.kryoserializer.buffer</code> <a href="http://spark.apache.org/docs/2.1.2/configuration.html#compression-and-serialization" target="_blank" rel="noopener">config</a>。该值必须足够大以容纳要序列化的<em>最大</em>对象。</p><table><thead><tr><th><code>spark.kryoserializer.buffer.max</code></th><th>64m</th><th>Maximum allowable size of Kryo serialization buffer. This must be larger than any object you attempt to serialize. Increase this if you get a “buffer limit exceeded” exception inside Kryo.</th></tr></thead><tbody><tr><td><code>spark.kryoserializer.buffer</code></td><td>64k</td><td>Initial size of Kryo’s serialization buffer. Note that there will be one buffer <em>per core</em> on each worker. This buffer will grow up to <code>spark.kryoserializer.buffer.max</code> if needed.</td></tr></tbody></table><p>最后，如果您不注册自定义类，Kryo仍然可以工作，但必须将完整的类名与每个对象一起存储，这很浪费。</p><h2 id="2-Memory-Tuning"><a href="#2-Memory-Tuning" class="headerlink" title="2. Memory Tuning"></a>2. Memory Tuning</h2><p>There are three considerations in tuning memory usage: the <em>amount</em> of memory used by your objects (you may want your entire dataset to fit in memory), the <em>cost</em> of accessing those objects, and the overhead of <em>garbage collection</em> (if you have high turnover in terms of objects).</p><p>By default, Java objects are fast to access, but can easily consume a factor of 2-5x more space than the “raw” data inside their fields. This is due to several reasons:</p><ul><li>Each distinct Java object has an “object header”, which is about 16 bytes and contains information such as a pointer to its class. For an object with very little data in it (say one <code>Int</code> field), this can be bigger than the data.</li><li>Java <code>String</code>s have about 40 bytes of overhead over the raw string data (since they store it in an array of <code>Char</code>s and keep extra data such as the length), and store each character as <em>two</em> bytes due to <code>String</code>’s internal usage of UTF-16 encoding. Thus a 10-character string can easily consume 60 bytes.</li><li>Common collection classes, such as <code>HashMap</code> and <code>LinkedList</code>, use linked data structures, where there is a “wrapper” object for each entry (e.g. <code>Map.Entry</code>). This object not only has a header, but also pointers (typically 8 bytes each) to the next object in the list.</li><li>Collections of primitive types often store them as “boxed” objects such as <code>java.lang.Integer</code>.</li></ul><p>This section will start with an overview of memory management in Spark, then discuss specific strategies the user can take to make more efficient use of memory in his/her application. In particular, we will describe how to determine the memory usage of your objects, and how to improve it – either by changing your data structures, or by storing data in a serialized format. We will then cover tuning Spark’s cache size and the Java garbage collector.</p><p>有三个方面的考虑在调整内存使用：该<em>量</em>的存储你的对象所使用的（你可能希望你的整个数据集，以适应在内存中），则<em>成本</em>访问这些对象，并且开销<em>垃圾收集</em>（如果你有高成交条款）。</p><p>默认情况下，Java对象的访问速度很快，但是与其字段内的“原始”数据相比，它们很容易消耗2-5倍的空间。这是由于以下几个原因：</p><ul><li>每个不同的Java对象都有一个“对象标头”，大约16个字节，并包含诸如指向其类的指针之类的信息。对于其中数据很少的对象（例如一个<code>Int</code>字段），该对象可能大于数据。</li><li>Java <code>String</code>相对于原始字符串数据有大约40个字节的开销（因为它们将其存储在<code>Char</code>s 数组中并保留诸如长度之类的额外数据），并且由于UTF-16的内部用法，因此每个字符都存储为<em>两个</em>字节<code>String</code>编码。因此，一个10个字符的字符串可以轻松消耗60个字节。</li><li>诸如<code>HashMap</code>和的通用集合类<code>LinkedList</code>使用链接的数据结构，其中每个条目（例如<code>Map.Entry</code>）都有一个“包装”对象。该对象不仅具有标题，而且具有指向列表中下一个对象的指针（通常每个指针8个字节）。</li><li>基本类型的集合通常将它们存储为“盒装”对象，例如<code>java.lang.Integer</code>。</li></ul><p>本节将首先概述Spark中的内存管理，然后讨论用户可以采取的特定策略，以更有效地使用其应用程序中的内存。特别是，我们将描述如何确定对象的内存使用以及如何通过更改数据结构或以串行化格式存储数据来改善对象的使用情况。然后，我们将介绍调整Spark的缓存大小和Java垃圾收集器。</p><h3 id="2-1-Memory-Management-Overview"><a href="#2-1-Memory-Management-Overview" class="headerlink" title="2.1 Memory Management Overview"></a>2.1 Memory Management Overview</h3><p>Memory usage in Spark largely falls under one of two categories: execution and storage. Execution memory refers to that used for computation in shuffles, joins, sorts and aggregations, while storage memory refers to that used for caching and propagating internal data across the cluster. In Spark, execution and storage share a unified region (M). When no execution memory is used, storage can acquire all the available memory and vice versa. Execution may evict storage if necessary, but only until total storage memory usage falls under a certain threshold (R). In other words, <code>R</code> describes a subregion within <code>M</code> where cached blocks are never evicted. Storage may not evict execution due to complexities in implementation.</p><p>Spark中的内存使用情况大体上属于以下两种类别之一：执行和存储。执行内存是指用于洗牌，联接，排序和聚合的计算的内存，而存储内存是指用于在集群中缓存和传播内部数据的内存。在Spark中，执行和存储共享一个统一的区域（M）。当不使用执行内存时，存储可以获取所有可用内存，反之亦然。如果有必要，执行可能会驱逐存储，但只有在存储总内存使用量下降到某个阈值（R）以下时，才可以执行该操作。换句话说，<code>R</code>描述了一个子区域，在该子区域<code>M</code>中永远不会移出缓存的块。由于实现的复杂性，存储可能无法退出执行。</p><p>This design ensures several desirable properties. First, applications that do not use caching can use the entire space for execution, obviating unnecessary disk spills. Second, applications that do use caching can reserve a minimum storage space (R) where their data blocks are immune to being evicted. Lastly, this approach provides reasonable out-of-the-box performance for a variety of workloads without requiring user expertise of how memory is divided internally.</p><p>这种设计确保了几种理想的性能。首先，不使用缓存的应用程序可以将整个空间用于执行，从而避免了不必要的磁盘溢出。其次，确实使用缓存的应用程序可以保留最小的存储空间（R），以免其数据块被逐出。最后，这种方法可为各种工作负载提供合理的即用即用性能，而无需用户了解如何在内部划分内存。</p><p>Although there are two relevant configurations, the typical user should not need to adjust them as the default values are applicable to most workloads:</p><ul><li><code>spark.memory.fraction</code> expresses the size of <code>M</code> as a fraction of the (JVM heap space - 300MB) (default 0.6). The rest of the space (40%) is reserved for user data structures, internal metadata in Spark, and safeguarding against OOM errors in the case of sparse and unusually large records.</li><li><code>spark.memory.storageFraction</code> expresses the size of <code>R</code> as a fraction of <code>M</code> (default 0.5). <code>R</code> is the storage space within <code>M</code> where cached blocks immune to being evicted by execution.</li></ul><p>尽管有两种相关的配置，但典型用户无需调整它们，因为默认值适用于大多数工作负载：</p><ul><li><code>spark.memory.fraction</code>表示的大小<code>M</code>为（JVM堆空间-300MB）的一部分（默认值为0.6）。其余的空间（40％）保留用于用户数据结构，Spark中的内部元数据，并在记录稀疏和异常大的情况下防止OOM错误。</li><li><code>spark.memory.storageFraction</code>将的大小表示<code>R</code>为的分数<code>M</code>（默认值为0.5）。 <code>R</code>是<code>M</code>其中的缓存块不受执行影响而退出的存储空间。</li></ul><p>The value of <code>spark.memory.fraction</code> should be set in order to fit this amount of heap space comfortably within the JVM’s old or “tenured” generation. See the discussion of advanced GC tuning below for details.</p><p><code>spark.memory.fraction</code>应该设置的值，以便在JVM的旧版本或“长期使用的”一代中舒适地适应此堆空间量。有关详细信息，请参见下面有关高级GC调整的讨论。</p><h3 id="2-2-Determining-Memory-Consumption"><a href="#2-2-Determining-Memory-Consumption" class="headerlink" title="2.2 Determining Memory Consumption"></a>2.2 Determining Memory Consumption</h3><p>The best way to size the amount of memory consumption a dataset will require is to create an RDD, put it into cache, and look at the “Storage” page in the web UI. The page will tell you how much memory the RDD is occupying.</p><p>To estimate the memory consumption of a particular object, use <code>SizeEstimator</code>’s <code>estimate</code> method This is useful for experimenting with different data layouts to trim memory usage, as well as determining the amount of space a broadcast variable will occupy on each executor heap.</p><p>确定数据集所需的内存消耗量的最佳方法是创建一个RDD，将其放入缓存中，然后查看Web UI中的“ Storage”页面。该页面将告诉您RDD占用了多少内存。</p><p>要估算特定对象的内存使用量，请使用<code>SizeEstimator</code>的<code>estimate</code>方法。此方法可用于试验不同的数据布局以减少内存使用量，并确定广播变量将在每个执行程序堆上占用的空间量。</p><h3 id="2-3-Tuning-Data-Structures"><a href="#2-3-Tuning-Data-Structures" class="headerlink" title="2.3 Tuning Data Structures"></a>2.3 Tuning Data Structures</h3><p>The first way to reduce memory consumption is to avoid the Java features that add overhead, such as pointer-based data structures and wrapper objects. There are several ways to do this:</p><ol><li>Design your data structures to prefer arrays of objects, and primitive types, instead of the standard Java or Scala collection classes (e.g. <code>HashMap</code>). The <a href="http://fastutil.di.unimi.it/" target="_blank" rel="noopener">fastutil</a> library provides convenient collection classes for primitive types that are compatible with the Java standard library.</li><li>Avoid nested structures with a lot of small objects and pointers when possible.</li><li>Consider using numeric IDs or enumeration objects instead of strings for keys.</li><li>If you have less than 32 GB of RAM, set the JVM flag <code>-XX:+UseCompressedOops</code> to make pointers be four bytes instead of eight. You can add these options in <a href="http://spark.apache.org/docs/2.1.2/configuration.html#environment-variables" target="_blank" rel="noopener"><code>spark-env.sh</code></a>.</li></ol><p>减少内存消耗的第一种方法是避免使用Java功能，这些功能会增加开销，例如基于指针的数据结构和包装对象。做这件事有很多种方法：</p><ol><li>设计数据结构，使其更喜欢对象数组和原始类型，而不是标准的Java或Scala集合类（例如<code>HashMap</code>）。该<a href="http://fastutil.di.unimi.it/" target="_blank" rel="noopener">fastutil</a> 库提供方便的集合类基本类型是与Java标准库兼容。</li><li>尽可能避免使用带有许多小对象和指针的嵌套结构。</li><li>考虑使用数字ID或枚举对象代替键的字符串。</li><li>如果您的RAM少于32 GB，则设置JVM标志<code>-XX:+UseCompressedOops</code>以使指针为四个字节而不是八个字节。您可以在中添加这些选项 <a href="http://spark.apache.org/docs/2.1.2/configuration.html#environment-variables" target="_blank" rel="noopener"><code>spark-env.sh</code></a>。</li></ol><h3 id="2-4-Serialized-RDD-Storage"><a href="#2-4-Serialized-RDD-Storage" class="headerlink" title="2.4 Serialized RDD Storage"></a>2.4 Serialized RDD Storage</h3><p>When your objects are still too large to efficiently store despite this tuning, a much simpler way to reduce memory usage is to store them in <em>serialized</em> form, using the serialized StorageLevels in the <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">RDD persistence API</a>, such as <code>MEMORY_ONLY_SER</code>. Spark will then store each RDD partition as one large byte array. The only downside of storing data in serialized form is slower access times, due to having to deserialize each object on the fly. We highly recommend <a href="http://spark.apache.org/docs/2.1.2/tuning.html#data-serialization" target="_blank" rel="noopener">using Kryo</a> if you want to cache data in serialized form, as it leads to much smaller sizes than Java serialization (and certainly than raw Java objects).</p><p>当您的对象仍然太大而无法进行优化存储时，减少内存使用的一种更简单的方法是使用<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">RDD持久性API中</a>的序列化StorageLevels （例如）以<em>序列化</em>形式存储它们。然后，Spark将每个RDD分区存储为一个大字节数组。由于必须动态地反序列化每个对象，因此以串行形式存储数据的唯一缺点是访问时间较慢。如果您要缓存序列化形式的数据，我们强烈建议<a href="http://spark.apache.org/docs/2.1.2/tuning.html#data-serialization" target="_blank" rel="noopener">使用Kryo</a>，因为它导致的大小比Java序列化（当然也比原始Java对象）小。</p><h3 id="2-5-Garbage-Collection-Tuning"><a href="#2-5-Garbage-Collection-Tuning" class="headerlink" title="2.5 Garbage Collection Tuning"></a>2.5 Garbage Collection Tuning</h3><p>JVM garbage collection can be a problem when you have large “churn” in terms of the RDDs stored by your program. (It is usually not a problem in programs that just read an RDD once and then run many operations on it.) When Java needs to evict old objects to make room for new ones, it will need to trace through all your Java objects and find the unused ones. The main point to remember here is that <em>the cost of garbage collection is proportional to the number of Java objects</em>, so using data structures with fewer objects (e.g. an array of <code>Int</code>s instead of a <code>LinkedList</code>) greatly lowers this cost. An even better method is to persist objects in serialized form, as described above: now there will be only <em>one</em> object (a byte array) per RDD partition. Before trying other techniques, the first thing to try if GC is a problem is to use <a href="http://spark.apache.org/docs/2.1.2/tuning.html#serialized-rdd-storage" target="_blank" rel="noopener">serialized caching</a>.</p><p>GC can also be a problem due to interference between your tasks’ working memory (the amount of space needed to run the task) and the RDDs cached on your nodes. We will discuss how to control the space allocated to the RDD cache to mitigate this.</p><p>当您在程序存储的RDD方面有较大的“搅动”时，JVM垃圾回收可能会成为问题。（在只读取一次RDD然后对其执行许多操作的程序中，这通常不是问题。）当Java需要逐出旧对象以为新对象腾出空间时，它将需要遍历所有Java对象并查找未使用的。这里要记住的要点是，<em>垃圾回收的成本与Java对象的数量成正比</em>，因此使用具有较少对象的数据结构（例如<code>Int</code>s而不是a 的数组<code>LinkedList</code>）可以大大降低此成本。更好的方法是如上所述以序列化形式持久化对象：现在将只有<em>一个</em>每个RDD分区的对象（字节数组）。在尝试其他技术之前，尝试尝试GC是否有问题的第一件事是使用<a href="http://spark.apache.org/docs/2.1.2/tuning.html#serialized-rdd-storage" target="_blank" rel="noopener">序列化缓存</a>。</p><p>由于任务的工作内存（运行任务所需的空间量）与节点上缓存的RDD之间的干扰，GC也会成为问题。我们将讨论如何控制分配给RDD缓存的空间以减轻这种情况。</p><p><strong>Measuring the Impact of GC</strong></p><p>The first step in GC tuning is to collect statistics on how frequently garbage collection occurs and the amount of time spent GC. This can be done by adding <code>-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps</code> to the Java options. (See the <a href="http://spark.apache.org/docs/2.1.2/configuration.html#Dynamically-Loading-Spark-Properties" target="_blank" rel="noopener">configuration guide</a> for info on passing Java options to Spark jobs.) Next time your Spark job is run, you will see messages printed in the worker’s logs each time a garbage collection occurs. Note these logs will be on your cluster’s worker nodes (in the <code>stdout</code> files in their work directories), <em>not</em> on your driver program.</p><p><strong>衡量GC的影响</strong></p><p>GC调整的第一步是收集有关垃圾收集发生频率和GC使用时间的统计信息。这可以通过添加<code>-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps</code>Java选项来完成。（有关将Java选项传递给Spark作业的信息，请参阅<a href="http://spark.apache.org/docs/2.1.2/configuration.html#Dynamically-Loading-Spark-Properties" target="_blank" rel="noopener">配置指南</a>。）下次运行Spark作业时，每次发生垃圾收集时，您都会在工作日志中看到打印的消息。请注意，这些日志将位于群集的工作节点上（<code>stdout</code>位于其工作目录中的文件中），<em>而不</em>位于驱动程序上。</p><p><strong>Advanced GC Tuning</strong></p><p>To further tune garbage collection, we first need to understand some basic information about memory management in the JVM:</p><ul><li>Java Heap space is divided in to two regions Young and Old. The Young generation is meant to hold short-lived objects while the Old generation is intended for objects with longer lifetimes.</li><li>The Young generation is further divided into three regions [Eden, Survivor1, Survivor2].</li><li>A simplified description of the garbage collection procedure: When Eden is full, a minor GC is run on Eden and objects that are alive from Eden and Survivor1 are copied to Survivor2. The Survivor regions are swapped. If an object is old enough or Survivor2 is full, it is moved to Old. Finally when Old is close to full, a full GC is invoked.</li></ul><p>The goal of GC tuning in Spark is to ensure that only long-lived RDDs are stored in the Old generation and that the Young generation is sufficiently sized to store short-lived objects. This will help avoid full GCs to collect temporary objects created during task execution. Some steps which may be useful are:</p><ul><li>Check if there are too many garbage collections by collecting GC stats. If a full GC is invoked multiple times for before a task completes, it means that there isn’t enough memory available for executing tasks.</li><li>If there are too many minor collections but not many major GCs, allocating more memory for Eden would help. You can set the size of the Eden to be an over-estimate of how much memory each task will need. If the size of Eden is determined to be <code>E</code>, then you can set the size of the Young generation using the option <code>-Xmn=4/3*E</code>. (The scaling up by 4/3 is to account for space used by survivor regions as well.)</li><li>In the GC stats that are printed, if the OldGen is close to being full, reduce the amount of memory used for caching by lowering <code>spark.memory.fraction</code>; it is better to cache fewer objects than to slow down task execution. Alternatively, consider decreasing the size of the Young generation. This means lowering <code>-Xmn</code> if you’ve set it as above. If not, try changing the value of the JVM’s <code>NewRatio</code> parameter. Many JVMs default this to 2, meaning that the Old generation occupies 2/3 of the heap. It should be large enough such that this fraction exceeds <code>spark.memory.fraction</code>.</li><li>Try the G1GC garbage collector with <code>-XX:+UseG1GC</code>. It can improve performance in some situations where garbage collection is a bottleneck. Note that with large executor heap sizes, it may be important to increase the <a href="https://blogs.oracle.com/g1gc/entry/g1_gc_tuning_a_case" target="_blank" rel="noopener">G1 region size</a> with <code>-XX:G1HeapRegionSize</code></li><li>As an example, if your task is reading data from HDFS, the amount of memory used by the task can be estimated using the size of the data block read from HDFS. Note that the size of a decompressed block is often 2 or 3 times the size of the block. So if we wish to have 3 or 4 tasks’ worth of working space, and the HDFS block size is 128 MB, we can estimate size of Eden to be <code>4*3*128MB</code>.</li><li>Monitor how the frequency and time taken by garbage collection changes with the new settings.</li></ul><p>Our experience suggests that the effect of GC tuning depends on your application and the amount of memory available. There are <a href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" target="_blank" rel="noopener">many more tuning options</a> described online, but at a high level, managing how frequently full GC takes place can help in reducing the overhead.</p><p>GC tuning flags for executors can be specified by setting <code>spark.executor.extraJavaOptions</code> in a job’s configuration.</p><p><strong>高级GC调整</strong></p><p>为了进一步调整垃圾回收，我们首先需要了解有关JVM中内存管理的一些基本信息：</p><ul><li>Java Heap空间分为Young和Old两个区域。年轻一代用于保存寿命短的对象，而老一代则用于寿命更长的对象。</li><li>年轻一代又分为三个区域[Eden，Survivor1，Survivor2]。</li><li>垃圾收集过程的简化描述：当Eden已满时，将在Eden上运行次要GC，并将来自Eden和Survivor1的活动对象复制到Survivor2。幸存者区域被交换。如果对象足够旧或Survivor2已满，则将其移到“旧”。最后，当Old接近满时，将调用完整的GC。</li></ul><p>在Spark中进行GC调整的目的是确保仅将长寿命的RDD存储在旧一代中，并确保年轻一代的大小足以存储短寿命的对象。这将有助于避免完整的GC收集任务执行期间创建的临时对象。可能有用的一些步骤是：</p><ul><li>通过收集GC统计信息检查是否有太多垃圾回收。如果在任务完成之前多次调用一个完整的GC，则意味着没有足够的内存来执行任务。</li><li>如果次要集合太多，但主要GC却没有很多，那么为Eden分配更多的内存将有所帮助。您可以将Eden的大小设置为高估每个任务所需的内存量。如果确定Eden的大小为<code>E</code>，则可以使用选项设置Young一代的大小<code>-Xmn=4/3*E</code>。（按4/3比例放大也是为了考虑幸存者区域使用的空间。）</li><li>在打印的GC统计信息中，如果OldGen即将满，请通过降低来减少用于缓存的内存量<code>spark.memory.fraction</code>；最好缓存较少的对象，而不是减慢任务的执行。或者，考虑减小Young代的大小。<code>-Xmn</code>如果您如上所述进行设置，则意味着降低。如果不是，请尝试更改JVM <code>NewRatio</code>参数的值。许多JVM将此默认值设置为2，这意味着旧代占据了堆的2/3。它应该足够大以使该分数超过<code>spark.memory.fraction</code>。</li><li>使用尝试G1GC垃圾收集器<code>-XX:+UseG1GC</code>。在垃圾收集成为瓶颈的某些情况下，它可以提高性能。需要注意的是大执行人堆大小，可能重要的是增加了<a href="https://blogs.oracle.com/g1gc/entry/g1_gc_tuning_a_case" target="_blank" rel="noopener">G1区域大小</a> 与<code>-XX:G1HeapRegionSize</code></li><li>例如，如果您的任务是从HDFS读取数据，则可以使用从HDFS读取的数据块的大小来估算任务使用的内存量。注意，解压缩块的大小通常是块大小的2或3倍。因此，如果我们希望拥有3或4个任务的工作空间，并且HDFS块大小为128 MB，则我们可以将Eden的大小估计为<code>4*3*128MB</code>。</li><li>使用新设置监视垃圾回收所花费的频率和时间如何变化。</li></ul><p>我们的经验表明，GC调整的效果取决于您的应用程序和可用内存量。有<a href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" target="_blank" rel="noopener">更多的微调选项</a>描述联机，但在较高的水平，管理GC如何充分频繁发生可以帮助减少开销。</p><p>可以通过<code>spark.executor.extraJavaOptions</code>在作业的配置中设置来指定执行程序的GC调整标志。</p><h1 id="Other-Considerations"><a href="#Other-Considerations" class="headerlink" title="Other Considerations"></a>Other Considerations</h1><h2 id="Level-of-Parallelism"><a href="#Level-of-Parallelism" class="headerlink" title="Level of Parallelism"></a>Level of Parallelism</h2><p>Clusters will not be fully utilized unless you set the level of parallelism for each operation high enough. Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to <code>SparkContext.textFile</code>, etc), and for distributed “reduce” operations, such as <code>groupByKey</code> and <code>reduceByKey</code>, it uses the largest parent RDD’s number of partitions. You can pass the level of parallelism as a second argument (see the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener"><code>spark.PairRDDFunctions</code></a> documentation), or set the config property <code>spark.default.parallelism</code> to change the default. In general, we recommend 2-3 tasks per CPU core in your cluster.</p><h2 id="并行度"><a href="#并行度" class="headerlink" title="并行度"></a>并行度</h2><p>除非您为每个操作设置足够高的并行度，否则群集将无法充分利用。Spark根据文件的大小自动设置要在每个文件上运行的“映射”任务的数量（尽管您可以通过可选的参数来控制它<code>SparkContext.textFile</code>，等等），并且对于分布式“减少”操作（例如<code>groupByKey</code>和）<code>reduceByKey</code>，它使用最大的父文件RDD的分区数。您可以将并行性级别作为第二个参数传递（请参阅<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener"><code>spark.PairRDDFunctions</code></a>文档），或将config属性设置<code>spark.default.parallelism</code>为更改默认值。通常，我们建议您群集中的每个CPU内核执行2-3个任务。</p><h2 id="Memory-Usage-of-Reduce-Tasks"><a href="#Memory-Usage-of-Reduce-Tasks" class="headerlink" title="Memory Usage of Reduce Tasks"></a>Memory Usage of Reduce Tasks</h2><p>Sometimes, you will get an OutOfMemoryError not because your RDDs don’t fit in memory, but because the working set of one of your tasks, such as one of the reduce tasks in <code>groupByKey</code>, was too large. Spark’s shuffle operations (<code>sortByKey</code>, <code>groupByKey</code>, <code>reduceByKey</code>, <code>join</code>, etc) build a hash table within each task to perform the grouping, which can often be large. The simplest fix here is to <em>increase the level of parallelism</em>, so that each task’s input set is smaller. Spark can efficiently support tasks as short as 200 ms, because it reuses one executor JVM across many tasks and it has a low task launching cost, so you can safely increase the level of parallelism to more than the number of cores in your clusters.</p><p>有时，您会收到OutOfMemoryError的原因不是因为您的RDD不能容纳在内存中，而是因为您的一项任务（例如中的reduce任务之一）的工作集<code>groupByKey</code>太大。斯巴克的整理操作（<code>sortByKey</code>，<code>groupByKey</code>，<code>reduceByKey</code>，<code>join</code>，等）建立每个任务中的哈希表来进行分组，而这往往是大的。这里最简单的解决方法是 <em>提高并行度</em>，以使每个任务的输入集更小。Spark可以高效地支持短至200 ms的任务，因为它可以在多个任务中重用一个执行器JVM，并且任务启动成本较低，因此您可以安全地将并行度提高到集群中内核的数量以上。</p><h2 id="Broadcasting-Large-Variables"><a href="#Broadcasting-Large-Variables" class="headerlink" title="Broadcasting Large Variables"></a>Broadcasting Large Variables</h2><p>Using the <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#broadcast-variables" target="_blank" rel="noopener">broadcast functionality</a> available in <code>SparkContext</code> can greatly reduce the size of each serialized task, and the cost of launching a job over a cluster. If your tasks use any large object from the driver program inside of them (e.g. a static lookup table), consider turning it into a broadcast variable. Spark prints the serialized size of each task on the master, so you can look at that to decide whether your tasks are too large; in general tasks larger than about 20 KB are probably worth optimizing.</p><p>利用<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#broadcast-variables" target="_blank" rel="noopener">广播功能</a> 提供<code>SparkContext</code>可大大降低每个序列化任务的大小，并在集群启动作业的成本。如果您的任务使用驱动程序中的任何大对象（例如，静态查找表），请考虑将其转换为广播变量。Spark在主服务器上打印每个任务的序列化大小，因此您可以查看它来确定任务是否太大；通常，大约20 KB以上的任务可能值得优化。</p><h2 id="Data-Locality"><a href="#Data-Locality" class="headerlink" title="Data Locality"></a>Data Locality</h2><p>Data locality can have a major impact on the performance of Spark jobs. If data and the code that operates on it are together then computation tends to be fast. But if code and data are separated, one must move to the other. Typically it is faster to ship serialized code from place to place than a chunk of data because code size is much smaller than data. Spark builds its scheduling around this general principle of data locality.</p><p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data’s current location. In order from closest to farthest:</p><ul><li><code>PROCESS_LOCAL</code> data is in the same JVM as the running code. This is the best locality possible</li><li><code>NODE_LOCAL</code> data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than <code>PROCESS_LOCAL</code> because the data has to travel between processes</li><li><code>NO_PREF</code> data is accessed equally quickly from anywhere and has no locality preference</li><li><code>RACK_LOCAL</code> data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li><li><code>ANY</code> data is elsewhere on the network and not in the same rack</li></ul><p>Spark prefers to schedule all tasks at the best locality level, but this is not always possible. In situations where there is no unprocessed data on any idle executor, Spark switches to lower locality levels. There are two options: a) wait until a busy CPU frees up to start a task on data on the same server, or b) immediately start a new task in a farther away place that requires moving data there.</p><p>What Spark typically does is wait a bit in the hopes that a busy CPU frees up. Once that timeout expires, it starts moving the data from far away to the free CPU. The wait timeout for fallback between each level can be configured individually or all together in one parameter; see the <code>spark.locality</code> parameters on the <a href="http://spark.apache.org/docs/2.1.2/configuration.html#scheduling" target="_blank" rel="noopener">configuration page</a> for details. You should increase these settings if your tasks are long and see poor locality, but the default usually works well.</p><p>数据局部性可能会对Spark作业的性能产生重大影响。如果数据和对其进行操作的代码在一起，则计算速度往往会很快。但是，如果代码和数据是分开的，那么一个必须移到另一个。通常，从一个地方到另一个地方传送序列化代码要比块数据更快，因为代码大小比数据小得多。Spark围绕此数据本地性一般原则构建调度。</p><p>数据局部性是数据与处理它的代码之间的接近程度。根据数据的当前位置，可分为多个级别。从最远到最远的顺序：</p><ul><li><code>PROCESS_LOCAL</code>数据与正在运行的代码位于同一JVM中。这是最好的位置</li><li><code>NODE_LOCAL</code>数据在同一节点上。示例可能在同一节点上的HDFS中，或者在同一节点上的另一执行程序中。这比<code>PROCESS_LOCAL</code>由于数据必须在进程之间传输而要慢一些</li><li><code>NO_PREF</code> 可以从任何地方快速访问数据，并且不受位置限制</li><li><code>RACK_LOCAL</code>数据在同一服务器机架上。数据位于同一机架上的另一台服务器上，因此通常需要通过网络通过单个交换机进行发送</li><li><code>ANY</code> 数据在网络上的其他位置，而不是在同一机架中</li></ul><p>Spark倾向于在最佳位置级别安排所有任务，但这并不总是可能的。在任何空闲执行器上没有未处理的数据的情况下，Spark会切换到较低的本地级别。有两种选择：a）等待忙碌的CPU释放以在同一服务器上的数据上启动任务，或b）立即在需要将数据移动到更远的地方启动新任务。</p><p>Spark通常要做的是稍等一下，以期释放繁忙的CPU。一旦超时到期，它将开始将数据从较远的地方移至空闲的CPU。每个级别之间的回退等待超时可以单独配置，也可以一起配置在一个参数中。有关详细信息，请参见<a href="http://spark.apache.org/docs/2.1.2/configuration.html#scheduling" target="_blank" rel="noopener">配置页面</a><code>spark.locality</code>上的 参数。如果您的任务很长并且位置不佳，则应该增加这些设置，但是默认设置通常效果很好。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>This has been a short guide to point out the main concerns you should know about when tuning a Spark application – most importantly, data serialization and memory tuning. For most programs, switching to Kryo serialization and persisting data in serialized form will solve most common performance issues. Feel free to ask on the <a href="https://spark.apache.org/community.html" target="_blank" rel="noopener">Spark mailing list</a> about other tuning best practices.</p><p>这是一个简短的指南，指出了在调整Spark应用程序时应了解的主要问题-最重要的是数据序列化和内存调整。对于大多数程序，切换到Kryo序列化并以序列化形式保留数据将解决大多数常见的性能问题。随时在 <a href="https://spark.apache.org/community.html" target="_blank" rel="noopener">Spark邮件列表中</a>询问其他调优最佳实践。</p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark系列（四）-- Spark SQL, DataFrames and Datasets Guide</title>
    <link href="/2020/07/21/Spark-SQL-DataFrames-and-Datasets-Guide/"/>
    <url>/2020/07/21/Spark-SQL-DataFrames-and-Datasets-Guide/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark-SQL-DataFrames-and-Datasets-Guide"><a href="#Spark-SQL-DataFrames-and-Datasets-Guide" class="headerlink" title="Spark SQL, DataFrames and Datasets Guide"></a>Spark SQL, DataFrames and Datasets Guide</h1><h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><p>Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API<strong>. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation.</strong> This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.</p><p>All of the examples on this page use sample data included in the Spark distribution and can be run in the <code>spark-shell</code>, <code>pyspark</code> shell, or <code>sparkR</code> shell.</p><p>Spark SQL是用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了有关数据结构和正在执行的计算的更多信息。在内部，Spark SQL使用这些额外的信息来执行额外的优化。与Spark SQL交互的方法有多种，包括SQL和Dataset API。计算结果时，将使用相同的执行引擎，而与要用来表达计算的API /语言无关。这种统一意味着开发人员可以轻松地在不同的API之间来回切换，从而提供最自然的方式来表达给定的转换。</p><p>此页面上的所有示例均使用Spark发行版中包含的示例数据，并且可以在<code>spark-shell</code>，<code>pyspark</code>shell或<code>sparkR</code>shell中运行。</p><h3 id="1-1-SQL"><a href="#1-1-SQL" class="headerlink" title="1.1 SQL"></a>1.1 SQL</h3><p>One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#hive-tables" target="_blank" rel="noopener">Hive Tables</a> section. When running SQL from within another programming language the results will be returned as a <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#datasets-and-dataframes" target="_blank" rel="noopener">Dataset/DataFrame</a>. You can also interact with the SQL interface using the <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#running-the-spark-sql-cli" target="_blank" rel="noopener">command-line</a> or over <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" target="_blank" rel="noopener">JDBC/ODBC</a>.</p><p>Spark SQL的一种用途是执行SQL查询。Spark SQL还可以用于从现有的Hive安装中读取数据。有关如何配置此功能的更多信息，请参考<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#hive-tables" target="_blank" rel="noopener">Hive Tables</a>部分。当从另一种编程语言中运行SQL时，结果将作为<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#datasets-and-dataframes" target="_blank" rel="noopener">Dataset / DataFrame</a>返回。您还可以使用<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#running-the-spark-sql-cli" target="_blank" rel="noopener">命令行</a> 或通过<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" target="_blank" rel="noopener">JDBC / ODBC</a>与SQL接口进行交互。</p><h3 id="1-2-Datasets-and-DataFrames"><a href="#1-2-Datasets-and-DataFrames" class="headerlink" title="1.2 Datasets and DataFrames"></a>1.2 Datasets and DataFrames</h3><p><strong>A Dataset is a distributed collection of data.</strong> Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#creating-datasets" target="_blank" rel="noopener">constructed</a> from JVM objects and then manipulated using functional transformations (<code>map</code>, <code>flatMap</code>, <code>filter</code>, etc.). The Dataset API is available in <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a> and <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally <code>row.columnName</code>). The case for R is similar.</p><p><strong>A DataFrame is a <em>Dataset</em> organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">sources</a> such as: structured data files, tables in Hive, external databases, or existing RDDs.</strong> The DataFrame API is available in Scala, Java, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>, and <a href="http://spark.apache.org/docs/2.1.2/api/R/index.html" target="_blank" rel="noopener">R</a>. In Scala and Java, a DataFrame is represented by a Dataset of <code>Row</code>s. In <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">the Scala API</a>, <code>DataFrame</code> is simply a type alias of <code>Dataset[Row]</code>. While, in <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java API</a>, users need to use <code>Dataset&lt;Row&gt;</code> to represent a <code>DataFrame</code>.</p><p>Throughout this document, we will often refer to Scala/Java Datasets of <code>Row</code>s as DataFrames.</p><p>数据集是数据的分布式集合。数据集是Spark 1.6中添加的新接口，它具有RDD的优点（强类型输入，使用强大的Lambda函数的能力）以及Spark SQL的优化执行引擎的优点。数据集可以<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#creating-datasets" target="_blank" rel="noopener">构成</a>从JVM对象，然后使用功能性的转换（操作<code>map</code>，<code>flatMap</code>，<code>filter</code>等等）。Dataset API在<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a>和 <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java中</a>可用。Python不支持Dataset API。但是由于Python的动态特性，Dataset API的许多优点已经可用（即，您可以自然地通过名称访问行的字段 <code>row.columnName</code>）。R的情况类似。</p><p>DataFrame是组织为命名列的<em>数据集</em>。从概念上讲，它等效于关系数据库中的表或R / Python中的数据框，但是在后台进行了更丰富的优化。可以从多种<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">来源</a>构造DataFrame，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。DataFrame API在Scala，Java，<a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>和<a href="http://spark.apache.org/docs/2.1.2/api/R/index.html" target="_blank" rel="noopener">R中</a>可用。在Scala和Java中，DataFrame由的数据集表示<code>Row</code>。在<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala API中</a>，<code>DataFrame</code>仅是类型别名<code>Dataset[Row]</code>。而在<a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java API中</a>，用户需要使用<code>Dataset&lt;Row&gt;</code>来代表<code>DataFrame</code>。</p><p>在整个文档中，我们通常将的Scala / Java数据集<code>Row</code>称为DataFrames。</p><h2 id="2-Getting-Started"><a href="#2-Getting-Started" class="headerlink" title="2. Getting Started"></a>2. Getting Started</h2><h3 id="2-1-Starting-Point-SparkSession"><a href="#2-1-Starting-Point-SparkSession" class="headerlink" title="2.1 Starting Point: SparkSession"></a>2.1 Starting Point: SparkSession</h3><p>The entry point into all functionality in Spark is the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.SparkSession" target="_blank" rel="noopener"><code>SparkSession</code></a> class. To create a basic <code>SparkSession</code>, just use <code>SparkSession.builder()</code>:</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>  .builder()  .appName(<span class="hljs-string">"Spark SQL basic example"</span>)  .config(<span class="hljs-string">"spark.some.config.option"</span>, <span class="hljs-string">"some-value"</span>)  .getOrCreate()<span class="hljs-comment">// For implicit conversions like converting RDDs to DataFrames</span><span class="hljs-keyword">import</span> spark.implicits._</code></pre><p>Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo.</p><p><code>SparkSession</code> in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables. To use these features, you do not need to have an existing Hive setup.</p><p><code>SparkSession</code>Spark 2.0中的内置支持Hive功能，包括使用HiveQL编写查询，访问Hive UDF以及从Hive表读取数据的功能。要使用这些功能，您不需要现有的Hive设置。</p><h3 id="2-2-Creating-DataFrames"><a href="#2-2-Creating-DataFrames" class="headerlink" title="2.2 Creating DataFrames"></a>2.2 Creating DataFrames</h3><p>With a <code>SparkSession</code>, applications can create DataFrames from an <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">existing <code>RDD</code></a>, from a Hive table, or from <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">Spark data sources</a>.</p><p>As an example, the following creates a DataFrame based on the content of a JSON file:</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> df = spark.read.json(<span class="hljs-string">"examples/src/main/resources/people.json"</span>)<span class="hljs-comment">// Displays the content of the DataFrame to stdout</span>df.show()<span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// | age|   name|</span><span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// |null|Michael|</span><span class="hljs-comment">// |  30|   Andy|</span><span class="hljs-comment">// |  19| Justin|</span><span class="hljs-comment">// +----+-------+</span></code></pre><p>Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo.</p><h3 id="2-3-Untyped-Dataset-Operations-aka-DataFrame-Operations"><a href="#2-3-Untyped-Dataset-Operations-aka-DataFrame-Operations" class="headerlink" title="2.3 Untyped Dataset Operations (aka DataFrame Operations)"></a>2.3 Untyped Dataset Operations (aka DataFrame Operations)</h3><p>DataFrames provide a domain-specific language for structured data manipulation in <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a> and <a href="http://spark.apache.org/docs/2.1.2/api/R/SparkDataFrame.html" target="_blank" rel="noopener">R</a>.</p><p>As mentioned above, in Spark 2.0, DataFrames are just Dataset of <code>Row</code>s in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.</p><p>Here we include some basic examples of structured data processing using Datasets:</p><p>DataFrame为<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a>，<a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>，<a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>和<a href="http://spark.apache.org/docs/2.1.2/api/R/SparkDataFrame.html" target="_blank" rel="noopener">R中的</a>结构化数据操作提供了一种特定于域的语言。</p><p>如上所述，在Spark 2.0中，DataFrames只是<code>Row</code>Scala和Java API中的的数据集。与强类型的Scala / Java数据集附带的“类型转换”相反，这些操作也称为“非类型转换”。</p><p>这里我们包括一些使用数据集进行结构化数据处理的基本示例：</p><pre><code class="hljs scala"><span class="hljs-comment">// This import is needed to use the $-notation</span><span class="hljs-keyword">import</span> spark.implicits._<span class="hljs-comment">// Print the schema in a tree format</span>df.printSchema()<span class="hljs-comment">// root</span><span class="hljs-comment">// |-- age: long (nullable = true)</span><span class="hljs-comment">// |-- name: string (nullable = true)</span><span class="hljs-comment">// Select only the "name" column</span>df.select(<span class="hljs-string">"name"</span>).show()<span class="hljs-comment">// +-------+</span><span class="hljs-comment">// |   name|</span><span class="hljs-comment">// +-------+</span><span class="hljs-comment">// |Michael|</span><span class="hljs-comment">// |   Andy|</span><span class="hljs-comment">// | Justin|</span><span class="hljs-comment">// +-------+</span><span class="hljs-comment">// Select everybody, but increment the age by 1</span>df.select($<span class="hljs-string">"name"</span>, $<span class="hljs-string">"age"</span> + <span class="hljs-number">1</span>).show()<span class="hljs-comment">// +-------+---------+</span><span class="hljs-comment">// |   name|(age + 1)|</span><span class="hljs-comment">// +-------+---------+</span><span class="hljs-comment">// |Michael|     null|</span><span class="hljs-comment">// |   Andy|       31|</span><span class="hljs-comment">// | Justin|       20|</span><span class="hljs-comment">// +-------+---------+</span><span class="hljs-comment">// Select people older than 21</span>df.filter($<span class="hljs-string">"age"</span> &gt; <span class="hljs-number">21</span>).show()<span class="hljs-comment">// +---+----+</span><span class="hljs-comment">// |age|name|</span><span class="hljs-comment">// +---+----+</span><span class="hljs-comment">// | 30|Andy|</span><span class="hljs-comment">// +---+----+</span><span class="hljs-comment">// Count people by age</span>df.groupBy(<span class="hljs-string">"age"</span>).count().show()<span class="hljs-comment">// +----+-----+</span><span class="hljs-comment">// | age|count|</span><span class="hljs-comment">// +----+-----+</span><span class="hljs-comment">// |  19|    1|</span><span class="hljs-comment">// |null|    1|</span><span class="hljs-comment">// |  30|    1|</span><span class="hljs-comment">// +----+-----+</span></code></pre><p>Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo.</p><p>For a complete list of the types of operations that can be performed on a Dataset refer to the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">API Documentation</a>.</p><p>In addition to simple column references and expressions, Datasets also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank" rel="noopener">DataFrame Function Reference</a>.</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">QuickExample</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<span class="hljs-comment">//    val sparkSession = SparkSession</span><span class="hljs-comment">//                          .builder()</span><span class="hljs-comment">//                          .appName("QuickExample")</span><span class="hljs-comment">//                          .config("spark.some.config.option", "some-value")</span><span class="hljs-comment">//                          .getOrCreate()</span>    <span class="hljs-comment">//报错 org.apache.spark.SparkException: A master URL must be set in your configuration</span>    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"QuickExample"</span>).setMaster(<span class="hljs-string">"local[*]"</span>)    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>                          .builder()                          .appName(<span class="hljs-string">"QuickExample"</span>)                          .config(sparkConf)                          .getOrCreate()    <span class="hljs-keyword">import</span> sparkSession.implicits._    <span class="hljs-keyword">val</span> dataFrame = sparkSession.read.json(<span class="hljs-string">"src\\main\\resources\\people.json"</span>)    dataFrame.show()<span class="hljs-comment">//    +----+-------+</span><span class="hljs-comment">//    | age|   name|</span><span class="hljs-comment">//    +----+-------+</span><span class="hljs-comment">//    |null|Michael|</span><span class="hljs-comment">//    |  30|   Andy|</span><span class="hljs-comment">//    |  19| Justin|</span><span class="hljs-comment">//    +----+-------+</span>    dataFrame.printSchema()<span class="hljs-comment">//    root</span><span class="hljs-comment">//    |-- age: long (nullable = true)</span><span class="hljs-comment">//    |-- name: string (nullable = true)</span>    dataFrame.select(<span class="hljs-string">"name"</span>).show()<span class="hljs-comment">//    +-------+</span><span class="hljs-comment">//    |   name|</span><span class="hljs-comment">//    +-------+</span><span class="hljs-comment">//    |Michael|</span><span class="hljs-comment">//    |   Andy|</span><span class="hljs-comment">//    | Justin|</span><span class="hljs-comment">//    +-------+</span>    <span class="hljs-comment">//dataFrame.select("name", "age"+1).show() 会报错</span>    dataFrame.select($<span class="hljs-string">"name"</span>,$<span class="hljs-string">"age"</span>+<span class="hljs-number">1</span>).show()<span class="hljs-comment">//    +-------+---------+</span><span class="hljs-comment">//    |   name|(age + 1)|</span><span class="hljs-comment">//    +-------+---------+</span><span class="hljs-comment">//    |Michael|     null|</span><span class="hljs-comment">//    |   Andy|       31|</span><span class="hljs-comment">//    | Justin|       20|</span><span class="hljs-comment">//    +-------+---------+</span>    dataFrame.filter($<span class="hljs-string">"age"</span> &gt; <span class="hljs-number">21</span>).show()<span class="hljs-comment">//    +---+----+</span><span class="hljs-comment">//    |age|name|</span><span class="hljs-comment">//    +---+----+</span><span class="hljs-comment">//    | 30|Andy|</span><span class="hljs-comment">//    +---+----+</span>    dataFrame.groupBy(<span class="hljs-string">"age"</span>).count().show()<span class="hljs-comment">//    +----+-----+</span><span class="hljs-comment">//    | age|count|</span><span class="hljs-comment">//    +----+-----+</span><span class="hljs-comment">//    |  19|    1|</span><span class="hljs-comment">//    |null|    1|</span><span class="hljs-comment">//    |  30|    1|</span><span class="hljs-comment">//    +----+-----+</span>  &#125;&#125;</code></pre><h3 id="2-4-Running-SQL-Queries-Programmatically"><a href="#2-4-Running-SQL-Queries-Programmatically" class="headerlink" title="2.4 Running SQL Queries Programmatically"></a>2.4 Running SQL Queries Programmatically</h3><p>The <code>sql</code> function on a <code>SparkSession</code> enables applications to run SQL queries programmatically and returns the result as a <code>DataFrame</code>.</p><p>上的<code>sql</code>函数<code>SparkSession</code>使应用程序能够以编程方式运行SQL查询，并以形式返回结果<code>DataFrame</code>。</p><pre><code class="hljs scala"><span class="hljs-comment">// Register the DataFrame as a SQL temporary view</span>df.createOrReplaceTempView(<span class="hljs-string">"people"</span>)<span class="hljs-keyword">val</span> sqlDF = spark.sql(<span class="hljs-string">"SELECT * FROM people"</span>)sqlDF.show()<span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// | age|   name|</span><span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// |null|Michael|</span><span class="hljs-comment">// |  30|   Andy|</span><span class="hljs-comment">// |  19| Justin|</span><span class="hljs-comment">// +----+-------+</span></code></pre><h3 id="2-5-Global-Temporary-View"><a href="#2-5-Global-Temporary-View" class="headerlink" title="2.5 Global Temporary View"></a>2.5 Global Temporary View</h3><p>Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database <code>global_temp</code>, and we must use the qualified name to refer it, e.g. <code>SELECT * FROM global_temp.view1</code>.</p><p>Spark SQL中的临时视图是会话作用域的，如果创建它的会话终止，它将消失。如果要在所有会话之间共享一个临时视图并保持活动状态，直到Spark应用程序终止，则可以创建全局临时视图。全局临时视图与系统保留的数据库相关联<code>global_temp</code>，我们必须使用限定名称来引用它，例如<code>SELECT * FROM global_temp.view1</code>。</p><pre><code class="hljs scala"><span class="hljs-comment">// Register the DataFrame as a global temporary view</span>df.createGlobalTempView(<span class="hljs-string">"people"</span>)<span class="hljs-comment">// Global temporary view is tied to a system preserved database `global_temp`</span>spark.sql(<span class="hljs-string">"SELECT * FROM global_temp.people"</span>).show()<span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// | age|   name|</span><span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// |null|Michael|</span><span class="hljs-comment">// |  30|   Andy|</span><span class="hljs-comment">// |  19| Justin|</span><span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// Global temporary view is cross-session</span>spark.newSession().sql(<span class="hljs-string">"SELECT * FROM global_temp.people"</span>).show()<span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// | age|   name|</span><span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// |null|Michael|</span><span class="hljs-comment">// |  30|   Andy|</span><span class="hljs-comment">// |  19| Justin|</span><span class="hljs-comment">// +----+-------+</span></code></pre><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-type">SparkConf</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">SQLExample</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"SQLExample"</span>).setMaster(<span class="hljs-string">"local[2]"</span>)    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()    <span class="hljs-keyword">val</span> dataFrame = sparkSession.read.json(<span class="hljs-string">"src\\main\\resources\\people.json"</span>)    <span class="hljs-comment">// Register the DataFrame as a SQL temporary view</span>    dataFrame.createOrReplaceTempView(<span class="hljs-string">"people"</span>)    <span class="hljs-keyword">val</span> frame = sparkSession.sql(<span class="hljs-string">"select * from people"</span>)    frame.show()    <span class="hljs-comment">//新创建一个session，报错，找不到 Table or view not found: people;</span>    <span class="hljs-comment">//sparkSession.newSession().sql("select * from people").show()</span>    <span class="hljs-comment">//20/07/21 11:20:04 INFO SparkSqlParser: Parsing command: select * from people</span>    <span class="hljs-comment">//Exception in thread "main" org.apache.spark.sql.AnalysisException: Table or view not found: people; line 1 pos 14</span>    <span class="hljs-comment">//注册成全局视图，只有spark程序关闭时，才会失效</span>    <span class="hljs-comment">//Global temporary view is tied to a system preserved database global_temp,</span>    <span class="hljs-comment">// and we must use the qualified name to refer it,</span>    <span class="hljs-comment">// e.g. SELECT * FROM global_temp.view1.</span>    <span class="hljs-comment">//类似放在global_temp库下了，访问时候把库名加上，不然找不到表</span>    dataFrame.createGlobalTempView(<span class="hljs-string">"glo_people"</span>)    sparkSession.sql(<span class="hljs-string">"select * from global_temp.glo_people"</span>).show()    sparkSession.newSession().sql(<span class="hljs-string">"select * from global_temp.glo_people"</span>).show()  &#125;&#125;</code></pre><h3 id="2-6-Creating-Datasets"><a href="#2-6-Creating-Datasets" class="headerlink" title="2.6 Creating Datasets"></a>2.6 Creating Datasets</h3><p>Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Encoder" target="_blank" rel="noopener">Encoder</a> to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.</p><p>数据集类似于RDD，但是，它们不使用Java序列化或Kryo，而是使用专用的<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Encoder" target="_blank" rel="noopener">Encoder</a>对对象进行序列化以进行处理或通过网络传输。虽然编码器和标准序列化都负责将对象转换为字节，但是编码器是动态生成的代码，并使用一种格式，该格式允许Spark执行许多操作，如过滤，排序和哈希处理，而无需将字节反序列化回对象。</p><pre><code class="hljs scala"><span class="hljs-comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span><span class="hljs-comment">// you can use custom classes that implement the Product interface</span><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, age: <span class="hljs-type">Long</span></span>)</span><span class="hljs-class"></span><span class="hljs-class"><span class="hljs-title">//</span> <span class="hljs-title">Encoders</span> <span class="hljs-title">are</span> <span class="hljs-title">created</span> <span class="hljs-title">for</span> <span class="hljs-title">case</span> <span class="hljs-title">classes</span></span><span class="hljs-class"><span class="hljs-title">val</span> <span class="hljs-title">caseClassDS</span> </span>= <span class="hljs-type">Seq</span>(<span class="hljs-type">Person</span>(<span class="hljs-string">"Andy"</span>, <span class="hljs-number">32</span>)).toDS()caseClassDS.show()<span class="hljs-comment">// +----+---+</span><span class="hljs-comment">// |name|age|</span><span class="hljs-comment">// +----+---+</span><span class="hljs-comment">// |Andy| 32|</span><span class="hljs-comment">// +----+---+</span><span class="hljs-comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span><span class="hljs-keyword">val</span> primitiveDS = <span class="hljs-type">Seq</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>).toDS()primitiveDS.map(_ + <span class="hljs-number">1</span>).collect() <span class="hljs-comment">// Returns: Array(2, 3, 4)</span><span class="hljs-comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span><span class="hljs-keyword">val</span> path = <span class="hljs-string">"examples/src/main/resources/people.json"</span><span class="hljs-keyword">val</span> peopleDS = spark.read.json(path).as[<span class="hljs-type">Person</span>]peopleDS.show()<span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// | age|   name|</span><span class="hljs-comment">// +----+-------+</span><span class="hljs-comment">// |null|Michael|</span><span class="hljs-comment">// |  30|   Andy|</span><span class="hljs-comment">// |  19| Justin|</span><span class="hljs-comment">// +----+-------+</span></code></pre><h3 id="2-7-Interoperating-with-RDDs"><a href="#2-7-Interoperating-with-RDDs" class="headerlink" title="2.7 Interoperating with RDDs"></a>2.7 Interoperating with RDDs</h3><p>Spark SQL supports two different methods for converting existing RDDs into Datasets. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.</p><p>The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime.</p><p>Spark SQL支持两种将现有RDD转换为数据集的方法。第一种方法使用反射来推断包含特定对象类型的RDD的架构。这种基于反射的方法可以使代码更简洁，当您在编写Spark应用程序时已经了解架构时，可以很好地工作。</p><p>创建数据集的第二种方法是通过编程界面，该界面允许您构造模式，然后将其应用于现有的RDD。尽管此方法较为冗长，但可以在运行时才知道列及其类型的情况下构造数据集。</p><h4 id="2-7-1-Inferring-the-Schema-Using-Reflection"><a href="#2-7-1-Inferring-the-Schema-Using-Reflection" class="headerlink" title="2.7.1 Inferring the Schema Using Reflection"></a>2.7.1 Inferring the Schema Using Reflection</h4><p>The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as <code>Seq</code>s or <code>Array</code>s. This RDD can be implicitly converted to a DataFrame and then be registered as a table. Tables can be used in subsequent SQL statements.</p><p>Spark SQL的Scala接口支持将包含样例类的RDD自动转换为DataFrame。样例类定义表的结构。样例类的参数名称使用反射读取，并成为列的名称。Case类也可以嵌套或包含<code>Seq</code>s或<code>Array</code>s之类的复杂类型。可以将该RDD隐式转换为DataFrame，然后将其注册为表。表可以在后续的SQL语句中使用。</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.catalyst.encoders.<span class="hljs-type">ExpressionEncoder</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoder</span><span class="hljs-comment">// For implicit conversions from RDDs to DataFrames</span><span class="hljs-keyword">import</span> spark.implicits._<span class="hljs-comment">// Create an RDD of Person objects from a text file, convert it to a Dataframe</span><span class="hljs-keyword">val</span> peopleDF = spark.sparkContext  .textFile(<span class="hljs-string">"examples/src/main/resources/people.txt"</span>)  .map(_.split(<span class="hljs-string">","</span>))  .map(attributes =&gt; <span class="hljs-type">Person</span>(attributes(<span class="hljs-number">0</span>), attributes(<span class="hljs-number">1</span>).trim.toInt))  .toDF()<span class="hljs-comment">// Register the DataFrame as a temporary view</span>peopleDF.createOrReplaceTempView(<span class="hljs-string">"people"</span>)<span class="hljs-comment">// SQL statements can be run by using the sql methods provided by Spark</span><span class="hljs-keyword">val</span> teenagersDF = spark.sql(<span class="hljs-string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)<span class="hljs-comment">// The columns of a row in the result can be accessed by field index</span>teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager(<span class="hljs-number">0</span>)).show()<span class="hljs-comment">// +------------+</span><span class="hljs-comment">// |       value|</span><span class="hljs-comment">// +------------+</span><span class="hljs-comment">// |Name: Justin|</span><span class="hljs-comment">// +------------+</span><span class="hljs-comment">// or by field name</span>teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager.getAs[<span class="hljs-type">String</span>](<span class="hljs-string">"name"</span>)).show()<span class="hljs-comment">// +------------+</span><span class="hljs-comment">// |       value|</span><span class="hljs-comment">// +------------+</span><span class="hljs-comment">// |Name: Justin|</span><span class="hljs-comment">// +------------+</span><span class="hljs-comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span><span class="hljs-keyword">implicit</span> <span class="hljs-keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="hljs-type">Encoders</span>.kryo[<span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">Any</span>]]<span class="hljs-comment">// Primitive types and case classes can be also defined as</span><span class="hljs-comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span><span class="hljs-comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="hljs-type">Any</span>](<span class="hljs-type">List</span>(<span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>))).collect()<span class="hljs-comment">// Array(Map("name" -&gt; "Justin", "age" -&gt; 19))</span></code></pre><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-type">SparkConf</span><span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;<span class="hljs-type">DataFrame</span>, <span class="hljs-type">Dataset</span>, <span class="hljs-type">SparkSession</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">RDD2DS</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">"local[*]"</span>)    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()    <span class="hljs-keyword">val</span> dataSet: <span class="hljs-type">Dataset</span>[<span class="hljs-type">String</span>] = sparkSession.read.textFile(<span class="hljs-string">"src\\main\\resources\\people.txt"</span>)    <span class="hljs-keyword">val</span> dataRdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sparkSession.sparkContext.textFile(<span class="hljs-string">"src\\main\\resources\\people.txt"</span>)    dataRdd.foreach(println)<span class="hljs-comment">//    Justin, 19</span><span class="hljs-comment">//    Michael, 29</span><span class="hljs-comment">//    Andy, 30</span>    dataSet.show()    <span class="hljs-comment">// 自动转成dataset,类型不对，是string应该</span><span class="hljs-comment">//    +-----------+</span><span class="hljs-comment">//    |      value|</span><span class="hljs-comment">//    +-----------+</span><span class="hljs-comment">//    |Michael, 29|</span><span class="hljs-comment">//    |   Andy, 30|</span><span class="hljs-comment">//    | Justin, 19|</span><span class="hljs-comment">//    +-----------+</span>    <span class="hljs-comment">//将这个RDD转成dateset</span>    <span class="hljs-keyword">val</span> dataRddPerson: <span class="hljs-type">RDD</span>[<span class="hljs-type">Person</span>] = dataRdd.map(_.split(<span class="hljs-string">","</span>))          .map(attributes =&gt; <span class="hljs-type">Person</span>(attributes(<span class="hljs-number">0</span>), attributes(<span class="hljs-number">1</span>).trim.toInt))    dataRddPerson.foreach(println)<span class="hljs-comment">//    Person(Justin,19)</span><span class="hljs-comment">//    Person(Michael,29)</span><span class="hljs-comment">//    Person(Andy,30)</span>    <span class="hljs-keyword">import</span> sparkSession.implicits._    <span class="hljs-keyword">val</span> dataSetPerson: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Person</span>] = dataRddPerson.toDS()    dataSetPerson.show()<span class="hljs-comment">//    +-------+---+</span><span class="hljs-comment">//    |   name|age|</span><span class="hljs-comment">//    +-------+---+</span><span class="hljs-comment">//    |Michael| 29|</span><span class="hljs-comment">//    |   Andy| 30|</span><span class="hljs-comment">//    | Justin| 19|</span><span class="hljs-comment">//    +-------+---+</span>    dataSetPerson.createOrReplaceTempView(<span class="hljs-string">"people"</span>)    <span class="hljs-keyword">val</span> teenagersDF: <span class="hljs-type">DataFrame</span> = sparkSession.sql(<span class="hljs-string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)    teenagersDF.show()<span class="hljs-comment">//    +------+---+</span><span class="hljs-comment">//    |  name|age|</span><span class="hljs-comment">//    +------+---+</span><span class="hljs-comment">//    |Justin| 19|</span><span class="hljs-comment">//    +------+---+</span>    <span class="hljs-comment">// The columns of a row in the result can be accessed by field index</span>    teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager(<span class="hljs-number">0</span>)).show()<span class="hljs-comment">//    +------------+</span><span class="hljs-comment">//    |       value|</span><span class="hljs-comment">//    +------------+</span><span class="hljs-comment">//    |Name: Justin|</span><span class="hljs-comment">//    +------------+</span>    teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager(<span class="hljs-number">0</span>) + <span class="hljs-string">"  age: "</span>+ teenager(<span class="hljs-number">1</span>)).show()<span class="hljs-comment">//    +--------------------+</span><span class="hljs-comment">//    |               value|</span><span class="hljs-comment">//    +--------------------+</span><span class="hljs-comment">//    |Name: Justin  Age...|</span><span class="hljs-comment">//    +--------------------+</span>    <span class="hljs-comment">//为什么age变成...</span>    <span class="hljs-comment">// or by field name</span>    teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager.getAs[<span class="hljs-type">String</span>](<span class="hljs-string">"name"</span>)).show()<span class="hljs-comment">//    +------------+</span><span class="hljs-comment">//    |       value|</span><span class="hljs-comment">//    +------------+</span><span class="hljs-comment">//    |Name: Justin|</span><span class="hljs-comment">//    +------------+</span>    teenagersDF.map(teenagers =&gt; teenagers.getAs[<span class="hljs-type">Long</span>](<span class="hljs-string">"age"</span>)).show()<span class="hljs-comment">//    +-----+</span><span class="hljs-comment">//    |value|</span><span class="hljs-comment">//    +-----+</span><span class="hljs-comment">//    |   19|</span><span class="hljs-comment">//    +-----+</span>    <span class="hljs-comment">// 不太理解</span>    <span class="hljs-comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span>    <span class="hljs-keyword">implicit</span> <span class="hljs-keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="hljs-type">Encoders</span>.kryo[<span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">Any</span>]]    <span class="hljs-comment">// Primitive types and case classes can be also defined as</span>    <span class="hljs-comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span>    <span class="hljs-comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>    teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="hljs-type">Any</span>](<span class="hljs-type">List</span>(<span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>))).collect()    <span class="hljs-comment">// Array(Map("name" -&gt; "Justin", "age" -&gt; 19))</span>  &#125;&#125;</code></pre><h4 id="2-7-2-Programmatically-Specifying-the-Schema"><a href="#2-7-2-Programmatically-Specifying-the-Schema" class="headerlink" title="2.7.2 Programmatically Specifying the Schema"></a>2.7.2 Programmatically Specifying the Schema</h4><p>When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a <code>DataFrame</code> can be created programmatically with three steps.</p><ol><li>Create an RDD of <code>Row</code>s from the original RDD;</li><li>Create the schema represented by a <code>StructType</code> matching the structure of <code>Row</code>s in the RDD created in Step 1.</li><li>Apply the schema to the RDD of <code>Row</code>s via <code>createDataFrame</code> method provided by <code>SparkSession</code>.</li></ol><p>如果无法提前定义样例类（例如，记录的结构编码为字符串，或者将解析文本数据集，并且针对不同的用户对字段进行不同的投影），<code>DataFrame</code>则可以通过三个步骤以编程方式创建 。</p><ol><li>从原始RDD 创建一个的RDD；</li><li>在第1步中创建的RDD中创建<code>StructType</code>与<code>Row</code>s 的结构匹配 表示的模式。</li><li><code>Row</code>通过<code>createDataFrame</code>提供的方法将架构应用于的RDD <code>SparkSession</code>。</li></ol><p>For example:</p><pre><code class="hljs scala"></code></pre><h3 id="2-8-Aggregations"><a href="#2-8-Aggregations" class="headerlink" title="2.8 Aggregations"></a>2.8 Aggregations</h3><p>The <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank" rel="noopener">built-in DataFrames functions</a> provide common aggregations such as <code>count()</code>, <code>countDistinct()</code>, <code>avg()</code>, <code>max()</code>, <code>min()</code>, etc. While those functions are designed for DataFrames, Spark SQL also has type-safe versions for some of them in <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$" target="_blank" rel="noopener">Scala</a> and <a href="http://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/sql/expressions/javalang/typed.html" target="_blank" rel="noopener">Java</a> to work with strongly typed Datasets. Moreover, users are not limited to the predefined aggregate functions and can create their own.</p><p>该<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank" rel="noopener">内置功能DataFrames</a>提供共同聚合，例如<code>count()</code>，<code>countDistinct()</code>，<code>avg()</code>，<code>max()</code>，<code>min()</code>，等。虽然这些功能是专为DataFrames，星火SQL也有类型安全的版本为他们中的一些 <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$" target="_blank" rel="noopener">斯卡拉</a>和 <a href="http://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/sql/expressions/javalang/typed.html" target="_blank" rel="noopener">Java的</a>与强类型数据集的工作。此外，用户不限于预定义的聚合功能，还可以创建自己的功能。</p><h4 id="2-8-1-Untyped-User-Defined-Aggregate-Functions"><a href="#2-8-1-Untyped-User-Defined-Aggregate-Functions" class="headerlink" title="2.8.1 Untyped User-Defined Aggregate Functions"></a>2.8.1 Untyped User-Defined Aggregate Functions</h4><p>Users have to extend the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" target="_blank" rel="noopener">UserDefinedAggregateFunction</a> abstract class to implement a custom untyped aggregate function. For example, a user-defined average can look like:</p><p>用户必须扩展<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" target="_blank" rel="noopener">UserDefinedAggregateFunction</a> 抽象类以实现自定义无类型的聚合函数。例如，用户定义的平均值如下所示：</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">MutableAggregationBuffer</span><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">UserDefinedAggregateFunction</span><span class="hljs-keyword">import</span> org.apache.spark.sql.types._<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Row</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">MyAverage</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">UserDefinedAggregateFunction</span> </span>&#123;  <span class="hljs-comment">// Data types of input arguments of this aggregate function</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inputSchema</span></span>: <span class="hljs-type">StructType</span> = <span class="hljs-type">StructType</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">"inputColumn"</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">Nil</span>)  <span class="hljs-comment">// Data types of values in the aggregation buffer</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferSchema</span></span>: <span class="hljs-type">StructType</span> = &#123;    <span class="hljs-type">StructType</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">"sum"</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">StructField</span>(<span class="hljs-string">"count"</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">Nil</span>)  &#125;  <span class="hljs-comment">// The data type of the returned value</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataType</span></span>: <span class="hljs-type">DataType</span> = <span class="hljs-type">DoubleType</span>  <span class="hljs-comment">// Whether this function always returns the same output on the identical input</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deterministic</span></span>: <span class="hljs-type">Boolean</span> = <span class="hljs-literal">true</span>  <span class="hljs-comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span>  <span class="hljs-comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span>  <span class="hljs-comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span>  <span class="hljs-comment">// immutable.</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>): <span class="hljs-type">Unit</span> = &#123;    buffer(<span class="hljs-number">0</span>) = <span class="hljs-number">0</span>L    buffer(<span class="hljs-number">1</span>) = <span class="hljs-number">0</span>L  &#125;  <span class="hljs-comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>, input: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">if</span> (!input.isNullAt(<span class="hljs-number">0</span>)) &#123;      buffer(<span class="hljs-number">0</span>) = buffer.getLong(<span class="hljs-number">0</span>) + input.getLong(<span class="hljs-number">0</span>)      buffer(<span class="hljs-number">1</span>) = buffer.getLong(<span class="hljs-number">1</span>) + <span class="hljs-number">1</span>    &#125;  &#125;  <span class="hljs-comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(buffer1: <span class="hljs-type">MutableAggregationBuffer</span>, buffer2: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = &#123;    buffer1(<span class="hljs-number">0</span>) = buffer1.getLong(<span class="hljs-number">0</span>) + buffer2.getLong(<span class="hljs-number">0</span>)    buffer1(<span class="hljs-number">1</span>) = buffer1.getLong(<span class="hljs-number">1</span>) + buffer2.getLong(<span class="hljs-number">1</span>)  &#125;  <span class="hljs-comment">// Calculates the final result</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span></span>(buffer: <span class="hljs-type">Row</span>): <span class="hljs-type">Double</span> = buffer.getLong(<span class="hljs-number">0</span>).toDouble / buffer.getLong(<span class="hljs-number">1</span>)&#125;<span class="hljs-comment">// Register the function to access it</span>spark.udf.register(<span class="hljs-string">"myAverage"</span>, <span class="hljs-type">MyAverage</span>)<span class="hljs-keyword">val</span> df = spark.read.json(<span class="hljs-string">"examples/src/main/resources/employees.json"</span>)df.createOrReplaceTempView(<span class="hljs-string">"employees"</span>)df.show()<span class="hljs-comment">// +-------+------+</span><span class="hljs-comment">// |   name|salary|</span><span class="hljs-comment">// +-------+------+</span><span class="hljs-comment">// |Michael|  3000|</span><span class="hljs-comment">// |   Andy|  4500|</span><span class="hljs-comment">// | Justin|  3500|</span><span class="hljs-comment">// |  Berta|  4000|</span><span class="hljs-comment">// +-------+------+</span><span class="hljs-keyword">val</span> result = spark.sql(<span class="hljs-string">"SELECT myAverage(salary) as average_salary FROM employees"</span>)result.show()<span class="hljs-comment">// +--------------+</span><span class="hljs-comment">// |average_salary|</span><span class="hljs-comment">// +--------------+</span><span class="hljs-comment">// |        3750.0|</span><span class="hljs-comment">// +--------------+</span></code></pre><h4 id="2-8-2-Type-Safe-User-Defined-Aggregate-Functions"><a href="#2-8-2-Type-Safe-User-Defined-Aggregate-Functions" class="headerlink" title="2.8.2 Type-Safe User-Defined Aggregate Functions"></a>2.8.2 Type-Safe User-Defined Aggregate Functions</h4><p>User-defined aggregations for strongly typed Datasets revolve around the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator" target="_blank" rel="noopener">Aggregator</a> abstract class. For example, a type-safe user-defined average can look like:</p><p>用户定义的强类型数据集的<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator" target="_blank" rel="noopener">聚合</a>围绕<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator" target="_blank" rel="noopener">Aggregator</a>抽象类展开。例如，类型安全的用户定义的平均值如下所示：</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">Aggregator</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoder</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoders</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Employee</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, salary: <span class="hljs-type">Long</span></span>)</span><span class="hljs-class"><span class="hljs-title">case</span> <span class="hljs-title">class</span> <span class="hljs-title">Average</span>(<span class="hljs-params">var sum: <span class="hljs-type">Long</span>, var count: <span class="hljs-type">Long</span></span>)</span><span class="hljs-class"></span><span class="hljs-class"><span class="hljs-title">object</span> <span class="hljs-title">MyAverage</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Aggregator</span>[<span class="hljs-type">Employee</span>, <span class="hljs-type">Average</span>, <span class="hljs-type">Double</span>] </span>&#123;  <span class="hljs-comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zero</span></span>: <span class="hljs-type">Average</span> = <span class="hljs-type">Average</span>(<span class="hljs-number">0</span>L, <span class="hljs-number">0</span>L)  <span class="hljs-comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span>  <span class="hljs-comment">// and return it instead of constructing a new object</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reduce</span></span>(buffer: <span class="hljs-type">Average</span>, employee: <span class="hljs-type">Employee</span>): <span class="hljs-type">Average</span> = &#123;    buffer.sum += employee.salary    buffer.count += <span class="hljs-number">1</span>    buffer  &#125;  <span class="hljs-comment">// Merge two intermediate values</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(b1: <span class="hljs-type">Average</span>, b2: <span class="hljs-type">Average</span>): <span class="hljs-type">Average</span> = &#123;    b1.sum += b2.sum    b1.count += b2.count    b1  &#125;  <span class="hljs-comment">// Transform the output of the reduction</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">finish</span></span>(reduction: <span class="hljs-type">Average</span>): <span class="hljs-type">Double</span> = reduction.sum.toDouble / reduction.count  <span class="hljs-comment">// Specifies the Encoder for the intermediate value type</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Average</span>] = <span class="hljs-type">Encoders</span>.product  <span class="hljs-comment">// Specifies the Encoder for the final output value type</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outputEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Double</span>] = <span class="hljs-type">Encoders</span>.scalaDouble&#125;<span class="hljs-keyword">val</span> ds = spark.read.json(<span class="hljs-string">"examples/src/main/resources/employees.json"</span>).as[<span class="hljs-type">Employee</span>]ds.show()<span class="hljs-comment">// +-------+------+</span><span class="hljs-comment">// |   name|salary|</span><span class="hljs-comment">// +-------+------+</span><span class="hljs-comment">// |Michael|  3000|</span><span class="hljs-comment">// |   Andy|  4500|</span><span class="hljs-comment">// | Justin|  3500|</span><span class="hljs-comment">// |  Berta|  4000|</span><span class="hljs-comment">// +-------+------+</span><span class="hljs-comment">// Convert the function to a `TypedColumn` and give it a name</span><span class="hljs-keyword">val</span> averageSalary = <span class="hljs-type">MyAverage</span>.toColumn.name(<span class="hljs-string">"average_salary"</span>)<span class="hljs-keyword">val</span> result = ds.select(averageSalary)result.show()<span class="hljs-comment">// +--------------+</span><span class="hljs-comment">// |average_salary|</span><span class="hljs-comment">// +--------------+</span><span class="hljs-comment">// |        3750.0|</span><span class="hljs-comment">// +--------------+</span></code></pre><h2 id="3-Data-Sources"><a href="#3-Data-Sources" class="headerlink" title="3. Data Sources"></a>3. Data Sources</h2><p>Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data. This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources.</p><p>Spark SQL支持通过DataFrame接口对各种数据源进行操作。DataFrame可以使用关系转换进行操作，也可以用于创建临时视图。将DataFrame注册为临时视图使您可以对其数据运行SQL查询。本节介绍了使用Spark数据源加载和保存数据的一般方法，然后介绍了可用于内置数据源的特定选项。</p><h3 id="3-1-Generic-Load-Save-Functions"><a href="#3-1-Generic-Load-Save-Functions" class="headerlink" title="3.1 Generic Load/Save Functions"></a>3.1 Generic Load/Save Functions</h3><p>In the simplest form, the default data source (<code>parquet</code> unless otherwise configured by <code>spark.sql.sources.default</code>) will be used for all operations.</p><p>以最简单的形式，所有操作都将使用默认数据源（<code>parquet</code>除非另有配置 <code>spark.sql.sources.default</code>）</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> usersDF = spark.read.load(<span class="hljs-string">"examples/src/main/resources/users.parquet"</span>)usersDF.select(<span class="hljs-string">"name"</span>, <span class="hljs-string">"favorite_color"</span>).write.save(<span class="hljs-string">"namesAndFavColors.parquet"</span>)</code></pre><h4 id="3-1-1-Manually-Specifying-Options"><a href="#3-1-1-Manually-Specifying-Options" class="headerlink" title="3.1.1 Manually Specifying Options"></a>3.1.1 Manually Specifying Options</h4><p>You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. Data sources are specified by their fully qualified name (i.e., <code>org.apache.spark.sql.parquet</code>), but for built-in sources you can also use their short names (<code>json</code>, <code>parquet</code>, <code>jdbc</code>, <code>orc</code>, <code>libsvm</code>, <code>csv</code>, <code>text</code>). DataFrames loaded from any data source type can be converted into other types using this syntax</p><p>您还可以手动指定将要使用的数据源以及要传递给数据源的任何其他选项。数据源通过其全名指定（即<code>org.apache.spark.sql.parquet</code>），但内置的来源，你也可以使用自己的短名称（<code>json</code>，<code>parquet</code>，<code>jdbc</code>，<code>orc</code>，<code>libsvm</code>，<code>csv</code>，<code>text</code>）。从任何数据源类型加载的DataFrame都可以使用此语法转换为其他类型。</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> peopleDF = spark.read.format(<span class="hljs-string">"json"</span>).load(<span class="hljs-string">"examples/src/main/resources/people.json"</span>)peopleDF.select(<span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>).write.format(<span class="hljs-string">"parquet"</span>).save(<span class="hljs-string">"namesAndAges.parquet"</span>)</code></pre><h4 id="3-1-2-Run-SQL-on-files-directly"><a href="#3-1-2-Run-SQL-on-files-directly" class="headerlink" title="3.1.2 Run SQL on files directly"></a>3.1.2 Run SQL on files directly</h4><p>Instead of using read API to load a file into DataFrame and query it, you can also query that file directly with SQL.</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> sqlDF = spark.sql(<span class="hljs-string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>)</code></pre><h4 id="3-1-3-Save-Modes"><a href="#3-1-3-Save-Modes" class="headerlink" title="3.1.3 Save Modes"></a>3.1.3 Save Modes</h4><p>Save operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing an <code>Overwrite</code>, the data will be deleted before writing out the new data.</p><p>保存操作可以选择带<code>SaveMode</code>，指定如何处理现有数据（如果存在）。重要的是要认识到这些保存模式不利用任何锁定，也不是原子的。另外，执行时<code>Overwrite</code>，将在写出新数据之前删除数据。</p><table><thead><tr><th align="left">Scala/Java</th><th align="left">Any Language</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>SaveMode.ErrorIfExists</code> (default)</td><td align="left"><code>&quot;error&quot;</code> (default)</td><td align="left">When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td></tr><tr><td align="left"><code>SaveMode.Append</code></td><td align="left"><code>&quot;append&quot;</code></td><td align="left">When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</td></tr><tr><td align="left"><code>SaveMode.Overwrite</code></td><td align="left"><code>&quot;overwrite&quot;</code></td><td align="left">Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td></tr><tr><td align="left"><code>SaveMode.Ignore</code></td><td align="left"><code>&quot;ignore&quot;</code></td><td align="left">Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a <code>CREATE TABLE IF NOT EXISTS</code> in SQL.</td></tr></tbody></table><h4 id="3-1-4-Saving-to-Persistent-Tables"><a href="#3-1-4-Saving-to-Persistent-Tables" class="headerlink" title="3.1.4 Saving to Persistent Tables"></a>3.1.4 Saving to Persistent Tables</h4><p><code>DataFrames</code> can also be saved as persistent tables into Hive metastore using the <code>saveAsTable</code> command. Notice that an existing Hive deployment is not necessary to use this feature. Spark will create a default local Hive metastore (using Derby) for you. Unlike the <code>createOrReplaceTempView</code> command, <code>saveAsTable</code> will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the <code>table</code> method on a <code>SparkSession</code> with the name of the table.</p><p><code>DataFrames</code>也可以使用以下<code>saveAsTable</code> 命令作为持久表保存到Hive Metastore中。请注意，使用此功能不需要现有的Hive部署。Spark将为您创建一个默认的本地Hive Metastore（使用Derby）。与<code>createOrReplaceTempView</code>命令不同， <code>saveAsTable</code>它将具体化DataFrame的内容并在Hive元存储中创建一个指向数据的指针。即使您重新启动Spark程序，持久表仍将存在，只要您保持与同一metastore的连接即可。可以通过使用表名称<code>table</code>在上调用方法来创建持久表的DataFrame <code>SparkSession</code>。</p><p>By default <code>saveAsTable</code> will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.</p><p>Currently, <code>saveAsTable</code> does not expose an API supporting the creation of an “external table” from a <code>DataFrame</code>. However, this functionality can be achieved by providing a <code>path</code> option to the <code>DataFrameWriter</code> with <code>path</code> as the key and location of the external table as its value (a string) when saving the table with <code>saveAsTable</code>. When an External table is dropped only its metadata is removed.</p><p>默认情况下，<code>saveAsTable</code>将创建一个“托管表”，这意味着数据的位置将由metastore控制。删除表时，托管表还将自动删除其数据。</p><p>当前，<code>saveAsTable</code>不公开支持从中创建“外部表”的API <code>DataFrame</code>。但是，可以通过以下方式来实现此功能：在使用保存表时，通过提供<code>path</code>选项<code>DataFrameWriter</code>with <code>path</code>作为键，并将外部表的位置作为其值（字符串）<code>saveAsTable</code>。删除外部表时，仅删除其元数据。</p><p>Starting from Spark 2.1, persistent datasource tables have per-partition metadata stored in the Hive metastore. This brings several benefits:</p><ul><li>Since the metastore can return only necessary partitions for a query, discovering all the partitions on the first query to the table is no longer needed.</li><li>Hive DDLs such as <code>ALTER TABLE PARTITION ... SET LOCATION</code> are now available for tables created with the Datasource API.</li></ul><p>Note that partition information is not gathered by default when creating external datasource tables (those with a <code>path</code> option). To sync the partition information in the metastore, you can invoke <code>MSCK REPAIR TABLE</code>.</p><p>从Spark 2.1开始，持久性数据源表在Hive元存储中存储了按分区的元数据。这带来了几个好处：</p><ul><li>由于元存储只能返回查询的必要分区，因此不再需要在第一个查询中将所有分区发现到表中。</li><li>Hive DDL，例如<code>ALTER TABLE PARTITION ... SET LOCATION</code>现在可用于使用Datasource API创建的表。</li></ul><p>请注意，在创建外部数据源表（带有<code>path</code>选项的表）时，默认情况下不会收集分区信息。要同步元存储中的分区信息，可以调用<code>MSCK REPAIR TABLE</code>。</p><h3 id="3-2-Parquet-Files"><a href="#3-2-Parquet-Files" class="headerlink" title="3.2 Parquet Files"></a>3.2 Parquet Files</h3><p><a href="http://parquet.io/" target="_blank" rel="noopener">Parquet</a> is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.</p><p><a href="http://parquet.io/" target="_blank" rel="noopener">Parquet</a>是许多其他数据处理系统支持的柱状格式。Spark SQL提供对读取和写入Parquet文件的支持，这些文件会自动保留原始数据的架构。编写Parquet文件时，出于兼容性原因，所有列都将自动转换为可为空。</p><h4 id="3-2-1-Loading-Data-Programmatically"><a href="#3-2-1-Loading-Data-Programmatically" class="headerlink" title="3.2.1 Loading Data Programmatically"></a>3.2.1 Loading Data Programmatically</h4><p>Using the data from the above example:</p><pre><code class="hljs scala"><span class="hljs-comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span><span class="hljs-keyword">import</span> spark.implicits._<span class="hljs-keyword">val</span> peopleDF = spark.read.json(<span class="hljs-string">"examples/src/main/resources/people.json"</span>)<span class="hljs-comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span>peopleDF.write.parquet(<span class="hljs-string">"people.parquet"</span>)<span class="hljs-comment">// Read in the parquet file created above</span><span class="hljs-comment">// Parquet files are self-describing so the schema is preserved</span><span class="hljs-comment">// The result of loading a Parquet file is also a DataFrame</span><span class="hljs-keyword">val</span> parquetFileDF = spark.read.parquet(<span class="hljs-string">"people.parquet"</span>)<span class="hljs-comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span>parquetFileDF.createOrReplaceTempView(<span class="hljs-string">"parquetFile"</span>)<span class="hljs-keyword">val</span> namesDF = spark.sql(<span class="hljs-string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>)namesDF.map(attributes =&gt; <span class="hljs-string">"Name: "</span> + attributes(<span class="hljs-number">0</span>)).show()<span class="hljs-comment">// +------------+</span><span class="hljs-comment">// |       value|</span><span class="hljs-comment">// +------------+</span><span class="hljs-comment">// |Name: Justin|</span><span class="hljs-comment">// +------------+</span></code></pre><h4 id="3-2-2-Partition-Discovery"><a href="#3-2-2-Partition-Discovery" class="headerlink" title="3.2.2 Partition Discovery"></a>3.2.2 Partition Discovery</h4><p>Table partitioning is a common optimization approach used in systems like Hive. In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory. The Parquet data source is now able to discover and infer partitioning information automatically. For example, we can store all our previously used population data into a partitioned table using the following directory structure, with two extra columns, <code>gender</code> and <code>country</code> as partitioning columns:</p><p>表分区是Hive等系统中常用的优化方法。在分区表中，数据通常存储在不同的目录中，分区列值编码在每个分区目录的路径中。现在，Parquet数据源能够自动发现和推断分区信息。例如，我们可以使用以下目录结构将之前使用的所有填充数据存储到一个分区表中，该目录结构具有两个额外的列<code>gender</code>并<code>country</code>作为分区列：</p><pre><code class="hljs routeros">path└── <span class="hljs-keyword">to</span>    └── table        ├── <span class="hljs-attribute">gender</span>=male        │   ├── <span class="hljs-built_in">..</span>.        │   │        │   ├── <span class="hljs-attribute">country</span>=US        │   │   └── data.parquet        │   ├── <span class="hljs-attribute">country</span>=CN        │   │   └── data.parquet        │   └── <span class="hljs-built_in">..</span>.        └── <span class="hljs-attribute">gender</span>=female            ├── <span class="hljs-built_in">..</span>.            │            ├── <span class="hljs-attribute">country</span>=US            │   └── data.parquet            ├── <span class="hljs-attribute">country</span>=CN            │   └── data.parquet            └── <span class="hljs-built_in">..</span>.</code></pre><p>By passing <code>path/to/table</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, Spark SQL will automatically extract the partitioning information from the paths. Now the schema of the returned DataFrame becomes:</p><p>通过传递<code>path/to/table</code>给<code>SparkSession.read.parquet</code>或<code>SparkSession.read.load</code>，Spark SQL将自动从路径中提取分区信息。现在，返回的DataFrame的架构变为：</p><pre><code class="hljs yaml"><span class="hljs-string">root</span><span class="hljs-string">|--</span> <span class="hljs-attr">name:</span> <span class="hljs-string">string</span> <span class="hljs-string">(nullable</span> <span class="hljs-string">=</span> <span class="hljs-literal">true</span><span class="hljs-string">)</span><span class="hljs-string">|--</span> <span class="hljs-attr">age:</span> <span class="hljs-string">long</span> <span class="hljs-string">(nullable</span> <span class="hljs-string">=</span> <span class="hljs-literal">true</span><span class="hljs-string">)</span><span class="hljs-string">|--</span> <span class="hljs-attr">gender:</span> <span class="hljs-string">string</span> <span class="hljs-string">(nullable</span> <span class="hljs-string">=</span> <span class="hljs-literal">true</span><span class="hljs-string">)</span><span class="hljs-string">|--</span> <span class="hljs-attr">country:</span> <span class="hljs-string">string</span> <span class="hljs-string">(nullable</span> <span class="hljs-string">=</span> <span class="hljs-literal">true</span><span class="hljs-string">)</span></code></pre><p>Notice that the data types of the partitioning columns are automatically inferred. Currently, numeric data types and string type are supported. Sometimes users may not want to automatically infer the data types of the partitioning columns. For these use cases, the automatic type inference can be configured by <code>spark.sql.sources.partitionColumnTypeInference.enabled</code>, which is default to <code>true</code>. When type inference is disabled, string type will be used for the partitioning columns.</p><p>Starting from Spark 1.6.0, partition discovery only finds partitions under the given paths by default. For the above example, if users pass <code>path/to/table/gender=male</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, <code>gender</code> will not be considered as a partitioning column. If users need to specify the base path that partition discovery should start with, they can set <code>basePath</code> in the data source options. For example, when <code>path/to/table/gender=male</code> is the path of the data and users set <code>basePath</code> to <code>path/to/table/</code>, <code>gender</code> will be a partitioning column.</p><p>请注意，分区列的数据类型是自动推断的。当前，支持数字数据类型和字符串类型。有时用户可能不希望自动推断分区列的数据类型。对于这些用例，可以使用来配置自动类型推断<code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认为 <code>true</code>。禁用类型推断时，字符串类型将用于分区列。</p><p>从Spark 1.6.0开始，默认情况下，分区发现仅在给定路径下查找分区。对于上面的示例，如果用户传递<code>path/to/table/gender=male</code>给 <code>SparkSession.read.parquet</code>或<code>SparkSession.read.load</code>，<code>gender</code>则不会被视为分区列。如果用户需要指定分区发现应开始的基本路径，则可以<code>basePath</code>在数据源选项中进行设置。例如，当<code>path/to/table/gender=male</code>数据路径是且用户设置<code>basePath</code>为时<code>path/to/table/</code>，<code>gender</code>将是一个分区列。</p><h4 id="3-2-3-Schema-Merging"><a href="#3-2-3-Schema-Merging" class="headerlink" title="3.2.3 Schema Merging"></a>3.2.3 Schema Merging</h4><p>Like ProtocolBuffer, Avro, and Thrift, Parquet also supports schema evolution. Users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible schemas. The Parquet data source is now able to automatically detect this case and merge schemas of all these files.</p><p>Since schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0. You may enable it by</p><ol><li>setting data source option <code>mergeSchema</code> to <code>true</code> when reading Parquet files (as shown in the examples below), or</li><li>setting the global SQL option <code>spark.sql.parquet.mergeSchema</code> to <code>true</code>.</li></ol><p>像ProtocolBuffer，Avro和Thrift一样，Parquet也支持架构演变。用户可以从简单的架构开始，然后根据需要逐渐向架构中添加更多列。这样，用户可能最终得到具有不同但相互兼容的架构的多个Parquet文件。现在，Parquet数据源能够自动检测到这种情况并合并所有这些文件的模式。</p><p>由于模式合并是一项相对昂贵的操作，并且在大多数情况下不是必需的，因此默认情况下，我们从1.5.0开始将其关闭。您可以通过以下方式启用它</p><ol><li>将数据源选项设置<code>mergeSchema</code>为<code>true</code>在读取Parquet文件时（如下例所示），或者</li><li>将全局SQL选项设置<code>spark.sql.parquet.mergeSchema</code>为<code>true</code>。</li></ol><pre><code class="hljs scala"><span class="hljs-comment">// This is used to implicitly convert an RDD to a DataFrame.</span><span class="hljs-keyword">import</span> spark.implicits._<span class="hljs-comment">// Create a simple DataFrame, store into a partition directory</span><span class="hljs-keyword">val</span> squaresDF = spark.sparkContext.makeRDD(<span class="hljs-number">1</span> to <span class="hljs-number">5</span>).map(i =&gt; (i, i * i)).toDF(<span class="hljs-string">"value"</span>, <span class="hljs-string">"square"</span>)squaresDF.write.parquet(<span class="hljs-string">"data/test_table/key=1"</span>)<span class="hljs-comment">// Create another DataFrame in a new partition directory,</span><span class="hljs-comment">// adding a new column and dropping an existing column</span><span class="hljs-keyword">val</span> cubesDF = spark.sparkContext.makeRDD(<span class="hljs-number">6</span> to <span class="hljs-number">10</span>).map(i =&gt; (i, i * i * i)).toDF(<span class="hljs-string">"value"</span>, <span class="hljs-string">"cube"</span>)cubesDF.write.parquet(<span class="hljs-string">"data/test_table/key=2"</span>)<span class="hljs-comment">// Read the partitioned table</span><span class="hljs-keyword">val</span> mergedDF = spark.read.option(<span class="hljs-string">"mergeSchema"</span>, <span class="hljs-string">"true"</span>).parquet(<span class="hljs-string">"data/test_table"</span>)mergedDF.printSchema()<span class="hljs-comment">// The final schema consists of all 3 columns in the Parquet files together</span><span class="hljs-comment">// with the partitioning column appeared in the partition directory paths</span><span class="hljs-comment">// root</span><span class="hljs-comment">//  |-- value: int (nullable = true)</span><span class="hljs-comment">//  |-- square: int (nullable = true)</span><span class="hljs-comment">//  |-- cube: int (nullable = true)</span><span class="hljs-comment">//  |-- key: int (nullable = true)</span></code></pre><h4 id="3-2-4-Hive-metastore-Parquet-table-conversion"><a href="#3-2-4-Hive-metastore-Parquet-table-conversion" class="headerlink" title="3.2.4 Hive metastore Parquet table conversion"></a>3.2.4 Hive metastore Parquet table conversion</h4><p>When reading from and writing to Hive metastore Parquet tables, Spark SQL will try to use its own Parquet support instead of Hive SerDe for better performance. This behavior is controlled by the <code>spark.sql.hive.convertMetastoreParquet</code> configuration, and is turned on by default.</p><p>在读取和写入Hive metastore Parquet表时，Spark SQL将尝试使用其自己的Parquet支持而不是Hive SerDe以获得更好的性能。此行为由<code>spark.sql.hive.convertMetastoreParquet</code>配置控制 ，并且默认情况下处于启用状态。</p><h5 id="3-2-4-1-Hive-Parquet-Schema-Reconciliation"><a href="#3-2-4-1-Hive-Parquet-Schema-Reconciliation" class="headerlink" title="3.2.4.1 Hive/Parquet Schema Reconciliation"></a>3.2.4.1 Hive/Parquet Schema Reconciliation</h5><p>There are two key differences between Hive and Parquet from the perspective of table schema processing.</p><ol><li>Hive is case insensitive, while Parquet is not</li><li>Hive considers all columns nullable, while nullability in Parquet is significant</li></ol><p>Due to this reason, we must reconcile Hive metastore schema with Parquet schema when converting a Hive metastore Parquet table to a Spark SQL Parquet table. The reconciliation rules are:</p><ol><li>Fields that have the same name in both schema must have the same data type regardless of nullability. The reconciled field should have the data type of the Parquet side, so that nullability is respected.</li><li>The reconciled schema contains exactly those fields defined in Hive metastore schema.<ul><li>Any fields that only appear in the Parquet schema are dropped in the reconciled schema.</li><li>Any fields that only appear in the Hive metastore schema are added as nullable field in the reconciled schema.</li></ul></li></ol><p>从表模式处理的角度来看，Hive和Parquet之间有两个关键区别。</p><ol><li>Hive不区分大小写，而Parquet区分</li><li>Hive认为所有列都可为空，而Parquet中的可为空性很重要</li></ol><p>由于这个原因，在将Hive Metastore Parquet表转换为Spark SQL Parquet表时，我们必须使Hive Metastore模式与Parquet模式一致。对帐规则为：</p><ol><li>在两个模式中具有相同名称的字段必须具有相同的数据类型，而不考虑可为空性。协调字段应具有Parquet端的数据类型，以便遵守可空性。</li><li>协调后的架构完全包含在Hive Metastore架构中定义的那些字段。<ul><li>仅出现在Parquet模式中的所有字段都将被放入对帐模式中。</li><li>仅在Hive Metastore模式中出现的所有字段都将添加为已对帐模式中的可为空字段。</li></ul></li></ol><h5 id="3-2-4-2-Metadata-Refreshing"><a href="#3-2-4-2-Metadata-Refreshing" class="headerlink" title="3.2.4.2 Metadata Refreshing"></a>3.2.4.2 Metadata Refreshing</h5><p>Spark SQL caches Parquet metadata for better performance. When Hive metastore Parquet table conversion is enabled, metadata of those converted tables are also cached. If these tables are updated by Hive or other external tools, you need to refresh them manually to ensure consistent metadata.</p><p>Spark SQL缓存Parquet元数据以获得更好的性能。启用Hive Metastore Parquet表转换后，这些转换表的元数据也会被缓存。如果这些表是通过Hive或其他外部工具更新的，则需要手动刷新它们以确保元数据一致。</p><pre><code class="hljs scala"><span class="hljs-comment">// spark is an existing SparkSession</span>spark.catalog.refreshTable(<span class="hljs-string">"my_table"</span>)</code></pre><h4 id="3-2-5-Configuration"><a href="#3-2-5-Configuration" class="headerlink" title="3.2.5 Configuration"></a>3.2.5 Configuration</h4><p>Configuration of Parquet can be done using the <code>setConf</code> method on <code>SparkSession</code> or by running <code>SET key=value</code> commands using SQL.</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>spark.sql.parquet.binaryAsString</code></td><td align="left">false</td><td align="left">Some other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</td></tr><tr><td align="left"><code>spark.sql.parquet.int96AsTimestamp</code></td><td align="left">true</td><td align="left">Some Parquet-producing systems, in particular Impala and Hive, store Timestamp into INT96. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</td></tr><tr><td align="left"><code>spark.sql.parquet.cacheMetadata</code></td><td align="left">true</td><td align="left">Turns on caching of Parquet schema metadata. Can speed up querying of static data.</td></tr><tr><td align="left"><code>spark.sql.parquet.compression.codec</code></td><td align="left">snappy</td><td align="left">Sets the compression codec use when writing Parquet files. Acceptable values include: uncompressed, snappy, gzip, lzo.</td></tr><tr><td align="left"><code>spark.sql.parquet.filterPushdown</code></td><td align="left">true</td><td align="left">Enables Parquet filter push-down optimization when set to true.</td></tr><tr><td align="left"><code>spark.sql.hive.convertMetastoreParquet</code></td><td align="left">true</td><td align="left">When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in support.</td></tr><tr><td align="left"><code>spark.sql.parquet.mergeSchema</code></td><td align="left">false</td><td align="left">When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</td></tr><tr><td align="left"><code>spark.sql.optimizer.metadataOnly</code></td><td align="left">true</td><td align="left">When true, enable the metadata-only query optimization that use the table’s metadata to produce the partition columns instead of table scans. It applies when all the columns scanned are partition columns and the query has an aggregate operator that satisfies distinct semantics.</td></tr></tbody></table><h3 id="3-3-JSON-Datasets"><a href="#3-3-JSON-Datasets" class="headerlink" title="3.3 JSON Datasets"></a>3.3 JSON Datasets</h3><p>Spark SQL can automatically infer the schema of a JSON dataset and load it as a <code>Dataset[Row]</code>. This conversion can be done using <code>SparkSession.read.json()</code> on either an RDD of String, or a JSON file.</p><p>Note that the file that is offered as <em>a json file</em> is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. For more information, please see <a href="http://jsonlines.org/" target="_blank" rel="noopener">JSON Lines text format, also called newline-delimited JSON</a>. As a consequence, a regular multi-line JSON file will most often fail.</p><p>Spark SQL可以自动推断JSON数据集的架构并将其作为加载<code>Dataset[Row]</code>。可以使用<code>SparkSession.read.json()</code>字符串的RDD或JSON文件来完成此转换。</p><p>请注意，<em>以json文件</em>形式提供<em>的文件</em>不是典型的JSON文件。每行必须包含一个单独的，自包含的有效JSON对象。有关更多信息，请参见 <a href="http://jsonlines.org/" target="_blank" rel="noopener">JSON Lines文本格式，也称为newline分隔的JSON</a>。因此，常规的多行JSON文件通常会失败。</p><pre><code class="hljs scala"><span class="hljs-comment">// A JSON dataset is pointed to by path.</span><span class="hljs-comment">// The path can be either a single text file or a directory storing text files</span><span class="hljs-keyword">val</span> path = <span class="hljs-string">"examples/src/main/resources/people.json"</span><span class="hljs-keyword">val</span> peopleDF = spark.read.json(path)<span class="hljs-comment">// The inferred schema can be visualized using the printSchema() method</span>peopleDF.printSchema()<span class="hljs-comment">// root</span><span class="hljs-comment">//  |-- age: long (nullable = true)</span><span class="hljs-comment">//  |-- name: string (nullable = true)</span><span class="hljs-comment">// Creates a temporary view using the DataFrame</span>peopleDF.createOrReplaceTempView(<span class="hljs-string">"people"</span>)<span class="hljs-comment">// SQL statements can be run by using the sql methods provided by spark</span><span class="hljs-keyword">val</span> teenagerNamesDF = spark.sql(<span class="hljs-string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>)teenagerNamesDF.show()<span class="hljs-comment">// +------+</span><span class="hljs-comment">// |  name|</span><span class="hljs-comment">// +------+</span><span class="hljs-comment">// |Justin|</span><span class="hljs-comment">// +------+</span><span class="hljs-comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><span class="hljs-comment">// an RDD[String] storing one JSON object per string</span><span class="hljs-keyword">val</span> otherPeopleRDD = spark.sparkContext.makeRDD(  <span class="hljs-string">""</span><span class="hljs-string">"&#123;"</span><span class="hljs-string">name":"</span><span class="hljs-type">Yin</span><span class="hljs-string">","</span><span class="hljs-string">address":&#123;"</span><span class="hljs-string">city":"</span><span class="hljs-type">Columbus</span><span class="hljs-string">","</span><span class="hljs-string">state":"</span><span class="hljs-type">Ohio</span><span class="hljs-string">"&#125;&#125;"</span><span class="hljs-string">""</span> :: <span class="hljs-type">Nil</span>)<span class="hljs-keyword">val</span> otherPeople = spark.read.json(otherPeopleRDD)otherPeople.show()<span class="hljs-comment">// +---------------+----+</span><span class="hljs-comment">// |        address|name|</span><span class="hljs-comment">// +---------------+----+</span><span class="hljs-comment">// |[Columbus,Ohio]| Yin|</span><span class="hljs-comment">// +---------------+----+</span></code></pre><h3 id="3-4-Hive-Tables"><a href="#3-4-Hive-Tables" class="headerlink" title="3.4 Hive Tables"></a>3.4 Hive Tables</h3><p>Spark SQL also supports reading and writing data stored in <a href="http://hive.apache.org/" target="_blank" rel="noopener">Apache Hive</a>. However, since Hive has a large number of dependencies, these dependencies are not included in the default Spark distribution. If Hive dependencies can be found on the classpath, Spark will load them automatically. Note that these Hive dependencies must also be present on all of the worker nodes, as they will need access to the Hive serialization and deserialization libraries (SerDes) in order to access data stored in Hive.</p><p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> (for security configuration), and <code>hdfs-site.xml</code> (for HDFS configuration) file in <code>conf/</code>.</p><p>When working with Hive, one must instantiate <code>SparkSession</code> with Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions. Users who do not have an existing Hive deployment can still enable Hive support. When not configured by the <code>hive-site.xml</code>, the context automatically creates <code>metastore_db</code> in the current directory and creates a directory configured by <code>spark.sql.warehouse.dir</code>, which defaults to the directory <code>spark-warehouse</code> in the current directory that the Spark application is started. Note that the <code>hive.metastore.warehouse.dir</code> property in <code>hive-site.xml</code> is deprecated since Spark 2.0.0. Instead, use <code>spark.sql.warehouse.dir</code> to specify the default location of database in warehouse. You may need to grant write privilege to the user who starts the Spark application.</p><p>Spark SQL还支持读写存储在<a href="http://hive.apache.org/" target="_blank" rel="noopener">Apache Hive中的</a>数据。但是，由于Hive具有大量依赖关系，因此默认的Spark分发中不包含这些依赖关系。如果可以在类路径上找到Hive依赖项，Spark将自动加载它们。请注意，这些Hive依赖项也必须存在于所有工作节点上，因为它们将需要访问Hive序列化和反序列化库（SerDes）才能访问存储在Hive中的数据。</p><p>通过将<code>hive-site.xml</code>，<code>core-site.xml</code>（对于安全性配置）和<code>hdfs-site.xml</code>（对于HDFS配置）文件放置在中来配置Hive <code>conf/</code>。</p><p>使用Hive时，必须实例化<code>SparkSession</code>Hive支持，包括与永久性Hive元存储库的连接，对Hive Serdes的支持以及Hive用户定义的功能。没有现有Hive部署的用户仍可以启用Hive支持。如果未由配置<code>hive-site.xml</code>，则上下文会自动<code>metastore_db</code>在当前目录中创建并创建一个由配置<code>spark.sql.warehouse.dir</code>的目录<code>spark-warehouse</code>，该目录默认 为启动Spark应用程序的当前目录中的目录。请注意，自Spark 2.0.0起不推荐使用<code>hive.metastore.warehouse.dir</code>in 的属性<code>hive-site.xml</code>。而是使用<code>spark.sql.warehouse.dir</code>指定仓库中数据库的默认位置。您可能需要向启动Spark应用程序的用户授予写权限。</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Row</span><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Record</span>(<span class="hljs-params">key: <span class="hljs-type">Int</span>, value: <span class="hljs-type">String</span></span>)</span><span class="hljs-class"></span><span class="hljs-class"><span class="hljs-title">//</span> <span class="hljs-title">warehouseLocation</span> <span class="hljs-title">points</span> <span class="hljs-title">to</span> <span class="hljs-title">the</span> <span class="hljs-title">default</span> <span class="hljs-title">location</span> <span class="hljs-title">for</span> <span class="hljs-title">managed</span> <span class="hljs-title">databases</span> <span class="hljs-title">and</span> <span class="hljs-title">tables</span></span><span class="hljs-class"><span class="hljs-title">val</span> <span class="hljs-title">warehouseLocation</span> </span>= <span class="hljs-string">"spark-warehouse"</span><span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>  .builder()  .appName(<span class="hljs-string">"Spark Hive Example"</span>)  .config(<span class="hljs-string">"spark.sql.warehouse.dir"</span>, warehouseLocation)  .enableHiveSupport()  .getOrCreate()<span class="hljs-keyword">import</span> spark.implicits._<span class="hljs-keyword">import</span> spark.sqlsql(<span class="hljs-string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)sql(<span class="hljs-string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)<span class="hljs-comment">// Queries are expressed in HiveQL</span>sql(<span class="hljs-string">"SELECT * FROM src"</span>).show()<span class="hljs-comment">// +---+-------+</span><span class="hljs-comment">// |key|  value|</span><span class="hljs-comment">// +---+-------+</span><span class="hljs-comment">// |238|val_238|</span><span class="hljs-comment">// | 86| val_86|</span><span class="hljs-comment">// |311|val_311|</span><span class="hljs-comment">// ...</span><span class="hljs-comment">// Aggregation queries are also supported.</span>sql(<span class="hljs-string">"SELECT COUNT(*) FROM src"</span>).show()<span class="hljs-comment">// +--------+</span><span class="hljs-comment">// |count(1)|</span><span class="hljs-comment">// +--------+</span><span class="hljs-comment">// |    500 |</span><span class="hljs-comment">// +--------+</span><span class="hljs-comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><span class="hljs-keyword">val</span> sqlDF = sql(<span class="hljs-string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>)<span class="hljs-comment">// The items in DaraFrames are of type Row, which allows you to access each column by ordinal.</span><span class="hljs-keyword">val</span> stringsDS = sqlDF.map &#123;  <span class="hljs-keyword">case</span> <span class="hljs-type">Row</span>(key: <span class="hljs-type">Int</span>, value: <span class="hljs-type">String</span>) =&gt; <span class="hljs-string">s"Key: <span class="hljs-subst">$key</span>, Value: <span class="hljs-subst">$value</span>"</span>&#125;stringsDS.show()<span class="hljs-comment">// +--------------------+</span><span class="hljs-comment">// |               value|</span><span class="hljs-comment">// +--------------------+</span><span class="hljs-comment">// |Key: 0, Value: val_0|</span><span class="hljs-comment">// |Key: 0, Value: val_0|</span><span class="hljs-comment">// |Key: 0, Value: val_0|</span><span class="hljs-comment">// ...</span><span class="hljs-comment">// You can also use DataFrames to create temporary views within a SparkSession.</span><span class="hljs-keyword">val</span> recordsDF = spark.createDataFrame((<span class="hljs-number">1</span> to <span class="hljs-number">100</span>).map(i =&gt; <span class="hljs-type">Record</span>(i, <span class="hljs-string">s"val_<span class="hljs-subst">$i</span>"</span>)))recordsDF.createOrReplaceTempView(<span class="hljs-string">"records"</span>)<span class="hljs-comment">// Queries can then join DataFrame data with data stored in Hive.</span>sql(<span class="hljs-string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show()<span class="hljs-comment">// +---+------+---+------+</span><span class="hljs-comment">// |key| value|key| value|</span><span class="hljs-comment">// +---+------+---+------+</span><span class="hljs-comment">// |  2| val_2|  2| val_2|</span><span class="hljs-comment">// |  4| val_4|  4| val_4|</span><span class="hljs-comment">// |  5| val_5|  5| val_5|</span><span class="hljs-comment">// ...</span></code></pre><h4 id="3-4-1-Interacting-with-Different-Versions-of-Hive-Metastore"><a href="#3-4-1-Interacting-with-Different-Versions-of-Hive-Metastore" class="headerlink" title="3.4.1 Interacting with Different Versions of Hive Metastore"></a>3.4.1 Interacting with Different Versions of Hive Metastore</h4><p>One of the most important pieces of Spark SQL’s Hive support is interaction with Hive metastore, which enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary build of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below. Note that independent of the version of Hive that is being used to talk to the metastore, internally Spark SQL will compile against Hive 1.2.1 and use those classes for internal execution (serdes, UDFs, UDAFs, etc).</p><p>与Hive metastore的交互是Spark SQL对Hive的最重要支持之一，它使Spark SQL能够访问Hive表的元数据。从Spark 1.4.0开始，使用以下描述的配置，可以使用Spark SQL的单个二进制版本来查询Hive元存储的不同版本。请注意，与用于与metastore进行通信的Hive版本无关，Spark SQL在内部将针对Hive 1.2.1进行编译，并将这些类用于内部执行（serdes，UDF，UDAF等）。</p><p>The following options can be used to configure the version of Hive that is used to retrieve metadata:</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>spark.sql.hive.metastore.version</code></td><td align="left"><code>1.2.1</code></td><td align="left">Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>1.2.1</code>.</td></tr><tr><td align="left"><code>spark.sql.hive.metastore.jars</code></td><td align="left"><code>builtin</code></td><td align="left">Location of the jars that should be used to instantiate the HiveMetastoreClient. This property can be one of three options:<code>builtin</code>Use Hive 1.2.1, which is bundled with the Spark assembly when <code>-Phive</code> is enabled. When this option is chosen, <code>spark.sql.hive.metastore.version</code> must be either <code>1.2.1</code> or not defined.<code>maven</code>Use Hive jars of specified version downloaded from Maven repositories. This configuration is not generally recommended for production deployments.A classpath in the standard format for the JVM. This classpath must include all of Hive and its dependencies, including the correct version of Hadoop. These jars only need to be present on the driver, but if you are running in yarn cluster mode then you must ensure they are packaged with your application.</td></tr><tr><td align="left"><code>spark.sql.hive.metastore.sharedPrefixes</code></td><td align="left"><code>com.mysql.jdbc,org.postgresql</code>,<code>com.microsoft.sqlserver,oracle.jdbc</code></td><td align="left">A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</td></tr><tr><td align="left"><code>spark.sql.hive.metastore.barrierPrefixes</code></td><td align="left"><code>(empty)</code></td><td align="left">A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).</td></tr></tbody></table><h3 id="3-5-JDBC-To-Other-Databases"><a href="#3-5-JDBC-To-Other-Databases" class="headerlink" title="3.5 JDBC To Other Databases"></a>3.5 JDBC To Other Databases</h3><p>Spark SQL also includes a data source that can read data from other databases using JDBC. This functionality should be preferred over using <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" target="_blank" rel="noopener">JdbcRDD</a>. This is because the results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources. The JDBC data source is also easier to use from Java or Python as it does not require the user to provide a ClassTag. (Note that this is different than the Spark SQL JDBC server, which allows other applications to run queries using Spark SQL).</p><p>To get started you will need to include the JDBC driver for you particular database on the spark classpath. For example, to connect to postgres from the Spark Shell you would run the following command:</p><p>Spark SQL还包括一个可以使用JDBC从其他数据库读取数据的数据源。与使用<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" target="_blank" rel="noopener">JdbcRDD相比，</a>应优先使用此功能。这是因为结果以DataFrame的形式返回，并且可以轻松地在Spark SQL中进行处理或与其他数据源合并。JDBC数据源也更易于从Java或Python使用，因为它不需要用户提供ClassTag。（请注意，这与Spark SQL JDBC服务器不同，后者允许其他应用程序使用Spark SQL运行查询）。</p><p>首先，您需要在spark类路径上包含特定数据库的JDBC驱动程序。例如，要从Spark Shell连接到postgres，您可以运行以下命令：</p><pre><code class="hljs lsl">bin/spark-shell --driver-class-path postgresql<span class="hljs-number">-9.4</span><span class="hljs-number">.1207</span>.jar --jars postgresql<span class="hljs-number">-9.4</span><span class="hljs-number">.1207</span>.jar</code></pre><p>Tables from the remote database can be loaded as a DataFrame or Spark SQL temporary view using the Data Sources API. Users can specify the JDBC connection properties in the data source options. <code>user</code> and <code>password</code> are normally provided as connection properties for logging into the data sources. In addition to the connection properties, Spark also supports the following case-insensitive options:</p><p>可以使用Data Sources API将远程数据库中的表作为DataFrame或Spark SQL临时视图加载。用户可以在数据源选项中指定JDBC连接属性。 <code>user</code>和<code>password</code>通常用于登录到数据源提供为连接属性。除连接属性外，Spark还支持以下不区分大小写的选项：</p><table><thead><tr><th align="left">Property Name</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>url</code></td><td align="left">The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., <code>jdbc:postgresql://localhost/test?user=fred&amp;password=secret</code></td></tr><tr><td align="left"><code>dbtable</code></td><td align="left">The JDBC table that should be read. Note that anything that is valid in a <code>FROM</code> clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.</td></tr><tr><td align="left"><code>driver</code></td><td align="left">The class name of the JDBC driver to use to connect to this URL.</td></tr><tr><td align="left"><code>partitionColumn, lowerBound, upperBound, numPartitions</code></td><td align="left">These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. <code>partitionColumn</code> must be a numeric column from the table in question. Notice that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.</td></tr><tr><td align="left"><code>fetchsize</code></td><td align="left">The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading.</td></tr><tr><td align="left"><code>batchsize</code></td><td align="left">The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to <code>1000</code>.</td></tr><tr><td align="left"><code>isolationLevel</code></td><td align="left">The transaction isolation level, which applies to current connection. It can be one of <code>NONE</code>, <code>READ_COMMITTED</code>, <code>READ_UNCOMMITTED</code>, <code>REPEATABLE_READ</code>, or <code>SERIALIZABLE</code>, corresponding to standard transaction isolation levels defined by JDBC’s Connection object, with default of <code>READ_UNCOMMITTED</code>. This option applies only to writing. Please refer the documentation in <code>java.sql.Connection</code>.</td></tr><tr><td align="left"><code>truncate</code></td><td align="left">This is a JDBC writer related option. When <code>SaveMode.Overwrite</code> is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to <code>false</code>. This option applies only to writing.</td></tr><tr><td align="left"><code>createTableOptions</code></td><td align="left">This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., <code>CREATE TABLE t (name string) ENGINE=InnoDB.</code>). This option applies only to writing.</td></tr></tbody></table><pre><code class="hljs scala"><span class="hljs-comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><span class="hljs-comment">// Loading data from a JDBC source</span><span class="hljs-keyword">val</span> jdbcDF = spark.read  .format(<span class="hljs-string">"jdbc"</span>)  .option(<span class="hljs-string">"url"</span>, <span class="hljs-string">"jdbc:postgresql:dbserver"</span>)  .option(<span class="hljs-string">"dbtable"</span>, <span class="hljs-string">"schema.tablename"</span>)  .option(<span class="hljs-string">"user"</span>, <span class="hljs-string">"username"</span>)  .option(<span class="hljs-string">"password"</span>, <span class="hljs-string">"password"</span>)  .load()<span class="hljs-keyword">val</span> connectionProperties = <span class="hljs-keyword">new</span> <span class="hljs-type">Properties</span>()connectionProperties.put(<span class="hljs-string">"user"</span>, <span class="hljs-string">"username"</span>)connectionProperties.put(<span class="hljs-string">"password"</span>, <span class="hljs-string">"password"</span>)<span class="hljs-keyword">val</span> jdbcDF2 = spark.read  .jdbc(<span class="hljs-string">"jdbc:postgresql:dbserver"</span>, <span class="hljs-string">"schema.tablename"</span>, connectionProperties)<span class="hljs-comment">// Saving data to a JDBC source</span>jdbcDF.write  .format(<span class="hljs-string">"jdbc"</span>)  .option(<span class="hljs-string">"url"</span>, <span class="hljs-string">"jdbc:postgresql:dbserver"</span>)  .option(<span class="hljs-string">"dbtable"</span>, <span class="hljs-string">"schema.tablename"</span>)  .option(<span class="hljs-string">"user"</span>, <span class="hljs-string">"username"</span>)  .option(<span class="hljs-string">"password"</span>, <span class="hljs-string">"password"</span>)  .save()jdbcDF2.write  .jdbc(<span class="hljs-string">"jdbc:postgresql:dbserver"</span>, <span class="hljs-string">"schema.tablename"</span>, connectionProperties)</code></pre><h3 id="3-6-Troubleshooting"><a href="#3-6-Troubleshooting" class="headerlink" title="3.6 Troubleshooting"></a>3.6 Troubleshooting</h3><ul><li><p>The JDBC driver class must be visible to the primordial class loader on the client session and on all executors. This is because Java’s DriverManager class does a security check that results in it ignoring all drivers not visible to the primordial class loader when one goes to open a connection. One convenient way to do this is to modify compute_classpath.sh on all worker nodes to include your driver JARs.</p></li><li><p>Some databases, such as H2, convert all names to upper case. You’ll need to use upper case to refer to those names in Spark SQL.</p></li><li><p>JDBC驱动程序类必须对客户机会话和所有执行程序上的原始类加载器可见。这是因为Java的DriverManager类进行了安全检查，导致它忽略了当打开连接时原始类加载器不可见的所有驱动程序。一种方便的方法是修改所有工作程序节点上的compute_classpath.sh以包括您的驱动程序JAR。</p></li><li><p>某些数据库（例如H2）会将所有名称都转换为大写。您需要使用大写字母在Spark SQL中引用这些名称。</p></li></ul><h2 id="4-Performance-Tuning"><a href="#4-Performance-Tuning" class="headerlink" title="4. Performance Tuning"></a>4. Performance Tuning</h2><p>For some workloads it is possible to improve performance by either caching data in memory, or by turning on some experimental options.</p><p>对于某些工作负载，可以通过在内存中缓存数据或打开某些实验选项来提高性能。</p><h3 id="4-1-Caching-Data-In-Memory"><a href="#4-1-Caching-Data-In-Memory" class="headerlink" title="4.1 Caching Data In Memory"></a>4.1 Caching Data In Memory</h3><p>Spark SQL can cache tables using an in-memory columnar format by calling <code>spark.catalog.cacheTable(&quot;tableName&quot;)</code> or <code>dataFrame.cache()</code>. Then Spark SQL will scan only required columns and will automatically tune compression to minimize memory usage and GC pressure. You can call <code>spark.catalog.uncacheTable(&quot;tableName&quot;)</code> to remove the table from memory.</p><p>Configuration of in-memory caching can be done using the <code>setConf</code> method on <code>SparkSession</code> or by running <code>SET key=value</code> commands using SQL.</p><p>Spark SQL可以通过调用<code>spark.catalog.cacheTable(&quot;tableName&quot;)</code>或使用内存列式格式缓存表<code>dataFrame.cache()</code>。然后，Spark SQL将仅扫描所需的列，并将自动调整压缩以最大程度地减少内存使用和GC压力。您可以调用<code>spark.catalog.uncacheTable(&quot;tableName&quot;)</code>从内存中删除表。</p><p>可以使用<code>setConf</code>on上的方法<code>SparkSession</code>或<code>SET key=value</code>使用SQL 运行 命令来完成内存中缓存的配置。</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>spark.sql.inMemoryColumnarStorage.compressed</code></td><td align="left">true</td><td align="left">When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.</td></tr><tr><td align="left"><code>spark.sql.inMemoryColumnarStorage.batchSize</code></td><td align="left">10000</td><td align="left">Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</td></tr></tbody></table><h3 id="4-2-Other-Configuration-Options"><a href="#4-2-Other-Configuration-Options" class="headerlink" title="4.2 Other Configuration Options"></a>4.2 Other Configuration Options</h3><p>The following options can also be used to tune the performance of query execution. It is possible that these options will be deprecated in future release as more optimizations are performed automatically.</p><p>以下选项也可以用于调整查询执行的性能。随着自动执行更多优化，这些选项可能会在将来的版本中被弃用。</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>spark.sql.files.maxPartitionBytes</code></td><td align="left">134217728 (128 MB)</td><td align="left">The maximum number of bytes to pack into a single partition when reading files.</td></tr><tr><td align="left"><code>spark.sql.files.openCostInBytes</code></td><td align="left">4194304 (4 MB)</td><td align="left">The estimated cost to open a file, measured by the number of bytes could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over estimated, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</td></tr><tr><td align="left"><code>spark.sql.broadcastTimeout</code></td><td align="left">300</td><td align="left">Timeout in seconds for the broadcast wait time in broadcast joins</td></tr><tr><td align="left"><code>spark.sql.autoBroadcastJoinThreshold</code></td><td align="left">10485760 (10 MB)</td><td align="left">Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code> has been run.</td></tr><tr><td align="left"><code>spark.sql.shuffle.partitions</code></td><td align="left">200</td><td align="left">Configures the number of partitions to use when shuffling data for joins or aggregations.</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark系列（三）-- Spark Streaming编程指南</title>
    <link href="/2020/07/20/Spark-Streaming%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
    <url>/2020/07/20/Spark-Streaming%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark-Streaming编程指南"><a href="#Spark-Streaming编程指南" class="headerlink" title="Spark Streaming编程指南"></a>Spark Streaming编程指南</h1><h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like <code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s <a href="http://spark.apache.org/docs/2.1.2/ml-guide.html" target="_blank" rel="noopener">machine learning</a> and <a href="http://spark.apache.org/docs/2.1.2/graphx-programming-guide.html" target="_blank" rel="noopener">graph processing</a> algorithms on data streams.</p><p>Spark Streaming是Spark core API的一种扩展，支持可扩展性，高吞吐和容错性的实时数据流处理。数据可来源于多种系统，例如Kafka, Flume, Kinesis, or TCP sockets，同时能利用一些复杂的函数做数据处理，比如<code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>.处理的结果可以写入文件系统，数据库，实时仪表盘。实时上，也可以在数据流上使用机器学习和图计算。</p><p><img src="http://spark.apache.org/docs/2.1.2/img/streaming-arch.png" srcset="/img/loading.gif" alt=""></p><p>Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.</p><p>在内部，它的工作方式如下。Spark Streaming接收实时输入数据流，并将数据分为批次，然后由Spark引擎进行处理，以生成批次的最终结果流。</p><p><img src="http://spark.apache.org/docs/2.1.2/img/streaming-flow.png" srcset="/img/loading.gif" alt=""></p><p>Spark Streaming provides a high-level abstraction called <em>discretized stream</em> or <em>DStream</em>, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">RDDs</a>.</p><p>Spark Streaming提供了称为<em>离散流</em>或<em>DStream</em>的高级抽象，它表示连续的数据流。DStreams可以根据来自诸如Kafka，Flume和Kinesis之类的源的输入数据流来创建，也可以通过对其他DStreams应用高级操作来创建。在内部，DStream表示为<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">RDD</a>序列 。</p><p>This guide shows you how to start writing Spark Streaming programs with DStreams. You can write Spark Streaming programs in Scala, Java or Python (introduced in Spark 1.2), all of which are presented in this guide. You will find tabs throughout this guide that let you choose between code snippets of different languages.</p><p><strong>Note:</strong> There are a few APIs that are either different or not available in Python. Throughout this guide, you will find the tag <strong>Python API</strong> highlighting these differences.</p><h2 id="2-A-Quick-Example"><a href="#2-A-Quick-Example" class="headerlink" title="2. A Quick Example"></a>2. A Quick Example</h2><p>Before we go into the details of how to write your own Spark Streaming program, let’s take a quick look at what a simple Spark Streaming program looks like. Let’s say we want to count the number of words in text data received from a data server listening on a TCP socket. All you need to do is as follows.</p><p>First, we import the names of the Spark Streaming classes and some implicit conversions from StreamingContext into our environment in order to add useful methods to other classes we need (like DStream). <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.StreamingContext" target="_blank" rel="noopener">StreamingContext</a> is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second.</p><p><a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.StreamingContext" target="_blank" rel="noopener">StreamingContext</a>是所有流功能的主要入口点。我们创建具有两个执行线程和1秒批处理间隔的本地StreamingContext。</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark._<span class="hljs-keyword">import</span> org.apache.spark.streaming._<span class="hljs-keyword">import</span> org.apache.spark.streaming.<span class="hljs-type">StreamingContext</span>._ <span class="hljs-comment">// not necessary since Spark 1.3</span><span class="hljs-comment">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span><span class="hljs-comment">// The master requires 2 cores to prevent from a starvation scenario.</span><span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">"local[2]"</span>).setAppName(<span class="hljs-string">"NetworkWordCount"</span>)<span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(conf, <span class="hljs-type">Seconds</span>(<span class="hljs-number">1</span>))</code></pre><p>Using this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. <code>localhost</code>) and port (e.g. <code>9999</code>).</p><pre><code class="hljs scala"><span class="hljs-comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span><span class="hljs-keyword">val</span> lines = ssc.socketTextStream(<span class="hljs-string">"localhost"</span>, <span class="hljs-number">9999</span>)</code></pre><p>This <code>lines</code> DStream represents the stream of data that will be received from the data server. Each record in this DStream is a line of text. Next, we want to split the lines by space characters into words.</p><pre><code class="hljs scala"><span class="hljs-comment">// Split each line into words</span><span class="hljs-keyword">val</span> words = lines.flatMap(_.split(<span class="hljs-string">" "</span>))</code></pre><p><code>flatMap</code> is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream. In this case, each line will be split into multiple words and the stream of words is represented as the <code>words</code> DStream. Next, we want to count these words.</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.streaming.<span class="hljs-type">StreamingContext</span>._ <span class="hljs-comment">// not necessary since Spark 1.3</span><span class="hljs-comment">// Count each word in each batch</span><span class="hljs-keyword">val</span> pairs = words.map(word =&gt; (word, <span class="hljs-number">1</span>))<span class="hljs-keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)<span class="hljs-comment">// Print the first ten elements of each RDD generated in this DStream to the console</span>wordCounts.print()</code></pre><p>The <code>words</code> DStream is further mapped (one-to-one transformation) to a DStream of <code>(word, 1)</code> pairs, which is then reduced to get the frequency of words in each batch of data. Finally, <code>wordCounts.print()</code> will print a few of the counts generated every second.</p><p>Note that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet. To start the processing after all the transformations have been setup, we finally call</p><pre><code class="hljs scala">ssc.start()             <span class="hljs-comment">// Start the computation</span>ssc.awaitTermination()  <span class="hljs-comment">// Wait for the computation to terminate</span></code></pre><p>The complete code can be found in the Spark Streaming example <a href="https://github.com/apache/spark/blob/v2.1.2/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala" target="_blank" rel="noopener">NetworkWordCount</a>.</p><p>If you have already <a href="http://spark.apache.org/docs/2.1.2/index.html#downloading" target="_blank" rel="noopener">downloaded</a> and <a href="http://spark.apache.org/docs/2.1.2/index.html#building" target="_blank" rel="noopener">built</a> Spark, you can run this example as follows. You will first need to run Netcat (a small utility found in most Unix-like systems) as a data server by using</p><pre><code class="hljs scala">$ nc -lk <span class="hljs-number">9999</span></code></pre><p>Then, in a different terminal, you can start the example by using</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-type">SparkConf</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">QuickExample</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"NetworkWordCount"</span>).setMaster(<span class="hljs-string">"local[2]"</span>)    <span class="hljs-keyword">val</span> streamingContext = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sparkConf, <span class="hljs-type">Seconds</span>(<span class="hljs-number">1</span>))    <span class="hljs-keyword">val</span> line = streamingContext.socketTextStream(<span class="hljs-string">"localhost"</span>, <span class="hljs-number">9999</span>)    <span class="hljs-keyword">val</span> words = line.flatMap(_.split(<span class="hljs-string">" "</span>))    <span class="hljs-keyword">val</span> pairs = words.map(word =&gt; (word, <span class="hljs-number">1</span>))    <span class="hljs-keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)    wordCounts.print()    <span class="hljs-comment">//开始执行，等待执行结束</span>    streamingContext.start()    streamingContext.awaitTermination()  &#125;&#125;</code></pre><h2 id="3-Basic-Concepts"><a href="#3-Basic-Concepts" class="headerlink" title="3. Basic Concepts"></a>3. Basic Concepts</h2><p>Next, we move beyond the simple example and elaborate on the basics of Spark Streaming.</p><h3 id="3-1-Linking"><a href="#3-1-Linking" class="headerlink" title="3.1 Linking"></a>3.1 Linking</h3><p>Similar to Spark, Spark Streaming is available through Maven Central. To write your own Spark Streaming program, you will have to add the following dependency to your SBT or Maven project.</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.1.2<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></code></pre><p>For ingesting data from sources like Kafka, Flume, and Kinesis that are not present in the Spark Streaming core API, you will have to add the corresponding artifact <code>spark-streaming-xyz_2.11</code> to the dependencies. For example, some of the common ones are as follows.</p><table><thead><tr><th align="left">Source</th><th align="left">Artifact</th></tr></thead><tbody><tr><td align="left">Kafka</td><td align="left">spark-streaming-kafka-0-8_2.11</td></tr><tr><td align="left">Flume</td><td align="left">spark-streaming-flume_2.11</td></tr><tr><td align="left">Kinesis</td><td align="left">spark-streaming-kinesis-asl_2.11 [Amazon Software License]</td></tr></tbody></table><h3 id="3-2-Initializing-StreamingContext"><a href="#3-2-Initializing-StreamingContext" class="headerlink" title="3.2 Initializing StreamingContext"></a>3.2 Initializing StreamingContext</h3><p>To initialize a Spark Streaming program, a <strong>StreamingContext</strong> object has to be created which is the main entry point of all Spark Streaming functionality. <strong>StreamingContext</strong> 是Streaming程序的入口。</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark._<span class="hljs-keyword">import</span> org.apache.spark.streaming._<span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(appName).setMaster(master)<span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(conf, <span class="hljs-type">Seconds</span>(<span class="hljs-number">1</span>))</code></pre><p>The <code>appName</code> parameter is a name for your application to show on the cluster UI. <code>master</code> is a <a href="http://spark.apache.org/docs/2.1.2/submitting-applications.html#master-urls" target="_blank" rel="noopener">Spark, Mesos or YARN cluster URL</a>, or a special <strong>“local[*]”</strong> string to run in local mode. In practice, when running on a cluster, you will not want to hardcode <code>master</code> in the program, but rather <a href="http://spark.apache.org/docs/2.1.2/submitting-applications.html" target="_blank" rel="noopener">launch the application with <code>spark-submit</code></a> and receive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming in-process (detects the number of cores in the local system). Note that this internally creates a <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.SparkContext" target="_blank" rel="noopener">SparkContext</a> (starting point of all Spark functionality) which can be accessed as <code>ssc.sparkContext</code>.</p><ul><li>在本地测试时候，可以用传入local</li><li>在集群上运行时，是spark-submit传入master的，所以代码中是不用指定的</li></ul><p>The batch interval must be set based on the latency requirements of your application and available cluster resources.必须根据应用程序的延迟要求和可用的群集资源设置批处理间隔。 See the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#setting-the-right-batch-interval" target="_blank" rel="noopener">Performance Tuning</a> section for more details.</p><p>A <code>StreamingContext</code> object can also be created from an existing <code>SparkContext</code> object.</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"NetworkWordCount"</span>).setMaster(<span class="hljs-string">"local[2]"</span>)<span class="hljs-keyword">val</span> sparkContext = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)<span class="hljs-keyword">val</span> streamingContext = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sparkContext, <span class="hljs-type">Seconds</span>(<span class="hljs-number">1</span>))</code></pre><p>fter a context is defined, you have to do the following.</p><ol><li>Define the input sources by creating input DStreams.</li><li>Define the streaming computations by applying transformation and output operations to DStreams.</li><li>Start receiving data and processing it using <code>streamingContext.start()</code>.</li><li>Wait for the processing to be stopped (manually or due to any error) using <code>streamingContext.awaitTermination()</code>.</li><li>The processing can be manually stopped using <code>streamingContext.stop()</code>.</li></ol><h5 id="Points-to-remember"><a href="#Points-to-remember" class="headerlink" title="Points to remember:"></a>Points to remember:</h5><ul><li>Once a context has been started, no new streaming computations can be set up or added to it.</li><li>Once a context has been stopped, it cannot be restarted.</li><li>Only one StreamingContext can be active in a JVM at the same time.</li><li>stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of <code>stop()</code> called <code>stopSparkContext</code> to false.</li><li>A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.</li></ul><p>定义上下文后，您必须执行以下操作。</p><ol><li>通过创建输入DStream定义输入源。</li><li>通过将转换和输出操作应用于DStream来定义流计算。</li><li>开始接收数据并使用进行处理<code>streamingContext.start()</code>。</li><li>等待使用停止处理（手动或由于任何错误）<code>streamingContext.awaitTermination()</code>。</li><li>可以使用手动停止处理<code>streamingContext.stop()</code>。</li></ol><h5 id="要记住的要点："><a href="#要记住的要点：" class="headerlink" title="要记住的要点："></a>要记住的要点：</h5><ul><li>一旦启动上下文，就无法设置新的流计算或将其添加到该流计算中。</li><li>上下文一旦停止，就无法重新启动。</li><li>JVM中只能同时激活一个StreamingContext。</li><li>StreamingContext上的stop（）也会停止SparkContext。要仅停止的StreamingContext，设置可选的参数<code>stop()</code>叫做<code>stopSparkContext</code>假。</li><li>只要在创建下一个StreamingContext之前停止了上一个StreamingContext（无需停止SparkContext），就可以将SparkContext重用于创建多个StreamingContext。</li></ul><h3 id="3-3-Discretized-Streams-DStreams"><a href="#3-3-Discretized-Streams-DStreams" class="headerlink" title="3.3 Discretized Streams (DStreams)"></a>3.3 Discretized Streams (DStreams)</h3><p><strong>Discretized Stream</strong> or <strong>DStream</strong> is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset (see <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#resilient-distributed-datasets-rdds" target="_blank" rel="noopener">Spark Programming Guide</a> for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.</p><p><strong>离散流</strong>或<strong>DStream</strong>是Spark Streaming提供的基本抽象。它表示连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。在内部，DStream由一系列连续的RDD表示，这是Spark对不可变的分布式数据集的抽象（有关更多详细信息，请参见<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#resilient-distributed-datasets-rdds" target="_blank" rel="noopener">Spark编程指南</a>）。DStream中的每个RDD都包含来自特定间隔的数据，如下图所示。</p><p><img src="http://spark.apache.org/docs/2.1.2/img/streaming-dstream.png" srcset="/img/loading.gif" alt=""></p><p>Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">earlier example</a> of converting a stream of lines to words, the <code>flatMap</code> operation is applied on each RDD in the <code>lines</code> DStream to generate the RDDs of the <code>words</code> DStream. This is shown in the following figure.</p><p>在DStream上执行的任何操作都转换为对基础RDD的操作。例如，在<a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">较早</a>的将行流转换为单词的<a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">示例</a>中，该<code>flatMap</code>操作应用于<code>lines</code>DStream中的每个RDD，以生成DStream的 <code>words</code>RDD。如下图所示。</p><p><img src="http://spark.apache.org/docs/2.1.2/img/streaming-dstream-ops.png" srcset="/img/loading.gif" alt=""></p><p>These underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with a higher-level API for convenience. These operations are discussed in detail in later sections.</p><p>这些基础的RDD转换由Spark引擎计算。DStream操作隐藏了大多数这些细节，并为开发人员提供了更高级别的API，以方便使用。这些操作将在后面的部分中详细讨论。</p><h3 id="3-4-Input-DStreams-and-Receivers"><a href="#3-4-Input-DStreams-and-Receivers" class="headerlink" title="3.4 Input DStreams and Receivers"></a>3.4 Input DStreams and Receivers</h3><p>Input DStreams are DStreams representing the stream of input data received from streaming sources. In the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">quick example</a>, <code>lines</code> was an input DStream as it represented the stream of data received from the netcat server. Every input DStream (except file stream, discussed later in this section) is associated with a <strong>Receiver</strong> (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.receiver.Receiver" target="_blank" rel="noopener">Scala doc</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/streaming/receiver/Receiver.html" target="_blank" rel="noopener">Java doc</a>) object which receives the data from a source and stores it in Spark’s memory for processing.</p><p>输入DStream是表示从流源接收的输入数据流的DStream。在<a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">快速示例中</a>，<code>lines</code>输入DStream代表从netcat服务器接收的数据流。每个输入DStream（文件流除外，本节稍后将讨论）都与一个<strong>Receiver对象</strong> （<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.receiver.Receiver" target="_blank" rel="noopener">Scala doc</a>， <a href="http://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/streaming/receiver/Receiver.html" target="_blank" rel="noopener">Java doc</a>）关联，该对象从源接收数据并将其存储在Spark的内存中以进行处理。</p><p>Spark Streaming provides two categories of built-in streaming sources.</p><ul><li><em>Basic sources</em>: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.</li><li><em>Advanced sources</em>: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#linking" target="_blank" rel="noopener">linking</a> section.</li></ul><p>We are going to discuss some of the sources present in each category later in this section.</p><p>Note that, if you want to receive multiple streams of data in parallel in your streaming application, you can create multiple input DStreams (discussed further in the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#level-of-parallelism-in-data-receiving" target="_blank" rel="noopener">Performance Tuning</a> section). This will create multiple receivers which will simultaneously receive multiple data streams. But note that a Spark worker/executor is a long-running task, hence it occupies one of the cores allocated to the Spark Streaming application. Therefore, it is important to remember that a Spark Streaming application needs to be allocated enough cores (or threads, if running locally) to process the received data, as well as to run the receiver(s).</p><p>Spark Streaming提供了两类内置的流媒体源。</p><ul><li><em>基本来源</em>：可直接在StreamingContext API中获得的来源。示例：文件系统和套接字连接。</li><li><em>高级资源</em>：可以通过其他实用程序类获得诸如Kafka，Flume，Kinesis等资源。如<a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#linking" target="_blank" rel="noopener">链接</a>部分所述，这些要求针对额外的依赖项进行 <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#linking" target="_blank" rel="noopener">链接</a>。</li></ul><p>我们将在本节后面的每个类别中讨论一些资源。</p><p>请注意，如果要在流应用程序中并行接收多个数据流，则可以创建多个输入DStream（在“ <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#level-of-parallelism-in-data-receiving" target="_blank" rel="noopener">性能调整”</a>部分中进一步讨论）。这将创建多个接收器，这些接收器将同时接收多个数据流。但是请注意，Spark工作者/执行者是一项长期运行的任务，因此它占用了分配给Spark Streaming应用程序的核心之一。因此，重要的是要记住，必须为Spark Streaming应用程序分配足够的内核（或线程，如果在本地运行），以处理接收到的数据以及运行接收器。</p><h5 id="Points-to-remember-1"><a href="#Points-to-remember-1" class="headerlink" title="Points to remember"></a>Points to remember</h5><ul><li>When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using an input DStream based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. Hence, when running locally, always use “local[<em>n</em>]” as the master URL, where <em>n</em> &gt; number of receivers to run (see <a href="http://spark.apache.org/docs/2.1.2/configuration.html#spark-properties" target="_blank" rel="noopener">Spark Properties</a> for information on how to set the master).</li><li>Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. Otherwise the system will receive data, but not be able to process it.</li><li>在本地运行Spark Streaming程序时，请勿使用“ local”或“ local [1]”作为主URL。这两种方式均意味着仅一个线程将用于本地运行任务。如果您使用的是基于接收方的输入DStream（例如套接字，Kafka，Flume等），则将使用单个线程来运行接收方，而不会留下任何线程来处理接收到的数据。因此，在本地运行时，请始终使用“ local [ <em>n</em> ]”作为主URL，其中<em>n</em> &gt;要运行的接收器数（有关如何设置主服务器的信息，请参见<a href="http://spark.apache.org/docs/2.1.2/configuration.html#spark-properties" target="_blank" rel="noopener">Spark属性</a>）。</li><li>为了将逻辑扩展到在集群上运行，分配给Spark Streaming应用程序的内核数必须大于接收器数。否则，系统将接收数据，但无法处理它。</li></ul><h4 id="3-4-1-Basic-Sources"><a href="#3-4-1-Basic-Sources" class="headerlink" title="3.4.1 Basic Sources"></a>3.4.1 Basic Sources</h4><p>We have already taken a look at the <code>ssc.socketTextStream(...)</code> in the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">quick example</a> which creates a DStream from text data received over a TCP socket connection. Besides sockets, the StreamingContext API provides methods for creating DStreams from files as input sources.</p><ul><li><p><strong>File Streams:</strong> For reading data from files on any file system compatible with the HDFS API (that is, HDFS, S3, NFS, etc.), a DStream can be created as:</p><pre><code class="hljs scala">streamingContext.fileStream[<span class="hljs-type">KeyClass</span>, <span class="hljs-type">ValueClass</span>, <span class="hljs-type">InputFormatClass</span>](dataDirectory)</code></pre><ul><li><p>Spark Streaming will monitor the directory <code>dataDirectory</code> and process any files created in that directory (files written in nested directories not supported). Note that</p><ul><li>The files must have the same data format.</li><li>The files must be created in the <code>dataDirectory</code> by atomically <em>moving</em> or <em>renaming</em> them into the data directory.</li><li>Once moved, the files must not be changed. So if the files are being continuously appended, the new data will not be read.</li></ul><p>For simple text files, there is an easier method <code>streamingContext.textFileStream(dataDirectory)</code>. And file streams do not require running a receiver, hence does not require allocating cores.</p><p><strong>Python API</strong> <code>fileStream</code> is not available in the Python API, only <code>textFileStream</code> is available.</p></li><li><p><strong>Streams based on Custom Receivers:</strong> DStreams can be created with data streams received through custom receivers. See the <a href="http://spark.apache.org/docs/2.1.2/streaming-custom-receivers.html" target="_blank" rel="noopener">Custom Receiver Guide</a> for more details.</p></li><li><p><strong>Queue of RDDs as a Stream:</strong> For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using <code>streamingContext.queueStream(queueOfRDDs)</code>. Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream.</p></li></ul><p>For more details on streams from sockets and files, see the API documentations of the relevant functions in <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.StreamingContext" target="_blank" rel="noopener">StreamingContext</a> for Scala, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html" target="_blank" rel="noopener">JavaStreamingContext</a> for Java, and <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext" target="_blank" rel="noopener">StreamingContext</a> for Python.</p><ul><li><p>Spark Streaming将监视目录<code>dataDirectory</code>并处理在该目录中创建的任何文件（不支持在嵌套目录中写入的文件）。注意</p><ul><li>文件必须具有相同的数据格式。</li><li>必须<code>dataDirectory</code>通过原子地<em>将</em>文件<em>移动</em>或<em>重命名</em>到数据目录中来创建文件。</li><li>移动后，不得更改文件。因此，如果文件被连续追加，则不会读取新数据。</li></ul><p>对于简单的文本文件，有一个更简单的方法<code>streamingContext.textFileStream(dataDirectory)</code>。文件流不需要运行接收器，因此不需要分配内核。</p><p><strong>Python API</strong> <code>fileStream</code>在<strong>Python API</strong>中不可用，仅 <code>textFileStream</code>可用。</p></li><li><p><strong>基于自定义接收器的流：</strong>可以使用通过自定义接收器接收的数据流创建DStream。有关更多详细信息，请参见《<a href="http://spark.apache.org/docs/2.1.2/streaming-custom-receivers.html" target="_blank" rel="noopener">定制接收器指南》</a>。</p></li><li><p><strong>RDD队列作为流：</strong>为了使用测试数据测试Spark Streaming应用程序，还可以使用，基于RDD队列创建DStream <code>streamingContext.queueStream(queueOfRDDs)</code>。推送到队列中的每个RDD将被视为DStream中的一批数据，并像流一样进行处理。</p></li></ul><p>有关套接字和文件流的更多详细信息，请参阅<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.StreamingContext" target="_blank" rel="noopener">StreamingContext</a> for Scala，<a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html" target="_blank" rel="noopener">JavaStreamingContext</a> for Java和<a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext" target="_blank" rel="noopener">StreamingContext</a> for Python中相关功能的API文档 。</p></li></ul><h4 id="3-4-2-Advanced-Sources"><a href="#3-4-2-Advanced-Sources" class="headerlink" title="3.4.2 Advanced Sources"></a>3.4.2 Advanced Sources</h4><p>  <strong>Python API</strong> As of Spark 2.1.2, out of these sources, Kafka, Kinesis and Flume are available in the Python API.</p><p>  This category of sources require interfacing with external non-Spark libraries, some of them with complex dependencies (e.g., Kafka and Flume). Hence, to minimize issues related to version conflicts of dependencies, the functionality to create DStreams from these sources has been moved to separate libraries that can be <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#linking" target="_blank" rel="noopener">linked</a> to explicitly when necessary.</p><p>  Note that these advanced sources are not available in the Spark shell, hence applications based on these advanced sources cannot be tested in the shell. If you really want to use them in the Spark shell you will have to download the corresponding Maven artifact’s JAR along with its dependencies and add it to the classpath.</p><p>  <strong>Python API</strong>从Spark 2.1.2开始，<strong>Python API</strong>中提供了上述来源中的Kafka，Kinesis和Flume。</p><p>  这类来源需要与外部非Spark库进行接口，其中一些库具有复杂的依存关系（例如，Kafka和Flume）。因此，为了最大程度地减少与依赖项版本冲突有关的问题，已从这些源创建DStream的功能已移至单独的库，可以在必要时显式<a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#linking" target="_blank" rel="noopener">链接</a>到这些库。</p><p>  请注意，Spark Shell中没有这些高级源，因此无法在Shell中测试基于这些高级源的应用程序。如果您真的想在Spark shell中使用它们，则必须下载相应的Maven工件的JAR及其依赖项，并将其添加到类路径中。</p><p>  Some of these advanced sources are as follows.</p><ul><li><strong>Kafka:</strong> Spark Streaming 2.1.2 is compatible with Kafka broker versions 0.8.2.1 or higher. See the <a href="http://spark.apache.org/docs/2.1.2/streaming-kafka-integration.html" target="_blank" rel="noopener">Kafka Integration Guide</a> for more details.</li><li><strong>Flume:</strong> Spark Streaming 2.1.2 is compatible with Flume 1.6.0. See the <a href="http://spark.apache.org/docs/2.1.2/streaming-flume-integration.html" target="_blank" rel="noopener">Flume Integration Guide</a> for more details.</li><li><strong>Kinesis:</strong> Spark Streaming 2.1.2 is compatible with Kinesis Client Library 1.2.1. See the <a href="http://spark.apache.org/docs/2.1.2/streaming-kinesis-integration.html" target="_blank" rel="noopener">Kinesis Integration Guide</a> for more details.</li></ul><h4 id="3-4-3-Custom-Sources"><a href="#3-4-3-Custom-Sources" class="headerlink" title="3.4.3 Custom Sources"></a>3.4.3 Custom Sources</h4><p>  <strong>Python API</strong> This is not yet supported in Python.</p><p>  Input DStreams can also be created out of custom data sources. All you have to do is implement a user-defined <strong>receiver</strong> (see next section to understand what that is) that can receive data from the custom sources and push it into Spark. See the <a href="http://spark.apache.org/docs/2.1.2/streaming-custom-receivers.html" target="_blank" rel="noopener">Custom Receiver Guide</a> for details.</p><p>  <strong>Python API Python</strong>尚不支持此功能。</p><p>  输入DStream也可以从自定义数据源中创建。您所需要做的就是实现一个用户定义的<strong>接收器</strong>（请参阅下一节以了解其含义），该接收器可以接收来自自定义源的数据并将其推送到Spark中。有关详细信息，请参见《<a href="http://spark.apache.org/docs/2.1.2/streaming-custom-receivers.html" target="_blank" rel="noopener">定制接收器指南</a>》。</p><h4 id="3-4-4-Receiver-Reliability"><a href="#3-4-4-Receiver-Reliability" class="headerlink" title="3.4.4 Receiver Reliability"></a>3.4.4 Receiver Reliability</h4><p>  There can be two kinds of data sources based on their <em>reliability</em>. Sources (like Kafka and Flume) allow the transferred data to be acknowledged. If the system receiving data from these <em>reliable</em> sources acknowledges the received data correctly, it can be ensured that no data will be lost due to any kind of failure. This leads to two kinds of receivers:</p><pre><code>1. *Reliable Receiver* - A *reliable receiver* correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication.2. *Unreliable Receiver* - An *unreliable receiver* does *not* send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment.</code></pre><p>  The details of how to write a reliable receiver are discussed in the <a href="http://spark.apache.org/docs/2.1.2/streaming-custom-receivers.html" target="_blank" rel="noopener">Custom Receiver Guide</a>.</p><p>  根据数据<em>可靠性，</em>可以有两种数据源。源（例如Kafka和Flume）允许确认传输的数据。如果从这些<em>可靠</em>来源接收数据的系统正确地确认了接收到的数据，则可以确保不会由于任何类型的故障而丢失任何数据。这导致两种接收器：</p><pre><code>1. *可靠的接收器* - *可靠的接收器*在接收到数据并通过复制将其存储在Spark中后，会正确地将确认发送到可靠的源。2. *不可靠的接收器* -一个*不可靠的接收器*并*没有*发送确认的资源等。可以将其用于不支持确认的来源，甚至可以用于不希望或不需要进入确认复杂性的可靠来源。</code></pre><p>  《<a href="http://spark.apache.org/docs/2.1.2/streaming-custom-receivers.html" target="_blank" rel="noopener">定制接收器指南》</a>中讨论了如何编写可靠的接收器的详细信息 。</p><h3 id="3-5-Transformations-on-DStreams"><a href="#3-5-Transformations-on-DStreams" class="headerlink" title="3.5 Transformations on DStreams"></a>3.5 Transformations on DStreams</h3><p>Similar to that of RDDs, transformations allow the data from the input DStream to be modified. DStreams support many of the transformations available on normal Spark RDD’s. Some of the common ones are as follows.</p><table><thead><tr><th align="left">Transformation</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>map</strong>(<em>func</em>)</td><td align="left">Return a new DStream by passing each element of the source DStream through a function <em>func</em>.</td></tr><tr><td align="left"><strong>flatMap</strong>(<em>func</em>)</td><td align="left">Similar to map, but each input item can be mapped to 0 or more output items.</td></tr><tr><td align="left"><strong>filter</strong>(<em>func</em>)</td><td align="left">Return a new DStream by selecting only the records of the source DStream on which <em>func</em> returns true.</td></tr><tr><td align="left"><strong>repartition</strong>(<em>numPartitions</em>)</td><td align="left">Changes the level of parallelism in this DStream by creating more or fewer partitions.</td></tr><tr><td align="left"><strong>union</strong>(<em>otherStream</em>)</td><td align="left">Return a new DStream that contains the union of the elements in the source DStream and <em>otherDStream</em>.</td></tr><tr><td align="left"><strong>count</strong>()</td><td align="left">Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</td></tr><tr><td align="left"><strong>reduce</strong>(<em>func</em>)</td><td align="left">Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function <em>func</em> (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</td></tr><tr><td align="left"><strong>countByValue</strong>()</td><td align="left">When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</td></tr><tr><td align="left"><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td><td align="left">When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. <strong>Note:</strong> By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property <code>spark.default.parallelism</code>) to do the grouping. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td></tr><tr><td align="left"><strong>join</strong>(<em>otherStream</em>, [<em>numTasks</em>])</td><td align="left">When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</td></tr><tr><td align="left"><strong>cogroup</strong>(<em>otherStream</em>, [<em>numTasks</em>])</td><td align="left">When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</td></tr><tr><td align="left"><strong>transform</strong>(<em>func</em>)</td><td align="left">Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.</td></tr><tr><td align="left"><strong>updateStateByKey</strong>(<em>func</em>)</td><td align="left">Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key</td></tr></tbody></table><p>A few of these transformations are worth discussing in more detail.</p><h4 id="3-5-1-UpdateStateByKey-Operation"><a href="#3-5-1-UpdateStateByKey-Operation" class="headerlink" title="3.5.1 UpdateStateByKey Operation"></a>3.5.1 UpdateStateByKey Operation</h4><p>The <code>updateStateByKey</code> operation allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps.</p><ol><li>Define the state - The state can be an arbitrary data type.</li><li>Define the state update function - Specify with a function how to update the state using the previous state and the new values from an input stream.</li></ol><p>In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns <code>None</code> then the key-value pair will be eliminated.</p><p>Let’s illustrate this with an example. Say you want to maintain a running count of each word seen in a text data stream. Here, the running count is the state and it is an integer. We define the update function as:</p><p>该<code>updateStateByKey</code>操作使您可以保持任意状态，同时不断用新信息更新它。要使用此功能，您将必须执行两个步骤。</p><ol><li>定义状态-状态可以是任意数据类型。</li><li>定义状态更新功能-使用功能指定如何使用输入流中的先前状态和新值来更新状态。</li></ol><p>在每个批次中，Spark都会对所有现有密钥应用状态更新功能，无论它们是否在批次中具有新数据。如果更新函数返回，<code>None</code>则将删除键值对。</p><p>让我们用一个例子来说明。假设您要保持在文本数据流中看到的每个单词的连续计数。此处，运行计数是状态，它是整数。我们将更新函数定义为</p><p><strong>不理解，具体事例有哪些？</strong></p><pre><code class="hljs scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">updateFunction</span></span>(newValues: <span class="hljs-type">Seq</span>[<span class="hljs-type">Int</span>], runningCount: <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>]): <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>] = &#123;    <span class="hljs-keyword">val</span> newCount = ...  <span class="hljs-comment">// add the new values with the previous running count to get the new count</span>    <span class="hljs-type">Some</span>(newCount)&#125;</code></pre><p>This is applied on a DStream containing words (say, the <code>pairs</code> DStream containing <code>(word, 1)</code> pairs in the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">earlier example</a>).</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> runningCounts = pairs.updateStateByKey[<span class="hljs-type">Int</span>](updateFunction _)</code></pre><p>The update function will be called for each word, with <code>newValues</code> having a sequence of 1’s (from the <code>(word, 1)</code> pairs) and the <code>runningCount</code> having the previous count.</p><p>Note that using <code>updateStateByKey</code> requires the checkpoint directory to be configured, which is discussed in detail in the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#checkpointing" target="_blank" rel="noopener">checkpointing</a> section.</p><p>将为每个单词调用更新函数，每个单词<code>newValues</code>的序列为1（来自各<code>(word, 1)</code>对），并<code>runningCount</code>具有先前的计数。</p><p>请注意，使用<code>updateStateByKey</code>需要配置检查点目录，这将在<a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#checkpointing" target="_blank" rel="noopener">检查点</a>部分中详细讨论。</p><p>对原来的状态就行记录，比如wordcount中，不断的累计单词出现的次数，flink中自动做到了。</p><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-type">SparkConf</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WorldCount</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]) &#123;    <span class="hljs-comment">// 定义更新状态方法，参数values为当前批次单词频度，state为以往批次单词频度</span>    <span class="hljs-keyword">val</span> updateFunc = (values: <span class="hljs-type">Seq</span>[<span class="hljs-type">Int</span>], state: <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>]) =&gt; &#123;      <span class="hljs-keyword">val</span> currentCount = values.foldLeft(<span class="hljs-number">0</span>)(_ + _)      <span class="hljs-keyword">val</span> previousCount = state.getOrElse(<span class="hljs-number">0</span>)      <span class="hljs-type">Some</span>(currentCount + previousCount)    &#125;    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">"local[2]"</span>).setAppName(<span class="hljs-string">"NetworkWordCount"</span>)    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(conf, <span class="hljs-type">Seconds</span>(<span class="hljs-number">3</span>))    ssc.checkpoint(<span class="hljs-string">"hdfs://hadoop102:9000/streamCheck"</span>)    <span class="hljs-comment">// Create a DStream that will connect to hostname:port, like hadoop102:9999</span>    <span class="hljs-keyword">val</span> lines = ssc.socketTextStream(<span class="hljs-string">"hadoop102"</span>, <span class="hljs-number">9999</span>)    <span class="hljs-comment">// Split each line into words</span>    <span class="hljs-keyword">val</span> words = lines.flatMap(_.split(<span class="hljs-string">" "</span>))    <span class="hljs-comment">//import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3</span>    <span class="hljs-comment">// Count each word in each batch</span>    <span class="hljs-keyword">val</span> pairs = words.map(word =&gt; (word, <span class="hljs-number">1</span>))    <span class="hljs-comment">// 使用updateStateByKey来更新状态，统计从运行开始以来单词总的次数</span>    <span class="hljs-keyword">val</span> stateDstream = pairs.updateStateByKey[<span class="hljs-type">Int</span>](updateFunc)    stateDstream.print()    <span class="hljs-comment">//val wordCounts = pairs.reduceByKey(_ + _)</span>    <span class="hljs-comment">// Print the first ten elements of each RDD generated in this DStream to the console</span>    <span class="hljs-comment">//wordCounts.print()</span>    ssc.start()             <span class="hljs-comment">// Start the computation</span>    ssc.awaitTermination()  <span class="hljs-comment">// Wait for the computation to terminate</span>    <span class="hljs-comment">//ssc.stop()</span>  &#125;&#125;</code></pre><h4 id="3-5-2-Transform-Operation"><a href="#3-5-2-Transform-Operation" class="headerlink" title="3.5.2 Transform Operation"></a>3.5.2 Transform Operation</h4><p>The <code>transform</code> operation (along with its variations like <code>transformWith</code>) allows arbitrary RDD-to-RDD functions to be applied on a DStream. It can be used to apply any RDD operation that is not exposed in the DStream API. For example, the functionality of joining every batch in a data stream with another dataset is not directly exposed in the DStream API. However, you can easily use <code>transform</code> to do this. This enables very powerful possibilities. For example, one can do real-time data cleaning by joining the input data stream with precomputed spam information (maybe generated with Spark as well) and then filtering based on it.</p><p>该<code>transform</code>操作（及其类似的变体<code>transformWith</code>）允许将任意RDD-to-RDD功能应用于DStream。它可用于应用DStream API中未公开的任何RDD操作。例如，将数据流中的每个批次与另一个数据集连接在一起的功能未直接在DStream API中公开。但是，您可以轻松地使用<code>transform</code>它。这实现了非常强大的可能性。例如，可以通过将输入数据流与预先计算的垃圾邮件信息（也可能由Spark生成）结合在一起，然后基于该信息进行过滤来进行实时数据清理。</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) <span class="hljs-comment">// RDD containing spam information</span><span class="hljs-keyword">val</span> cleanedDStream = wordCounts.transform &#123; rdd =&gt;  rdd.join(spamInfoRDD).filter(...) <span class="hljs-comment">// join data stream with spam information to do data cleaning</span>  ...&#125;</code></pre><p>Note that the supplied function gets called in every batch interval. This allows you to do time-varying RDD operations, that is, RDD operations, number of partitions, broadcast variables, etc. can be changed between batches.</p><p>请注意，在每个批处理间隔中都会调用提供的函数。这使您可以执行随时间变化的RDD操作，即可以在批之间更改RDD操作，分区数，广播变量等。</p><p>Transform 能进行rdd to rdd的转换，能实现DStream中没有提供的操作。</p><h4 id="3-5-3-Window-Operations"><a href="#3-5-3-Window-Operations" class="headerlink" title="3.5.3 Window Operations"></a>3.5.3 Window Operations</h4><p>Spark Streaming also provides <em>windowed computations</em>, which allow you to apply transformations over a sliding window of data. The following figure illustrates this sliding window.</p><p><img src="http://spark.apache.org/docs/2.1.2/img/streaming-dstream-window.png" srcset="/img/loading.gif" alt=""></p><p>As shown in the figure, every time the window <em>slides</em> over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream. In this specific case, the operation is applied over the last 3 time units of data, and slides by 2 time units. This shows that any window operation needs to specify two parameters.</p><ul><li><em>window length</em> - The duration of the window (3 in the figure).</li><li><em>sliding interval</em> - The interval at which the window operation is performed (2 in the figure).</li></ul><p>These two parameters must be multiples of the batch interval of the source DStream (1 in the figure).</p><p>如该图所示，每当窗口<em>滑动</em>在源DSTREAM，落入窗口内的源RDDS被组合及操作以产生RDDS的窗DSTREAM。在此特定情况下，该操作将应用于数据的最后3个时间单位，并以2个时间单位滑动。这表明任何窗口操作都需要指定两个参数。</p><ul><li><em>窗口长度</em> - <em>窗口</em>的持续时间（图中3）。</li><li><em>滑动间隔</em> -执行窗口操作的间隔（图中为2）。</li></ul><p>这两个参数必须是源DStream的批处理间隔的倍数（图中为1）。</p><p>Let’s illustrate the window operations with an example. Say, you want to extend the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">earlier example</a> by generating word counts over the last 30 seconds of data, every 10 seconds. To do this, we have to apply the <code>reduceByKey</code> operation on the <code>pairs</code> DStream of <code>(word, 1)</code> pairs over the last 30 seconds of data. This is done using the operation <code>reduceByKeyAndWindow</code>.</p><p>让我们用一个例子来说明窗口操作。假设您要扩展 <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">前面的示例</a>，方法是每10秒在数据的最后30秒生成一次字数统计。为此，我们必须在最后30秒的数据<code>reduceByKey</code>上对<code>pairs</code>DStream <code>(word, 1)</code>对应用该操作。这是通过操作完成的<code>reduceByKeyAndWindow</code>。</p><pre><code class="hljs scala"><span class="hljs-comment">// Reduce last 30 seconds of data, every 10 seconds</span><span class="hljs-keyword">val</span> windowedWordCounts = pairs.reduceByKeyAndWindow((a:<span class="hljs-type">Int</span>,b:<span class="hljs-type">Int</span>) =&gt; (a + b), <span class="hljs-type">Seconds</span>(<span class="hljs-number">30</span>), <span class="hljs-type">Seconds</span>(<span class="hljs-number">10</span>))</code></pre><p>Some of the common window operations are as follows. All of these operations take the said two parameters - <em>windowLength</em> and <em>slideInterval</em>.</p><table><thead><tr><th align="left">Transformation</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>window</strong>(<em>windowLength</em>, <em>slideInterval</em>)</td><td align="left">Return a new DStream which is computed based on windowed batches of the source DStream.</td></tr><tr><td align="left"><strong>countByWindow</strong>(<em>windowLength</em>, <em>slideInterval</em>)</td><td align="left">Return a sliding window count of elements in the stream.</td></tr><tr><td align="left"><strong>reduceByWindow</strong>(<em>func</em>, <em>windowLength</em>, <em>slideInterval</em>)</td><td align="left">Return a new single-element stream, created by aggregating elements in the stream over a sliding interval using <em>func</em>. The function should be associative and commutative so that it can be computed correctly in parallel.</td></tr><tr><td align="left"><strong>reduceByKeyAndWindow</strong>(<em>func</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td><td align="left">When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function <em>func</em> over batches in a sliding window. <strong>Note:</strong> By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property <code>spark.default.parallelism</code>) to do the grouping. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td></tr><tr><td align="left"><strong>reduceByKeyAndWindow</strong>(<em>func</em>, <em>invFunc</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td><td align="left">A more efficient version of the above <code>reduceByKeyAndWindow()</code> where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter <em>invFunc</em>). Like in <code>reduceByKeyAndWindow</code>, the number of reduce tasks is configurable through an optional argument. Note that <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#checkpointing" target="_blank" rel="noopener">checkpointing</a> must be enabled for using this operation.</td></tr><tr><td align="left"><strong>countByValueAndWindow</strong>(<em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td><td align="left">When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in <code>reduceByKeyAndWindow</code>, the number of reduce tasks is configurable through an optional argument.</td></tr></tbody></table><h4 id="3-5-4-Join-Operations"><a href="#3-5-4-Join-Operations" class="headerlink" title="3.5.4 Join Operations"></a>3.5.4 Join Operations</h4><p>Finally, its worth highlighting how easily you can perform different kinds of joins in Spark Streaming.</p><h5 id="Stream-stream-joins"><a href="#Stream-stream-joins" class="headerlink" title="Stream-stream joins"></a>Stream-stream joins</h5><p>Streams can be very easily joined with other streams.</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> stream1: <span class="hljs-type">DStream</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>] = ...<span class="hljs-keyword">val</span> stream2: <span class="hljs-type">DStream</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>] = ...<span class="hljs-keyword">val</span> joinedStream = stream1.join(stream2)</code></pre><p>Here, in each batch interval, the RDD generated by <code>stream1</code> will be joined with the RDD generated by <code>stream2</code>. You can also do <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, <code>fullOuterJoin</code>. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well.</p><p>在此，在每个批处理间隔中，生成的RDD <code>stream1</code>将与生成的RDD合并在一起<code>stream2</code>。你也可以做<code>leftOuterJoin</code>，<code>rightOuterJoin</code>，<code>fullOuterJoin</code>。此外，在流的窗口上进行联接通常非常有用。这也很容易。</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> windowedStream1 = stream1.window(<span class="hljs-type">Seconds</span>(<span class="hljs-number">20</span>))<span class="hljs-keyword">val</span> windowedStream2 = stream2.window(<span class="hljs-type">Minutes</span>(<span class="hljs-number">1</span>))<span class="hljs-keyword">val</span> joinedStream = windowedStream1.join(windowedStream2)</code></pre><h5 id="Stream-dataset-joins"><a href="#Stream-dataset-joins" class="headerlink" title="Stream-dataset joins"></a>Stream-dataset joins</h5><p>This has already been shown earlier while explain <code>DStream.transform</code> operation. Here is yet another example of joining a windowed stream with a dataset.</p><p>这已经在前面解释<code>DStream.transform</code>操作时显示过了。这是将窗口流与数据集结合在一起的另一个示例。类似黑名单例子。</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> dataset: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>] = ...<span class="hljs-keyword">val</span> windowedStream = stream.window(<span class="hljs-type">Seconds</span>(<span class="hljs-number">20</span>))...<span class="hljs-keyword">val</span> joinedStream = windowedStream.transform &#123; rdd =&gt; rdd.join(dataset) &#125;</code></pre><p>In fact, you can also dynamically change the dataset you want to join against. The function provided to <code>transform</code> is evaluated every batch interval and therefore will use the current dataset that <code>dataset</code> reference points to.</p><p>The complete list of DStream transformations is available in the API documentation. For the Scala API, see <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.dstream.DStream" target="_blank" rel="noopener">DStream</a> and <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.dstream.PairDStreamFunctions" target="_blank" rel="noopener">PairDStreamFunctions</a>. For the Java API, see <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/streaming/api/java/JavaDStream.html" target="_blank" rel="noopener">JavaDStream</a> and <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/streaming/api/java/JavaPairDStream.html" target="_blank" rel="noopener">JavaPairDStream</a>. For the Python API, see <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.streaming.html#pyspark.streaming.DStream" target="_blank" rel="noopener">DStream</a>.</p><p>实际上，您还可以动态更改要加入的数据集。<code>transform</code>每个批次间隔都会评估提供给该函数的功能，因此将使用<code>dataset</code>参考所指向的当前数据集。 </p><p>API文档中提供了DStream转换的完整列表。有关Scala API，请参见<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.dstream.DStream" target="_blank" rel="noopener">DStream</a> 和<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.streaming.dstream.PairDStreamFunctions" target="_blank" rel="noopener">PairDStreamFunctions</a>。有关Java API，请参见<a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/streaming/api/java/JavaDStream.html" target="_blank" rel="noopener">JavaDStream</a> 和<a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/streaming/api/java/JavaPairDStream.html" target="_blank" rel="noopener">JavaPairDStream</a>。有关Python API，请参见<a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.streaming.html#pyspark.streaming.DStream" target="_blank" rel="noopener">DStream</a>。</p><h3 id="3-6-Output-Operations-on-DStreams"><a href="#3-6-Output-Operations-on-DStreams" class="headerlink" title="3.6 Output Operations on DStreams"></a>3.6 Output Operations on DStreams</h3><p>Output operations allow DStream’s data to be pushed out to external systems like a database or a file systems. Since the output operations actually allow the transformed data to be consumed by external systems, they trigger the actual execution of all the DStream transformations (similar to actions for RDDs). Currently, the following output operations are defined:</p><p>输出操作允许将DStream的数据推出到外部系统，例如数据库或文件系统。由于输出操作实际上允许外部系统使用转换后的数据，因此它们会触发所有DStream转换的实际执行（类似于RDD的操作）。当前，定义了以下输出操作：</p><table><thead><tr><th align="left">Output Operation</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>print</strong>()</td><td align="left">Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. <strong>Python API</strong> This is called <strong>pprint()</strong> in the Python API.</td></tr><tr><td align="left"><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td align="left">Save this DStream’s contents as text files. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: *”prefix-TIME_IN_MS[.suffix]”*.</td></tr><tr><td align="left"><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td align="left">Save this DStream’s contents as <code>SequenceFiles</code> of serialized Java objects. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: *”prefix-TIME_IN_MS[.suffix]”<em>. *</em>Python API** This is not available in the Python API.</td></tr><tr><td align="left"><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td align="left">Save this DStream’s contents as Hadoop files. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: *”prefix-TIME_IN_MS[.suffix]”<em>. *</em>Python API** This is not available in the Python API.</td></tr><tr><td align="left"><strong>foreachRDD</strong>(<em>func</em>)</td><td align="left">The most generic output operator that applies a function, <em>func</em>, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function <em>func</em> is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs.</td></tr></tbody></table><h3 id="3-7-Design-Patterns-for-using-foreachRDD"><a href="#3-7-Design-Patterns-for-using-foreachRDD" class="headerlink" title="3.7 Design Patterns for using foreachRDD"></a>3.7 Design Patterns for using foreachRDD</h3><p><code>dstream.foreachRDD</code> is a powerful primitive that allows data to be sent out to external systems. However, it is important to understand how to use this primitive correctly and efficiently. Some of the common mistakes to avoid are as follows.</p><p>Often writing data to external system requires creating a connection object (e.g. TCP connection to a remote server) and using it to send data to a remote system. For this purpose, a developer may inadvertently try creating a connection object at the Spark driver, and then try to use it in a Spark worker to save records in the RDDs. For example (in Scala),</p><p><code>dstream.foreachRDD</code>是一个强大的原语，可以将数据发送到外部系统。但是，重要的是要了解如何正确有效地使用此原语。应避免的一些常见错误如下。</p><p>通常，将数据写入外部系统需要创建一个连接对象（例如，到远程服务器的TCP连接），并使用该对象将数据发送到远程系统。为此，开发人员可能会无意间尝试在Spark驱动程序中创建连接对象，然后尝试在Spark辅助程序中使用该对象以将记录保存在RDD中。例如（在Scala中），</p><pre><code class="hljs scala">dstream.foreachRDD &#123; rdd =&gt;  <span class="hljs-keyword">val</span> connection = createNewConnection()  <span class="hljs-comment">// executed at the driver</span>  rdd.foreach &#123; record =&gt;    connection.send(record) <span class="hljs-comment">// executed at the worker</span>  &#125;&#125;</code></pre><p>This is incorrect as this requires the connection object to be serialized and sent from the driver to the worker. Such connection objects are rarely transferable across machines. This error may manifest as serialization errors (connection object not serializable), initialization errors (connection object needs to be initialized at the workers), etc. The correct solution is to create the connection object at the worker.</p><p>However, this can lead to another common mistake - creating a new connection for every record. For example,</p><p>这是不正确的，因为这需要将连接对象序列化并从驱动程序发送给工作程序。这样的连接对象很少能在机器之间转移。此错误可能表现为序列化错误（连接对象不可序列化），初始化错误（连接对象需要在工作程序中初始化）等。正确的解决方案是在工作程序中创建连接对象。</p><p>但是，这可能会导致另一个常见错误-为每个记录创建一个新的连接。例如，</p><pre><code class="hljs scala">dstream.foreachRDD &#123; rdd =&gt;  rdd.foreach &#123; record =&gt;    <span class="hljs-keyword">val</span> connection = createNewConnection()    connection.send(record)    connection.close()  &#125;&#125;</code></pre><p>Typically, creating a connection object has time and resource overheads. Therefore, creating and destroying a connection object for each record can incur unnecessarily high overheads and can significantly reduce the overall throughput of the system. A better solution is to use <code>rdd.foreachPartition</code> - create a single connection object and send all the records in a RDD partition using that connection.</p><p>通常，创建连接对象会浪费时间和资源。因此，为每个记录创建和销毁连接对象会导致不必要的高开销，并且会大大降低系统的整体吞吐量。更好的解决方案是使用 <code>rdd.foreachPartition</code>-创建单个连接对象，并使用该连接在RDD分区中发送所有记录。</p><pre><code class="hljs scala">dstream.foreachRDD &#123; rdd =&gt;  rdd.foreachPartition &#123; partitionOfRecords =&gt;    <span class="hljs-keyword">val</span> connection = createNewConnection()    partitionOfRecords.foreach(record =&gt; connection.send(record))    connection.close()  &#125;&#125;</code></pre><p>This amortizes the connection creation overheads over many records.</p><p>Finally, this can be further optimized by reusing connection objects across multiple RDDs/batches. One can maintain a static pool of connection objects than can be reused as RDDs of multiple batches are pushed to the external system, thus further reducing the overheads.</p><p>这将分摊许多记录上的连接创建开销。</p><p>最后，可以通过在多个RDD /批次之间重用连接对象来进一步优化。与将多个批次的RDD推送到外部系统时可以重用的连接对象相比，它可以维护一个静态的连接对象池，从而进一步减少了开销。</p><pre><code class="hljs scala">dstream.foreachRDD &#123; rdd =&gt;  rdd.foreachPartition &#123; partitionOfRecords =&gt;    <span class="hljs-comment">// ConnectionPool is a static, lazily initialized pool of connections</span>    <span class="hljs-keyword">val</span> connection = <span class="hljs-type">ConnectionPool</span>.getConnection()    partitionOfRecords.foreach(record =&gt; connection.send(record))    <span class="hljs-type">ConnectionPool</span>.returnConnection(connection)  <span class="hljs-comment">// return to the pool for future reuse</span>  &#125;&#125;</code></pre><p>Note that the connections in the pool should be lazily created on demand and timed out if not used for a while. This achieves the most efficient sending of data to external systems.</p><p>请注意，应按需延迟创建池中的连接，如果一段时间不使用，则超时。这样可以最有效地将数据发送到外部系统。</p><h5 id="Other-points-to-remember"><a href="#Other-points-to-remember" class="headerlink" title="Other points to remember:"></a>Other points to remember:</h5><ul><li>DStreams are executed lazily by the output operations, just like RDDs are lazily executed by RDD actions. Specifically, RDD actions inside the DStream output operations force the processing of the received data. Hence, if your application does not have any output operation, or has output operations like <code>dstream.foreachRDD()</code> without any RDD action inside them, then nothing will get executed. The system will simply receive the data and discard it.</li><li>By default, output operations are executed one-at-a-time. And they are executed in the order they are defined in the application.</li></ul><h5 id="其他要记住的要点："><a href="#其他要记住的要点：" class="headerlink" title="其他要记住的要点："></a>其他要记住的要点：</h5><ul><li>DStream由输出操作延迟执行，就像RDD由RDD操作延迟执行一样。具体来说，DStream输出操作内部的RDD动作会强制处理接收到的数据。因此，如果您的应用程序没有任何输出操作，或者<code>dstream.foreachRDD()</code>内部没有任何RDD操作，就不会执行任何输出操作。系统将仅接收数据并将其丢弃。</li><li>默认情况下，输出操作一次执行一次。它们按照在应用程序中定义的顺序执行。</li></ul><h3 id="3-8-DataFrame-and-SQL-Operations"><a href="#3-8-DataFrame-and-SQL-Operations" class="headerlink" title="3.8 DataFrame and SQL Operations"></a>3.8 DataFrame and SQL Operations</h3><p>You can easily use <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html" target="_blank" rel="noopener">DataFrames and SQL</a> operations on streaming data. You have to create a SparkSession using the SparkContext that the StreamingContext is using. Furthermore this has to done such that it can be restarted on driver failures. This is done by creating a lazily instantiated singleton instance of SparkSession. This is shown in the following example. It modifies the earlier <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">word count example</a> to generate word counts using DataFrames and SQL. Each RDD is converted to a DataFrame, registered as a temporary table and then queried using SQL.</p><p>您可以轻松地对流数据使用<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html" target="_blank" rel="noopener">DataFrames和SQL</a>操作。您必须使用StreamingContext使用的SparkContext创建一个SparkSession。此外，必须这样做，以便可以在驱动程序故障时重新启动它。这是通过创建SparkSession的延迟实例化单例实例来完成的。在下面的示例中显示。它修改了前面的<a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">单词计数示例，</a>以使用DataFrames和SQL生成单词计数。每个RDD都转换为一个DataFrame，注册为临时表，然后使用SQL查询。</p><pre><code class="hljs scala"><span class="hljs-comment">/** DataFrame operations inside your streaming program */</span><span class="hljs-keyword">val</span> words: <span class="hljs-type">DStream</span>[<span class="hljs-type">String</span>] = ...words.foreachRDD &#123; rdd =&gt;  <span class="hljs-comment">// Get the singleton instance of SparkSession</span>  <span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>.builder.config(rdd.sparkContext.getConf).getOrCreate()  <span class="hljs-keyword">import</span> spark.implicits._  <span class="hljs-comment">// Convert RDD[String] to DataFrame</span>  <span class="hljs-keyword">val</span> wordsDataFrame = rdd.toDF(<span class="hljs-string">"word"</span>)  <span class="hljs-comment">// Create a temporary view</span>  wordsDataFrame.createOrReplaceTempView(<span class="hljs-string">"words"</span>)  <span class="hljs-comment">// Do word count on DataFrame using SQL and print it</span>  <span class="hljs-keyword">val</span> wordCountsDataFrame =     spark.sql(<span class="hljs-string">"select word, count(*) as total from words group by word"</span>)  wordCountsDataFrame.show()&#125;</code></pre><p>See the full <a href="https://github.com/apache/spark/blob/v2.1.2/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala" target="_blank" rel="noopener">source code</a>.</p><p>You can also run SQL queries on tables defined on streaming data from a different thread (that is, asynchronous to the running StreamingContext). Just make sure that you set the StreamingContext to remember a sufficient amount of streaming data such that the query can run. Otherwise the StreamingContext, which is unaware of the any asynchronous SQL queries, will delete off old streaming data before the query can complete. For example, if you want to query the last batch, but your query can take 5 minutes to run, then call <code>streamingContext.remember(Minutes(5))</code> (in Scala, or equivalent in other languages).</p><p>See the <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html" target="_blank" rel="noopener">DataFrames and SQL</a> guide to learn more about DataFrames.</p><p>您还可以在来自不同线程的流数据定义的表上运行SQL查询（即与正在运行的StreamingContext异步）。只需确保将StreamingContext设置为记住足够的流数据即可运行查询。否则，不知道任何异步SQL查询的StreamingContext将在查询完成之前删除旧的流数据。例如，如果您要查询最后一批，但是查询可能需要5分钟才能运行，然后调用<code>streamingContext.remember(Minutes(5))</code>（使用Scala或其他语言的等效语言）。</p><h3 id="3-9-MLlib-Operations"><a href="#3-9-MLlib-Operations" class="headerlink" title="3.9 MLlib Operations"></a>3.9 MLlib Operations</h3><p>You can also easily use machine learning algorithms provided by <a href="http://spark.apache.org/docs/2.1.2/ml-guide.html" target="_blank" rel="noopener">MLlib</a>. First of all, there are streaming machine learning algorithms (e.g. <a href="http://spark.apache.org/docs/2.1.2/mllib-linear-methods.html#streaming-linear-regression" target="_blank" rel="noopener">Streaming Linear Regression</a>, <a href="http://spark.apache.org/docs/2.1.2/mllib-clustering.html#streaming-k-means" target="_blank" rel="noopener">Streaming KMeans</a>, etc.) which can simultaneously learn from the streaming data as well as apply the model on the streaming data. Beyond these, for a much larger class of machine learning algorithms, you can learn a learning model offline (i.e. using historical data) and then apply the model online on streaming data. See the <a href="http://spark.apache.org/docs/2.1.2/ml-guide.html" target="_blank" rel="noopener">MLlib</a> guide for more details.</p><p>您还可以轻松使用<a href="http://spark.apache.org/docs/2.1.2/ml-guide.html" target="_blank" rel="noopener">MLlib</a>提供的机器学习算法。首先，有流机器学习算法（例如，<a href="http://spark.apache.org/docs/2.1.2/mllib-linear-methods.html#streaming-linear-regression" target="_blank" rel="noopener">流线性回归</a>，<a href="http://spark.apache.org/docs/2.1.2/mllib-clustering.html#streaming-k-means" target="_blank" rel="noopener">流KMeans</a>等），它们可以同时从流数据中学习并将模型应用于流数据。除此之外，对于更多种类的机器学习算法，您可以离线学习学习模型（即使用历史数据），然后在线将模型应用于流数据。有关更多详细信息，请参见<a href="http://spark.apache.org/docs/2.1.2/ml-guide.html" target="_blank" rel="noopener">MLlib</a>指南。</p><hr><h3 id="3-10-Caching-Persistence"><a href="#3-10-Caching-Persistence" class="headerlink" title="3.10 Caching / Persistence"></a>3.10 Caching / Persistence</h3><p>Similar to RDDs, DStreams also allow developers to persist the stream’s data in memory. That is, using the <code>persist()</code> method on a DStream will automatically persist every RDD of that DStream in memory. This is useful if the data in the DStream will be computed multiple times (e.g., multiple operations on the same data). For window-based operations like <code>reduceByWindow</code> and <code>reduceByKeyAndWindow</code> and state-based operations like <code>updateStateByKey</code>, this is implicitly true. Hence, DStreams generated by window-based operations are automatically persisted in memory, without the developer calling <code>persist()</code>.</p><p>For input streams that receive data over the network (such as, Kafka, Flume, sockets, etc.), the default persistence level is set to replicate the data to two nodes for fault-tolerance.</p><p>Note that, unlike RDDs, the default persistence level of DStreams keeps the data serialized in memory. This is further discussed in the <a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#memory-tuning" target="_blank" rel="noopener">Performance Tuning</a> section. More information on different persistence levels can be found in the <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">Spark Programming Guide</a>.</p><p>与RDD相似，DStream也允许开发人员将流的数据持久存储在内存中。也就是说，<code>persist()</code>在DStream上使用该方法将自动将该DStream的每个RDD持久存储在内存中。如果DStream中的数据将被多次计算（例如，对同一数据进行多次操作），这将很有用。对于和的基于窗口的操作<code>reduceByWindow</code>和 <code>reduceByKeyAndWindow</code>和的基于状态的操作<code>updateStateByKey</code>，这都是隐含的。因此，由基于窗口的操作生成的DStream会自动保存在内存中，而无需开发人员调用<code>persist()</code>。</p><p>对于通过网络接收数据的输入流（例如Kafka，Flume，套接字等），默认的持久性级别设置为将数据复制到两个节点以实现容错。</p><p>请注意，与RDD不同，DStream的默认持久性级别将数据序列化在内存中。<a href="http://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#memory-tuning" target="_blank" rel="noopener">性能调整</a>部分将对此进行进一步讨论。有关不同持久性级别的更多信息，请参见《<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">Spark编程指南》</a>。</p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark系列（一） —— Spark入门</title>
    <link href="/2020/07/17/spark%E7%AE%80%E4%BB%8B/"/>
    <url>/2020/07/17/spark%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark系列（一）-——-Spark入门"><a href="#Spark系列（一）-——-Spark入门" class="headerlink" title="Spark系列（一） —— Spark入门"></a>Spark系列（一） —— Spark入门</h1><h2 id="1-Spark是什么？"><a href="#1-Spark是什么？" class="headerlink" title="1. Spark是什么？"></a>1. Spark是什么？</h2><h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>Apache Spark is a unified analytics engine for large-scale data processing.（Apache Spark是用于大规模数据处理的统一分析引擎。）</p><h3 id="1-2-介绍"><a href="#1-2-介绍" class="headerlink" title="1.2 介绍"></a>1.2 介绍</h3><p>Spark支持多种常用的编程语言（Python，Java，Scala和R），提供支持SQL、流处理、机器学习等多种任务的软件库，能单机运行，也可以集群运行。</p><p>spark提供的组件的软件库如下：</p><p><img src="https://s1.ax1x.com/2020/07/07/UknygH.png" srcset="/img/loading.gif" alt=""></p><h3 id="1-3-特点"><a href="#1-3-特点" class="headerlink" title="1.3 特点"></a>1.3 特点</h3><ol><li>相比较MapReduce，基于内存分析快速</li><li>使用简单，提供多种API</li><li>提供统一解决方案，sparkSQL，Spark Streaming，Spark MLlibdeng</li></ol><h2 id="2-RDD"><a href="#2-RDD" class="headerlink" title="2. RDD"></a>2. RDD</h2><p>spark的主要抽象称为弹性分布式数据集（RDD），可以通过读取文件创建RDD。</p><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> textFile = sc.textFile(<span class="hljs-string">"README.md"</span>)textFile: org.apache.spark.rdd.<span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = <span class="hljs-type">README</span>.md <span class="hljs-type">MapPartitionsRDD</span>[<span class="hljs-number">1</span>] at textFile at &lt;console&gt;:<span class="hljs-number">25</span></code></pre><p>RDD具有两种算子，行动算子（有返回值）和转换算子（返回指向新RDD的指针）</p><p><strong>行动算子：</strong></p><pre><code class="hljs scala">scala&gt; textFile.count() <span class="hljs-comment">// Number of items in this RDD</span>res0: <span class="hljs-type">Long</span> = <span class="hljs-number">126</span> <span class="hljs-comment">// May be different from yours as README.md will change over time, similar to other outputs</span>scala&gt; textFile.first() <span class="hljs-comment">// First item in this RDD</span>res1: <span class="hljs-type">String</span> = # <span class="hljs-type">Apache</span> <span class="hljs-type">Spark</span></code></pre><p><strong>转换算子：</strong></p><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> linesWithSpark = textFile.filter(line =&gt; line.contains(<span class="hljs-string">"Spark"</span>))linesWithSpark: org.apache.spark.rdd.<span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = <span class="hljs-type">MapPartitionsRDD</span>[<span class="hljs-number">2</span>] at filter at &lt;console&gt;:<span class="hljs-number">27</span></code></pre><p>两种算子同时使用：</p><pre><code class="hljs scala">scala&gt; textFile.filter(line =&gt; line.contains(<span class="hljs-string">"Spark"</span>)).count() <span class="hljs-comment">// How many lines contain "Spark"?</span>res3: <span class="hljs-type">Long</span> = <span class="hljs-number">15</span></code></pre><p>Spark还支持将数据集提取到群集范围的内存中缓存中，当重复访问数据时，例如查询小的“热”数据集或运行迭代算法（如PageRank）时，这非常有用。</p><pre><code class="hljs scala">scala&gt; linesWithSpark.cache()res7: linesWithSpark<span class="hljs-class">.<span class="hljs-keyword">type</span> </span>= <span class="hljs-type">MapPartitionsRDD</span>[<span class="hljs-number">2</span>] at filter at &lt;console&gt;:<span class="hljs-number">27</span>scala&gt; linesWithSpark.count()res8: <span class="hljs-type">Long</span> = <span class="hljs-number">15</span>scala&gt; linesWithSpark.count()res9: <span class="hljs-type">Long</span> = <span class="hljs-number">15</span></code></pre><h2 id="3-Wordcount例子"><a href="#3-Wordcount例子" class="headerlink" title="3. Wordcount例子"></a>3. Wordcount例子</h2><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Wordcount</span></span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;      <span class="hljs-comment">// 创建saprkcontext</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"quickStart"</span>).setMaster(<span class="hljs-string">"local"</span>)    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)        <span class="hljs-comment">// 读取文件</span>    <span class="hljs-keyword">val</span> logData = sc.textFile(<span class="hljs-string">"D:\\testfile"</span>,<span class="hljs-number">2</span>).cache()        <span class="hljs-comment">// 计算包含a，b的行数</span>    <span class="hljs-keyword">val</span> aNum = logData.filter(_.contains(<span class="hljs-string">"a"</span>)).count()    <span class="hljs-keyword">val</span> bNum = logData.filter(_.contains(<span class="hljs-string">"b"</span>)).count()    println(<span class="hljs-string">s"a=<span class="hljs-subst">$aNum</span> b=<span class="hljs-subst">$bNum</span>"</span>)    <span class="hljs-comment">// 包含最多单词的行的单词数</span>    <span class="hljs-keyword">val</span> bigline = logData.map(_.split(<span class="hljs-string">" "</span>).size)    .reduce((a, b) =&gt; <span class="hljs-keyword">if</span> (a &gt; b) a <span class="hljs-keyword">else</span> b)    println(bigline)    <span class="hljs-comment">// wordcount</span>    <span class="hljs-keyword">val</span> wc = logData.flatMap(line =&gt; line.split(<span class="hljs-string">" "</span>))                    .map(word =&gt; (word,<span class="hljs-number">1</span>))                    .reduceByKey((a,b)=&gt; a+b)    wc.foreach(println)    sc.stop()  &#125;&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark系列（二）—— Spark编程指南</title>
    <link href="/2020/07/17/spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
    <url>/2020/07/17/spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark系列（二）——-Spark编程指南"><a href="#Spark系列（二）——-Spark编程指南" class="headerlink" title="Spark系列（二）—— Spark编程指南"></a>Spark系列（二）—— Spark编程指南</h1><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><blockquote><p>At a high level, every Spark application consists of a <em>driver program</em> that runs the user’s <code>main</code> function and executes various <em>parallel operations</em> on a cluster. The main abstraction Spark provides is a <em>resilient distributed dataset</em> (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to <em>persist</em> an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.</p><p>A second abstraction in Spark is <em>shared variables</em> that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: <em>broadcast variables</em>, which can be used to cache a value in memory on all nodes, and <em>accumulators</em>, which are variables that are only “added” to, such as counters and sums.</p><p>This guide shows each of these features in each of Spark’s supported languages. It is easiest to follow along with if you launch Spark’s interactive shell – either <code>bin/spark-shell</code> for the Scala shell or <code>bin/pyspark</code> for the Python one.</p><p>在较高级别上，每个Spark应用程序都包含一个<em>驱动程序</em>，该<em>程序</em>运行用户的<code>main</code>功能并在集群上执行各种<em>并行操作</em>。Spark提供的主要抽象是<em>弹性分布式数据集</em>（RDD），它是跨集群节点划分的元素的集合，可以并行操作。通过从Hadoop文件系统（或任何其他Hadoop支持的文件系统）中的文件或驱动程序中现有的Scala集合开始并进行转换来创建RDD。用户还可以要求Spark将RDD <em>持久</em>存储在内存中，从而使其能够在并行操作中高效地重用。最后，RDD会自动从节点故障中恢复。</p><p>Spark中的第二个抽象是可以在并行操作中使用的<em>共享变量</em>。默认情况下，当Spark作为一组任务在不同节点上并行运行一个函数时，它会将函数中使用的每个变量的副本传送给每个任务。有时，需要在任务之间或任务与驱动程序之间共享变量。Spark支持两种类型的共享变量：<em>广播变量</em>（可用于在所有节点上的内存中缓存值）和<em>累加器（accumulator）</em>，这些变量仅被“添加”到其上，例如计数器和总和。</p></blockquote><h2 id="2-Linking-with-Spark"><a href="#2-Linking-with-Spark" class="headerlink" title="2. Linking with Spark"></a>2. Linking with Spark</h2><ol><li><p>spark的版本与scala的版本要兼容，可以在maven依赖项中体现</p><pre><code class="hljs angelscript">groupId = org.apache.sparkartifactId = spark-core_2<span class="hljs-number">.11</span> (spark版本<span class="hljs-number">2.1</span><span class="hljs-number">.2</span>，scala版本<span class="hljs-number">2.11</span>)version = <span class="hljs-number">2.1</span><span class="hljs-number">.2</span></code></pre></li><li><p>spark和scala的兼容情况</p><ul><li>spark1.2.x 到 spark2.2.x 兼容 scala2.10，scala2.11；</li><li>spark2.3.x 兼容 scala2.11；</li><li>spark2.4.x 兼容 scala2.11，scala2.12；</li><li>spark3.0.0 兼容 scala2.12</li></ul></li></ol><h2 id="3-Initializing-Spark"><a href="#3-Initializing-Spark" class="headerlink" title="3. Initializing Spark"></a>3. Initializing Spark</h2><p>创建spark程序的第一步需要先创建SparkContext对象，SparkContext告诉Spark如何访问集群。创建SparkContext需要先创建SparkConf对象，SparkConf对象包含一些描述信息</p><pre><code class="hljs scala"><span class="hljs-comment">// 创建saprkcontext</span><span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"quickStart"</span>).setMaster(<span class="hljs-string">"local"</span>)<span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)</code></pre><p>每个JVM只能激活一个SparkContext，启动新的SparkContext之前需要先stop()开启的SparkConte</p><h2 id="4-Using-the-Shell"><a href="#4-Using-the-Shell" class="headerlink" title="4. Using the Shell"></a>4. Using the Shell</h2><ol><li>在shell中，spark提供SparkContext sc, 创建自己的SparkContext不生效。</li><li>可以指定参数<ul><li>–master 设置运行模式</li><li>–jars 指定jar包</li><li>–packages 添加maven依赖（？）</li></ul></li></ol><h2 id="5-Resilient-Distributed-Datasets-RDDs"><a href="#5-Resilient-Distributed-Datasets-RDDs" class="headerlink" title="5. Resilient Distributed Datasets (RDDs)"></a>5. Resilient Distributed Datasets (RDDs)</h2><p>弹性分布式数据集（RDD）：可并行操作的容错数据集</p><p>创建方式：</p><ol><li><em>parallelizing</em> 一个存在的集合</li><li>引用外部存储系统中的数据集，hdfs，hbase等</li></ol><h3 id="5-1-Parallelized-Collections"><a href="#5-1-Parallelized-Collections" class="headerlink" title="5.1 Parallelized Collections"></a>5.1 Parallelized Collections</h3><p>Parallelized collections are created by calling <code>SparkContext</code>’s <code>parallelize</code> method on an existing collection in your driver program (a Scala <code>Seq</code>)，复制集合中的元素形成可并行操作分布式数据集。</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> data = <span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<span class="hljs-keyword">val</span> distData = sc.parallelize(data)</code></pre><p>One important parameter for parallel collections is the number of <em>partitions</em> to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to <code>parallelize</code> (e.g. <code>sc.parallelize(data, 10)</code>). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.</p><h3 id="5-2-External-Datasets"><a href="#5-2-External-Datasets" class="headerlink" title="5.2 External Datasets"></a>5.2 External Datasets</h3><p>Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, <a href="http://wiki.apache.org/hadoop/AmazonS3" target="_blank" rel="noopener">Amazon S3</a>, etc. Spark supports text files, <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank" rel="noopener">SequenceFiles</a>, and any other Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html" target="_blank" rel="noopener">InputFormat</a>.Spark可以从hadoop支持的任何存储源创建分布式数据集。</p><p>Text file RDDs can be created using <code>SparkContext</code>’s <code>textFile</code> method.这个方法需要提供文件URI </p><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> distFile = sc.textFile(<span class="hljs-string">"data.txt"</span>)distFile: org.apache.spark.rdd.<span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = data.txt <span class="hljs-type">MapPartitionsRDD</span>[<span class="hljs-number">10</span>] at textFile at &lt;console&gt;:<span class="hljs-number">26</span></code></pre><p>Some notes on reading files with Spark:</p><ul><li>If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.<ul><li>本地文件系统，注意别的工作节点是否能访问到。</li></ul></li><li>All of Spark’s file-based input methods, including <code>textFile</code>, support running on directories, compressed files, and wildcards as well. For example, you can use <code>textFile(&quot;/my/directory&quot;)</code>, <code>textFile(&quot;/my/directory/*.txt&quot;)</code>, and <code>textFile(&quot;/my/directory/*.gz&quot;)</code>.<ul><li>支持读文件夹，压缩格式，和通配符选择文件</li></ul></li><li>The <code>textFile</code> method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.<ul><li>可以传入第二个参数，手动设置分区个数，默认情况下，文件的一个块创建一个分区，可以传入一个更大的值，但是分区数不能小于文件块的数量。</li></ul></li></ul><p>Apart from text files, Spark’s Scala API also supports several other data formats:</p><ul><li><p><code>SparkContext.wholeTextFiles</code> lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with <code>textFile</code>, which would return one record per line in each file.</p><ul><li><code>SparkContext.wholeTextFiles</code>合并小文件，每一行书文件名和文件内容，类似hadoop中的tar文件</li></ul></li><li><p>For <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank" rel="noopener">SequenceFiles</a>, use SparkContext’s <code>sequenceFile[K, V]</code> method where <code>K</code> and <code>V</code> are the types of key and values in the file. These should be subclasses of Hadoop’s <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html" target="_blank" rel="noopener">Writable</a> interface, like <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/IntWritable.html" target="_blank" rel="noopener">IntWritable</a> and <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Text.html" target="_blank" rel="noopener">Text</a>. In addition, Spark allows you to specify native types for a few common Writables; for example, <code>sequenceFile[Int, String]</code> will automatically read IntWritables and Texts.</p><ul><li><code>sequenceFile[K, V]</code>操作<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank" rel="noopener">SequenceFiles</a>文件，这是Hadoop的<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html" target="_blank" rel="noopener">Writable</a>接口的子类，例如<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/IntWritable.html" target="_blank" rel="noopener">IntWritable</a>和<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Text.html" target="_blank" rel="noopener">Text</a>。没用过，不太理解。</li></ul></li></ul><ul><li>For other Hadoop InputFormats, you can use the <code>SparkContext.hadoopRDD</code> method, which takes an arbitrary <code>JobConf</code> and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use <code>SparkContext.newAPIHadoopRDD</code> for InputFormats based on the “new” MapReduce API (<code>org.apache.hadoop.mapreduce</code>).</li></ul><ul><li><code>RDD.saveAsObjectFile</code> and <code>SparkContext.objectFile</code> support saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.</li></ul><h3 id="5-3-RDD-Operations"><a href="#5-3-RDD-Operations" class="headerlink" title="5.3 RDD Operations"></a>5.3 RDD Operations</h3><p>RDDs support two types of operations: <em>transformations</em>, which create a new dataset from an existing one, and <em>actions</em>, which return a value to the driver program after running a computation on the dataset. For example, <code>map</code> is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, <code>reduce</code> is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel <code>reduceByKey</code> that returns a distributed dataset). 为什么 <code>reduce(_+_)</code>会执行累加？</p><ul><li>RDD提供两种操作<ul><li>转换：根据一个现有的RDD创建一个新的RDD</li><li>行动：对数据集计算之后返回给driver一个值</li></ul></li></ul><p>All transformations in Spark are <em>lazy</em>, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through <code>map</code> will be used in a <code>reduce</code> and return only the result of the <code>reduce</code> to the driver, rather than the larger mapped dataset.</p><ul><li>spark的转换是惰性的，只有driver需要action返回数据时候，才会触发transformmations进行计算。</li></ul><p>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also <em>persist</em> an RDD in memory using the <code>persist</code> (or <code>cache</code>) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.</p><ul><li>默认情况下每个action会去触发transform RDD，可以把转换的RDD缓存起来，用persist，cache缓存在内存中。也可以持久化到磁盘中。</li></ul><h3 id="5-4-Basics"><a href="#5-4-Basics" class="headerlink" title="5.4 Basics"></a>5.4 Basics</h3><pre><code class="hljs scala"><span class="hljs-keyword">val</span> lines = sc.textFile(<span class="hljs-string">"data.txt"</span>)<span class="hljs-keyword">val</span> lineLengths = lines.map(s =&gt; s.length)<span class="hljs-keyword">val</span> totalLength = lineLengths.reduce((a, b) =&gt; a + b)</code></pre><p>The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: <code>lines</code> is merely a pointer to the file. The second line defines <code>lineLengths</code> as the result of a <code>map</code> transformation. Again, <code>lineLengths</code> is <em>not</em> immediately computed, due to laziness. Finally, we run <code>reduce</code>, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program.。此时，Spark将计算分解为任务，以在不同的机器上运行，每台机器都运行其映射的一部分和局部归约，仅将其答案返回给驱动程序。</p><p>If we also wanted to use <code>lineLengths</code> again later, we could add:</p><pre><code class="hljs scala">lineLengths.persist()</code></pre><p>before the <code>reduce</code>, which would cause <code>lineLengths</code> to be saved in memory after the first time it is computed.</p><h3 id="5-5-Passing-Functions-to-Spark"><a href="#5-5-Passing-Functions-to-Spark" class="headerlink" title="5.5 Passing Functions to Spark"></a>5.5 Passing Functions to Spark</h3><p>Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are two recommended ways to do this:  Spark的API在很大程度上依赖于在驱动程序中传递函数以在集群上运行</p><ul><li><p><a href="http://docs.scala-lang.org/tutorials/tour/anonymous-function-syntax.html" target="_blank" rel="noopener">Anonymous function syntax</a>, which can be used for short pieces of code.</p></li><li><p>Static methods in a global singleton object. For example, you can define <code>object MyFunctions</code> and then pass <code>MyFunctions.func1</code>, as follows:</p><ul><li><p>伴生对象中的静态方法，调用是半生对象.方法名</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">MyFunctions</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func1</span></span>(s: <span class="hljs-type">String</span>): <span class="hljs-type">String</span> = &#123; ... &#125;&#125;myRdd.map(<span class="hljs-type">MyFunctions</span>.func1)</code></pre></li></ul></li><li><p>Note that while it is also possible to pass a reference to a method in a class instance (as opposed to a singleton object), this requires sending the object that contains that class along with the method. For example, consider:</p><ul><li>类中定义方法，传递时候需要类的对象和方法一起传递</li></ul><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyClass</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func1</span></span>(s: <span class="hljs-type">String</span>): <span class="hljs-type">String</span> = &#123; ... &#125;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doStuff</span></span>(rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = &#123; rdd.map(func1) &#125;&#125;</code></pre></li></ul><p>Here, if we create a new <code>MyClass</code> instance and call <code>doStuff</code> on it, the <code>map</code> inside there references the <code>func1</code> method <em>of that <code>MyClass</code> instance</em>, so the whole object needs to be sent to the cluster. It is similar to writing <code>rdd.map(x =&gt; this.func1(x))</code>.</p><p>在这里，如果我们创建一个新的<code>MyClass</code>实例，并调用<code>doStuff</code>就可以了，<code>map</code>里面有引用的 <code>func1</code>方法<em>是的<code>MyClass</code>实例</em>，所以整个对象需要被发送到群集。它类似于写作<code>rdd.map(x =&gt; this.func1(x))</code>。</p><p><strong>* * * * * 不理解</strong></p><p>In a similar way, accessing fields of the outer object will reference the whole object:</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyClass</span> </span>&#123;  <span class="hljs-keyword">val</span> field = <span class="hljs-string">"Hello"</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doStuff</span></span>(rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = &#123; rdd.map(x =&gt; field + x) &#125;&#125;</code></pre><p>is equivalent to writing <code>rdd.map(x =&gt; this.field + x)</code>, which references all of <code>this</code>. To avoid this issue, the simplest way is to copy <code>field</code> into a local variable instead of accessing it externally:</p><pre><code class="hljs scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doStuff</span></span>(rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = &#123;  <span class="hljs-keyword">val</span> field_ = <span class="hljs-keyword">this</span>.field  rdd.map(x =&gt; field_ + x)&#125;</code></pre><h3 id="5-6-Understanding-closures"><a href="#5-6-Understanding-closures" class="headerlink" title="5.6 Understanding closures"></a>5.6 Understanding closures</h3><p>One of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses <code>foreach()</code> to increment a counter, but similar issues can occur for other operations as well.</p><p>跨集群执行代码时变量和方法的作用范围和生命周期，修改超出其范围的变量的RDD操作可能经常引起混乱</p><h4 id="5-6-1-Example"><a href="#5-6-1-Example" class="headerlink" title="5.6.1 Example"></a>5.6.1 Example</h4><p>Consider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in <code>local</code> mode (<code>--master = local[n]</code>) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):</p><pre><code class="hljs scala"><span class="hljs-keyword">var</span> counter = <span class="hljs-number">0</span><span class="hljs-keyword">var</span> rdd = sc.parallelize(data)<span class="hljs-comment">// Wrong: Don't do this!!</span>rdd.foreach(x =&gt; counter += x)println(<span class="hljs-string">"Counter value: "</span> + counter)</code></pre><p>没有报错，为什么累加之后counter还为0</p><h4 id="5-6-2-Local-vs-cluster-modes"><a href="#5-6-2-Local-vs-cluster-modes" class="headerlink" title="5.6.2 Local vs. cluster modes"></a>5.6.2 Local vs. cluster modes</h4><p>The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s <strong>closure</strong>. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case <code>foreach()</code>). This closure is serialized and sent to each executor.</p><p>上面的代码的行为是未定义的，可能无法按预期工作。为了执行作业，Spark将RDD操作的处理分解为任务，每个任务都由执行程序执行。在执行之前，Spark计算任务的<strong>闭包</strong>。闭包是执行者在RDD上执行其计算时必须可见的那些变量和方法（在本例中为<code>foreach()</code>）。此闭包被序列化并发送给每个执行器。</p><p>The variables within the closure sent to each executor are now copies and thus, when <strong>counter</strong> is referenced within the <code>foreach</code> function, it’s no longer the <strong>counter</strong> on the driver node. There is still a <strong>counter</strong> in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of <strong>counter</strong> will still be zero since all operations on <strong>counter</strong> were referencing the value within the serialized closure.</p><p>发送给每个执行程序的闭包中的变量现在是副本，因此，在函数中引用<strong>计数器</strong>时<code>foreach</code>，它不再是驱动程序节点上的<strong>计数器</strong>。驱动程序节点的内存中仍然存在一个<strong>计数器</strong>，但是执行者将不再看到该<strong>计数器</strong>！执行者仅从序列化闭包中看到副本。因此，由于对<strong>计数器的</strong>所有操作都引用了序列化闭包内的值，所以<strong>counter</strong>的最终值仍将为零。（解释了为什么counter为0 ，因为executor中也有一个counter副本，在这个副本上累加的，打印的是driver端的counter，driver端的counter值为0）</p><p>In local mode, in some circumstances the <code>foreach</code> function will actually execute within the same JVM as the driver and will reference the same original <strong>counter</strong>, and may actually update it.</p><p>在本地模式下，在某些情况下，该<code>foreach</code>函数实际上将在与驱动程序相同的JVM中执行，并且将引用相同的原始<strong>计数器</strong>，并且实际上可能会对其进行更新。(怎样知道driver和executor在一个JVM中执行)</p><p>To ensure well-defined behavior in these sorts of scenarios one should use an <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#accumulators" target="_blank" rel="noopener"><code>Accumulator</code></a>. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.</p><p>In general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.</p><p>为确保在此类情况下行为明确，应使用<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#accumulators" target="_blank" rel="noopener"><code>Accumulator</code></a>。Spark中的累加器专门用于提供一种机制，用于在集群中的各个工作节点之间拆分执行时安全地更新变量。本指南的“累加器”部分将详细讨论这些内容。</p><p>通常，闭包-诸如循环或局部定义的方法之类的构造，不应用于突变某些全局状态。Spark不定义或保证从闭包外部引用的对象的突变行为。某些执行此操作的代码可能会在本地模式下工作，但这只是偶然的情况，此类代码在分布式模式下将无法按预期运行。如果需要一些全局聚合，请使用累加器。</p><h4 id="5-6-3-Printing-elements-of-an-RDD"><a href="#5-6-3-Printing-elements-of-an-RDD" class="headerlink" title="5.6.3 Printing elements of an RDD"></a>5.6.3 Printing elements of an RDD</h4><p>Another common idiom is attempting to print out the elements of an RDD using <code>rdd.foreach(println)</code> or <code>rdd.map(println)</code>. On a single machine, this will generate the expected output and print all the RDD’s elements. However, in <code>cluster</code> mode, the output to <code>stdout</code> being called by the executors is now writing to the executor’s <code>stdout</code> instead, not the one on the driver, so <code>stdout</code> on the driver won’t show these! To print all elements on the driver, one can use the <code>collect()</code> method to first bring the RDD to the driver node thus: <code>rdd.collect().foreach(println)</code>. This can cause the driver to run out of memory, though, because <code>collect()</code> fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the <code>take()</code>: <code>rdd.take(100).foreach(println)</code>.</p><p>另一个常见用法是尝试使用<code>rdd.foreach(println)</code>或打印RDD的元素<code>rdd.map(println)</code>。在单台机器上，这将生成预期的输出并打印所有RDD的元素。但是，在<code>cluster</code>模式下，<code>stdout</code>执行程序要调用的输出现在正在写入执行程序的输出，<code>stdout</code>而不是驱动程序上的那个，因此<code>stdout</code>驱动程序将不会显示这些！要打印在驱动器的所有元素，可以使用的<code>collect()</code>方法，首先使RDD到驱动器节点从而：<code>rdd.collect().foreach(println)</code>。但是，这可能会导致驱动程序用尽内存，因为<code>collect()</code>会将整个RDD提取到一台计算机上。如果只需要打印RDD的一些元素，则更安全的方法是使用<code>take()</code>：<code>rdd.take(100).foreach(println)</code>。</p><ol><li>本地模式：可以用<code>rdd.foreach(println)</code>或者<code>rdd.map(println)</code></li><li>集群模式：<code>rdd.collect().foreach(println)</code>，可能会内存溢出，取出部分数据用<code>rdd.take(100).foreach(println)</code></li></ol><h3 id="5-7-Working-with-Key-Value-Pairs"><a href="#5-7-Working-with-Key-Value-Pairs" class="headerlink" title="5.7 Working with Key-Value Pairs"></a>5.7 Working with Key-Value Pairs</h3><p>While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements by a key.</p><p>In Scala, these operations are automatically available on RDDs containing <a href="http://www.scala-lang.org/api/2.11.8/index.html#scala.Tuple2" target="_blank" rel="noopener">Tuple2</a> objects (the built-in tuples in the language, created by simply writing <code>(a, b)</code>). The key-value pair operations are available in the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">PairRDDFunctions</a> class, which automatically wraps around an RDD of tuples.</p><p>For example, the following code uses the <code>reduceByKey</code> operation on key-value pairs to count how many times each line of text occurs in a file:</p><p>尽管大多数Spark操作可在包含任何类型对象的RDD上运行，但一些特殊操作仅可用于键-值对的RDD。最常见的是分布式“混洗”操作，例如通过键对元素进行分组或聚合。</p><p>在Scala中，这些操作在包含<a href="http://www.scala-lang.org/api/2.11.8/index.html#scala.Tuple2" target="_blank" rel="noopener">Tuple2</a>对象（该语言的内置元组，只需编写即可创建<code>(a, b)</code>）的<a href="http://www.scala-lang.org/api/2.11.8/index.html#scala.Tuple2" target="_blank" rel="noopener">RDD</a>上自动可用 。<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">PairRDDFunctions</a>类中提供键值对操作， 该类会自动包装RDD元组。</p><p>例如，以下代码<code>reduceByKey</code>对键值对使用运算来计算文件中每一行文本出现的次数：</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> lines = sc.textFile(<span class="hljs-string">"data.txt"</span>)<span class="hljs-keyword">val</span> pairs = lines.map(s =&gt; (s, <span class="hljs-number">1</span>))<span class="hljs-keyword">val</span> counts = pairs.reduceByKey((a, b) =&gt; a + b)</code></pre><p>We could also use <code>counts.sortByKey()</code>, for example, to sort the pairs alphabetically, and finally <code>counts.collect()</code> to bring them back to the driver program as an array of objects.</p><p><strong>Note:</strong> when using custom objects as the key in key-value pair operations, you must be sure that a custom <code>equals()</code> method is accompanied with a matching <code>hashCode()</code> method. For full details, see the contract outlined in the <a href="http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()" target="_blank" rel="noopener">Object.hashCode() documentation</a>.</p><p><code>counts.sortByKey()</code>例如，我们还可以使用按字母顺序对，最后 <code>counts.collect()</code>将它们作为对象数组带回到驱动程序中。</p><p><strong>注意：</strong>在键-值对操作中使用自定义对象作为键时，必须确保自定义<code>equals()</code>方法与匹配<code>hashCode()</code>方法一起使用。有关完整的详细信息，请参见<a href="http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()" target="_blank" rel="noopener">Object.hashCode（）文档中</a>概述的合同。</p><h3 id="5-8-Transformations"><a href="#5-8-Transformations" class="headerlink" title="5.8 Transformations"></a>5.8 Transformations</h3><p>The following table lists some of the common transformations supported by Spark. Refer to the RDD API doc (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/api/java/JavaRDD.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.html#pyspark.RDD" target="_blank" rel="noopener">Python</a>, <a href="http://spark.apache.org/docs/2.1.2/api/R/index.html" target="_blank" rel="noopener">R</a>) and pair RDD functions doc (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html" target="_blank" rel="noopener">Java</a>) for details.</p><table><thead><tr><th align="left">Transformation</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>map</strong>(<em>func</em>)</td><td align="left">Return a new distributed dataset formed by passing each element of the source through a function <em>func</em>.</td></tr><tr><td align="left"><strong>filter</strong>(<em>func</em>)</td><td align="left">Return a new dataset formed by selecting those elements of the source on which <em>func</em> returns true.</td></tr><tr><td align="left"><strong>flatMap</strong>(<em>func</em>)</td><td align="left">Similar to map, but each input item can be mapped to 0 or more output items (so <em>func</em> should return a Seq rather than a single item).</td></tr><tr><td align="left"><strong>mapPartitions</strong>(<em>func</em>)</td><td align="left">Similar to map, but runs separately on each partition (block) of the RDD, so <em>func</em> must be of type Iterator<T> =&gt; Iterator<U> when running on an RDD of type T.</td></tr><tr><td align="left"><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td><td align="left">Similar to mapPartitions, but also provides <em>func</em> with an integer value representing the index of the partition, so <em>func</em> must be of type (Int, Iterator<T>) =&gt; Iterator<U> when running on an RDD of type T.</td></tr><tr><td align="left"><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td><td align="left">Sample a fraction <em>fraction</em> of the data, with or without replacement, using a given random number generator seed.</td></tr><tr><td align="left"><strong>union</strong>(<em>otherDataset</em>)</td><td align="left">Return a new dataset that contains the union of the elements in the source dataset and the argument.</td></tr><tr><td align="left"><strong>intersection</strong>(<em>otherDataset</em>)</td><td align="left">Return a new RDD that contains the intersection of elements in the source dataset and the argument.</td></tr><tr><td align="left"><strong>distinct</strong>([<em>numTasks</em>]))</td><td align="left">Return a new dataset that contains the distinct elements of the source dataset.</td></tr><tr><td align="left"><strong>groupByKey</strong>([<em>numTasks</em>])</td><td align="left">When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. <strong>Note:</strong> If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better performance. <strong>Note:</strong> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td></tr><tr><td align="left"><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td><td align="left">When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <em>func</em>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument.</td></tr><tr><td align="left"><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td><td align="left">When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral “zero” value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument.</td></tr><tr><td align="left"><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td><td align="left">When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td></tr><tr><td align="left"><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td><td align="left">When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.</td></tr><tr><td align="left"><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td><td align="left">When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called <code>groupWith</code>.</td></tr><tr><td align="left"><strong>cartesian</strong>(<em>otherDataset</em>)</td><td align="left">When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).</td></tr><tr><td align="left"><strong>pipe</strong>(<em>command</em>, <em>[envVars]</em>)</td><td align="left">Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process’s stdin and lines output to its stdout are returned as an RDD of strings.</td></tr><tr><td align="left"><strong>coalesce</strong>(<em>numPartitions</em>)</td><td align="left">Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.</td></tr><tr><td align="left"><strong>repartition</strong>(<em>numPartitions</em>)</td><td align="left">Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.</td></tr><tr><td align="left"><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td><td align="left">Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within each partition because it can push the sorting down into the shuffle machinery.</td></tr></tbody></table><h3 id="5-9-Actions"><a href="#5-9-Actions" class="headerlink" title="5.9 Actions"></a>5.9 Actions</h3><p>The following table lists some of the common actions supported by Spark. Refer to the RDD API doc (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/api/java/JavaRDD.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.html#pyspark.RDD" target="_blank" rel="noopener">Python</a>, <a href="http://spark.apache.org/docs/2.1.2/api/R/index.html" target="_blank" rel="noopener">R</a>)</p><p>and pair RDD functions doc (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html" target="_blank" rel="noopener">Java</a>) for details.</p><table><thead><tr><th align="left">Action</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>reduce</strong>(<em>func</em>)</td><td align="left">Aggregate the elements of the dataset using a function <em>func</em> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.</td></tr><tr><td align="left"><strong>collect</strong>()</td><td align="left">Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.</td></tr><tr><td align="left"><strong>count</strong>()</td><td align="left">Return the number of elements in the dataset.</td></tr><tr><td align="left"><strong>first</strong>()</td><td align="left">Return the first element of the dataset (similar to take(1)).</td></tr><tr><td align="left"><strong>take</strong>(<em>n</em>)</td><td align="left">Return an array with the first <em>n</em> elements of the dataset.</td></tr><tr><td align="left"><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td><td align="left">Return an array with a random sample of <em>num</em> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td></tr><tr><td align="left"><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td><td align="left">Return the first <em>n</em> elements of the RDD using either their natural order or a custom comparator.</td></tr><tr><td align="left"><strong>saveAsTextFile</strong>(<em>path</em>)</td><td align="left">Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.</td></tr><tr><td align="left"><strong>saveAsSequenceFile</strong>(<em>path</em>) (Java and Scala)</td><td align="left">Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop’s Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).</td></tr><tr><td align="left"><strong>saveAsObjectFile</strong>(<em>path</em>) (Java and Scala)</td><td align="left">Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using <code>SparkContext.objectFile()</code>.</td></tr><tr><td align="left"><strong>countByKey</strong>()</td><td align="left">Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.</td></tr><tr><td align="left"><strong>foreach</strong>(<em>func</em>)</td><td align="left">Run a function <em>func</em> on each element of the dataset. This is usually done for side effects such as updating an <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#accumulators" target="_blank" rel="noopener">Accumulator</a> or interacting with external storage systems. <strong>Note</strong>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#understanding-closures-a-nameclosureslinka" target="_blank" rel="noopener">Understanding closures </a>for more details.</td></tr></tbody></table><h3 id="5-10-Shuffle-operations"><a href="#5-10-Shuffle-operations" class="headerlink" title="5.10 Shuffle operations"></a>5.10 Shuffle operations</h3><p>Certain operations within Spark trigger an event known as the shuffle. The shuffle is Spark’s mechanism for re-distributing data so that it’s grouped differently across partitions. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation.</p><p>Spark中的某些操作会触发一个称为shuffle的事件。shuffle是Spark的一种用于重新分配数据的机制，因此可以跨分区对数据进行不同的分组。这通常涉及跨执行程序和机器复制数据，从而使改组成为复杂且昂贵的操作。</p><h4 id="5-10-1-Background"><a href="#5-10-1-Background" class="headerlink" title="5.10.1 Background"></a>5.10.1 Background</h4><p>To understand what happens during the shuffle we can consider the example of the <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#ReduceByLink" target="_blank" rel="noopener"><code>reduceByKey</code></a> operation. The <code>reduceByKey</code> operation generates a new RDD where all values for a single key are combined into a tuple - the key and the result of executing a reduce function against all values associated with that key. The challenge is that not all values for a single key necessarily reside on the same partition, or even the same machine, but they must be co-located to compute the result.</p><p>In Spark, data is generally not distributed across partitions to be in the necessary place for a specific operation. During computations, a single task will operate on a single partition - thus, to organize all the data for a single <code>reduceByKey</code> reduce task to execute, Spark needs to perform an all-to-all operation. It must read from all partitions to find all the values for all keys, and then bring together values across partitions to compute the final result for each key - this is called the <strong>shuffle</strong>.</p><p>Although the set of elements in each partition of newly shuffled data will be deterministic, and so is the ordering of partitions themselves, the ordering of these elements is not. If one desires predictably ordered data following shuffle then it’s possible to use:</p><ul><li><code>mapPartitions</code> to sort each partition using, for example, <code>.sorted</code></li><li><code>repartitionAndSortWithinPartitions</code> to efficiently sort partitions while simultaneously repartitioning</li><li><code>sortBy</code> to make a globally ordered RDD</li></ul><p>Operations which can cause a shuffle include <strong>repartition</strong> operations like <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#RepartitionLink" target="_blank" rel="noopener"><code>repartition</code></a> and <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#CoalesceLink" target="_blank" rel="noopener"><code>coalesce</code></a>, <strong>‘ByKey</strong> operations (except for counting) like <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#GroupByLink" target="_blank" rel="noopener"><code>groupByKey</code></a> and <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#ReduceByLink" target="_blank" rel="noopener"><code>reduceByKey</code></a>, and <strong>join</strong> operations like <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#CogroupLink" target="_blank" rel="noopener"><code>cogroup</code></a> and <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#JoinLink" target="_blank" rel="noopener"><code>join</code></a>.</p><p>要了解随机播放期间发生的情况，我们可以考虑<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#ReduceByLink" target="_blank" rel="noopener"><code>reduceByKey</code></a>操作示例 。该<code>reduceByKey</code>操作将生成一个新的RDD，其中将单个键的所有值组合为一个元组-键以及针对与该键关联的所有值执行reduce函数的结果。挑战在于，并非单个键的所有值都必须位于同一分区，甚至同一台机器上，但是必须将它们放在同一位置才能计算结果。</p><p>在Spark中，数据通常不会跨分区分布在特定操作的必要位置。在计算期间，单个任务将在单个分区上运行-因此，要组织所有数据<code>reduceByKey</code>以执行单个reduce任务，Spark需要执行所有操作。它必须从所有分区读取以找到所有键的所有值，然后将各个分区的值汇总在一起以计算每个键的最终结果-这称为<strong>shuffle</strong>。</p><p>尽管新改组后的数据的每个分区中的元素集都是确定性的，分区本身的顺序也是如此，但这些元素的顺序不是确定性的。如果人们希望在改组后可以预期地排序数据，则可以使用：</p><ul><li><code>mapPartitions</code> 使用例如，对每个分区进行排序 <code>.sorted</code></li><li><code>repartitionAndSortWithinPartitions</code> 在对分区进行有效排序的同时进行重新分区</li><li><code>sortBy</code> 制作全球订购的RDD</li></ul><p>这可能会导致一个洗牌的操作包括<strong>重新分区</strong>一样操作 <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#RepartitionLink" target="_blank" rel="noopener"><code>repartition</code></a>和<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#CoalesceLink" target="_blank" rel="noopener"><code>coalesce</code></a>，<strong>ByKey”</strong>操作，比如（除计数）<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#GroupByLink" target="_blank" rel="noopener"><code>groupByKey</code></a>和<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#ReduceByLink" target="_blank" rel="noopener"><code>reduceByKey</code></a>，并 <strong>加入</strong>操作，如<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#CogroupLink" target="_blank" rel="noopener"><code>cogroup</code></a>和<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#JoinLink" target="_blank" rel="noopener"><code>join</code></a>。</p><h4 id="5-10-2-Performance-Impact"><a href="#5-10-2-Performance-Impact" class="headerlink" title="5.10.2 Performance Impact"></a>5.10.2 Performance Impact</h4><p>The <strong>Shuffle</strong> is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - <em>map</em> tasks to organize the data, and a set of <em>reduce</em> tasks to aggregate it. This nomenclature comes from MapReduce and does not directly relate to Spark’s <code>map</code> and <code>reduce</code> operations.</p><p>Internally, results from individual map tasks are kept in memory until they can’t fit. Then, these are sorted based on the target partition and written to a single file. On the reduce side, tasks read the relevant sorted blocks.</p><p>Certain shuffle operations can consume significant amounts of heap memory since they employ in-memory data structures to organize records before or after transferring them. Specifically, <code>reduceByKey</code> and <code>aggregateByKey</code> create these structures on the map side, and <code>&#39;ByKey</code> operations generate these on the reduce side. When data does not fit in memory Spark will spill these tables to disk, incurring the additional overhead of disk I/O and increased garbage collection.</p><p>Shuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. This is done so the shuffle files don’t need to be re-created if the lineage is re-computed. Garbage collection may happen only after a long period of time, if the application retains references to these RDDs or if GC does not kick in frequently. This means that long-running Spark jobs may consume a large amount of disk space. The temporary storage directory is specified by the <code>spark.local.dir</code> configuration parameter when configuring the Spark context.</p><p>Shuffle behavior can be tuned by adjusting a variety of configuration parameters. See the ‘Shuffle Behavior’ section within the <a href="http://spark.apache.org/docs/2.1.2/configuration.html" target="_blank" rel="noopener">Spark Configuration Guide</a>.</p><p>所述<strong>随机播放</strong>是昂贵的操作，因为它涉及的磁盘I / O，数据序列，和网络I / O。为了组织随机数据，Spark生成任务集- <em>映射</em>任务以组织数据，以及一组<em>reduce</em>任务来聚合数据。此术语来自MapReduce，与Spark <code>map</code>和<code>reduce</code>操作没有直接关系。</p><p>在内部，单个地图任务的结果会保留在内存中，直到无法容纳为止。然后，根据目标分区对它们进行排序并写入单个文件。在简化方面，任务读取相关的已排序块。</p><p>某些混洗操作会消耗大量的堆内存，因为它们在转移它们之前或之后采用内存中的数据结构来组织记录。具体而言， <code>reduceByKey</code>并<code>aggregateByKey</code>创建在地图上侧这样的结构，和<code>&#39;ByKey</code>操作产生这些上减少侧。当数据不适合内存时，Spark会将这些表溢出到磁盘上，从而产生磁盘I / O的额外开销并增加垃圾回收。</p><p>随机播放还会在磁盘上生成大量中间文件。从Spark 1.3开始，将保留这些文件，直到不再使用相应的RDD并进行垃圾回收为止。这样做是为了在重新计算沿袭时无需重新创建随机播放文件。如果应用程序保留了对这些RDD的引用，或者如果GC不经常启动，则垃圾收集可能仅在很长一段时间之后才会发生。这意味着长时间运行的Spark作业可能会占用大量磁盘空间。<code>spark.local.dir</code>在配置Spark上下文时，临时存储目录由配置参数指定 。</p><p>可以通过调整各种配置参数来调整随机播放行为。请参阅《<a href="http://spark.apache.org/docs/2.1.2/configuration.html" target="_blank" rel="noopener">Spark配置指南</a>》中的“随机播放行为”部分。</p><h2 id="5-11-RDD-Persistence"><a href="#5-11-RDD-Persistence" class="headerlink" title="5.11 RDD Persistence"></a>5.11 RDD Persistence</h2><p>One of the most important capabilities in Spark is <em>persisting</em> (or <em>caching</em>) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.</p><p>You can mark an RDD to be persisted using the <code>persist()</code> or <code>cache()</code> methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.</p><p>In addition, each persisted RDD can be stored using a different <em>storage level</em>, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a <code>StorageLevel</code> object (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.storage.StorageLevel" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/storage/StorageLevel.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.html#pyspark.StorageLevel" target="_blank" rel="noopener">Python</a>) to <code>persist()</code>. The <code>cache()</code> method is a shorthand for using the default storage level, which is <code>StorageLevel.MEMORY_ONLY</code> (store deserialized objects in memory). The full set of storage levels is:</p><p>Spark中最重要的功能之一就是跨操作将数据集<em>持久化</em>（或<em>缓存</em>）在内存中。当您保留RDD时，每个节点都会将其计算的所有分区存储在内存中，并在该数据集（或从该数据集派生的数据集）上的其他操作中重用它们。这样可以使以后的操作更快（通常快10倍以上）。缓存是用于迭代算法和快速交互使用的关键工具。</p><p>您可以使用RDD上的<code>persist()</code>或<code>cache()</code>方法将其标记为要保留。第一次在操作中对其进行计算时，它将被保存在节点上的内存中。Spark的缓存是容错的-如果RDD的任何分区丢失，它将使用最初创建它的转换自动重新计算。</p><p>此外，每个持久化的RDD可以使用不同的<em>存储级别</em>进行存储，例如，允许您将数据集持久化在磁盘上，持久化在内存中，但作为序列化的Java对象（以节省空间）在节点之间复制。通过将一个<code>StorageLevel</code>对象（<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.storage.StorageLevel" target="_blank" rel="noopener">Scala</a>， <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/storage/StorageLevel.html" target="_blank" rel="noopener">Java</a>， <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.html#pyspark.StorageLevel" target="_blank" rel="noopener">Python</a>）传递给来设置这些级别 <code>persist()</code>。该<code>cache()</code>方法是使用默认存储级别<code>StorageLevel.MEMORY_ONLY</code>（将反序列化的对象存储在内存中）的简写。完整的存储级别集是：</p><table><thead><tr><th align="left">Storage Level</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left">MEMORY_ONLY</td><td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</td></tr><tr><td align="left">MEMORY_AND_DISK</td><td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed.</td></tr><tr><td align="left">MEMORY_ONLY_SER (Java and Scala)</td><td align="left">Store RDD as <em>serialized</em> Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a <a href="http://spark.apache.org/docs/2.1.2/tuning.html" target="_blank" rel="noopener">fast serializer</a>, but more CPU-intensive to read.</td></tr><tr><td align="left">MEMORY_AND_DISK_SER (Java and Scala)</td><td align="left">Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of recomputing them on the fly each time they’re needed.</td></tr><tr><td align="left">DISK_ONLY</td><td align="left">Store the RDD partitions only on disk.</td></tr><tr><td align="left">MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td><td align="left">Same as the levels above, but replicate each partition on two cluster nodes.</td></tr><tr><td align="left">OFF_HEAP (experimental)</td><td align="left">Similar to MEMORY_ONLY_SER, but store the data in <a href="http://spark.apache.org/docs/2.1.2/configuration.html#memory-management" target="_blank" rel="noopener">off-heap memory</a>. This requires off-heap memory to be enabled.</td></tr></tbody></table><p><strong>Note:</strong> <em>In Python, stored objects will always be serialized with the <a href="https://docs.python.org/2/library/pickle.html" target="_blank" rel="noopener">Pickle</a> library, so it does not matter whether you choose a serialized level. The available storage levels in Python include <code>MEMORY_ONLY</code>, <code>MEMORY_ONLY_2</code>, <code>MEMORY_AND_DISK</code>, <code>MEMORY_AND_DISK_2</code>, <code>DISK_ONLY</code>, and <code>DISK_ONLY_2</code>.</em></p><p>Spark also automatically persists some intermediate data in shuffle operations (e.g. <code>reduceByKey</code>), even without users calling <code>persist</code>. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call <code>persist</code> on the resulting RDD if they plan to reuse it.</p><h3 id="5-12-Which-Storage-Level-to-Choose"><a href="#5-12-Which-Storage-Level-to-Choose" class="headerlink" title="5.12 Which Storage Level to Choose?"></a>5.12 Which Storage Level to Choose?</h3><p>Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. We recommend going through the following process to select one:</p><ul><li>If your RDDs fit comfortably with the default storage level (<code>MEMORY_ONLY</code>), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.</li><li>If not, try using <code>MEMORY_ONLY_SER</code> and <a href="http://spark.apache.org/docs/2.1.2/tuning.html" target="_blank" rel="noopener">selecting a fast serialization library</a> to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)</li><li>Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.</li><li>Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). <em>All</em> the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.</li></ul><p>Spark的存储级别旨在在内存使用率和CPU效率之间提供不同的权衡。我们建议通过以下过程选择一个：</p><ul><li>如果您的RDD与默认的存储级别（<code>MEMORY_ONLY</code>）相称，请保持这种状态。这是CPU效率最高的选项，允许对RDD的操作尽可能快地运行。</li><li>如果不是，请尝试使用<code>MEMORY_ONLY_SER</code>并<a href="http://spark.apache.org/docs/2.1.2/tuning.html" target="_blank" rel="noopener">选择一个快速的序列化库，</a>以使对象的空间效率更高，但访问速度仍然相当快。（Java和Scala）</li><li>除非用于计算数据集的函数很昂贵，否则它们会过滤到磁盘上，否则它们会过滤大量数据。否则，重新计算分区可能与从磁盘读取分区一样快。</li><li>如果要快速恢复故障，请使用复制的存储级别（例如，如果使用Spark来处理来自Web应用程序的请求）。<em>所有</em>存储级别都通过重新计算丢失的数据来提供完全的容错能力，但是复制的存储级别使您可以继续在RDD上运行任务，而不必等待重新计算丢失的分区。</li></ul><h2 id="6-Shared-Variables"><a href="#6-Shared-Variables" class="headerlink" title="6. Shared Variables"></a>6. Shared Variables</h2><p>Normally, when a function passed to a Spark operation (such as <code>map</code> or <code>reduce</code>) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of <em>shared variables</em> for two common usage patterns: broadcast variables and accumulators.</p><p>通常，当传递给Spark操作的函数（例如<code>map</code>或<code>reduce</code>）在远程集群节点上执行时，它将在该函数中使用的所有变量的单独副本上工作。这些变量将复制到每台计算机，并且远程计算机上变量的更新不会传播回驱动程序。在各个任务之间支持通用的读写共享变量将效率很低。但是，Spark确实为两种常用用法模式提供了两种有限类型的<em>共享变量</em>：广播变量和累加器。</p><h3 id="6-1-Broadcast-Variables"><a href="#6-1-Broadcast-Variables" class="headerlink" title="6.1 Broadcast Variables"></a>6.1 Broadcast Variables</h3><p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.</p><p>Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.</p><p>Broadcast variables are created from a variable <code>v</code> by calling <code>SparkContext.broadcast(v)</code>. The broadcast variable is a wrapper around <code>v</code>, and its value can be accessed by calling the <code>value</code> method. The code below shows this:</p><p>广播变量使程序员可以在每台计算机上保留一个只读变量，而不是将其副本与任务一起发送。例如，可以使用它们以有效的方式为每个节点提供大型输入数据集的副本。Spark还尝试使用有效的广播算法分配广播变量，以降低通信成本。</p><p>火花动作是通过一组阶段执行的，这些阶段由分布式“随机”操作分开。Spark自动广播每个阶段任务所需的通用数据。在运行每个任务之前，以这种方式广播的数据以序列化形式缓存并反序列化。这意味着仅当跨多个阶段的任务需要相同的数据或以反序列化形式缓存数据非常重要时，显式创建广播变量才有用。</p><p><code>v</code>通过调用从变量创建广播变量<code>SparkContext.broadcast(v)</code>。broadcast变量是的包装<code>v</code>，可以通过调用<code>value</code> 方法访问其值。下面的代码显示了这一点：</p><pre><code class="hljs scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;  <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"broadcast variables"</span>).setMaster(<span class="hljs-string">"local"</span>)  <span class="hljs-keyword">val</span> sparkContext = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)  <span class="hljs-keyword">val</span> broadcast = sparkContext.broadcast(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>))  broadcast.value.foreach(println)&#125;</code></pre><p>After the broadcast variable is created, it should be used instead of the value <code>v</code> in any functions run on the cluster so that <code>v</code> is not shipped to the nodes more than once. In addition, the object <code>v</code> should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).</p><p>创建广播变量之后，应使用它而不是<code>v</code>集群中运行的任何函数中的值，这样才<code>v</code>不会多次将其传送给节点。此外，在<code>v</code>广播对象后不应修改该对象 ，以确保所有节点都获得相同的广播变量值（例如，如果变量稍后被运送到新节点）。</p><h2 id="6-2-Accumulators"><a href="#6-2-Accumulators" class="headerlink" title="6.2 Accumulators"></a>6.2 Accumulators</h2><p>Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.</p><p>As a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance <code>counter</code>) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table.</p><p>累加器是仅通过关联和交换操作“添加”的变量，因此可以有效地并行支持。它们可用于实现计数器（如在MapReduce中）或总和。Spark本机支持数字类型的累加器，程序员可以添加对新类型的支持。</p><p>作为用户，您可以创建命名或未命名的累加器。如下图所示，一个已命名的累加器（在这种情况下<code>counter</code>）将在Web UI中显示修改该累加器的阶段。Spark在“任务”表中显示由任务修改的每个累加器的值。</p><p>A numeric accumulator can be created by calling <code>SparkContext.longAccumulator()</code> or <code>SparkContext.doubleAccumulator()</code> to accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using the <code>add</code> method. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its <code>value</code> method.</p><p>The code below shows an accumulator being used to add up the elements of an array:</p><p>可以通过分别调用<code>SparkContext.longAccumulator()</code>或<code>SparkContext.doubleAccumulator()</code> 累加Long或Double类型的值来创建数字累加器。然后，可以使用<code>add</code>方法将在集群上运行的任务添加到集群中。但是，他们无法读取其值。只有驱动程序可以使用其<code>value</code>方法读取累加器的值。</p><p>下面的代码显示了一个累加器，用于累加一个数组的元素：</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Accumulators</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"Accumulators"</span>).setMaster(<span class="hljs-string">"local[*]"</span>)    <span class="hljs-keyword">val</span> sparkContext = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)    <span class="hljs-keyword">val</span> accumulator = sparkContext.longAccumulator(<span class="hljs-string">"my accumulator"</span>)    <span class="hljs-keyword">val</span> myRdd = sparkContext.parallelize(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>))    <span class="hljs-keyword">val</span> sum = myRdd.foreach(x =&gt; accumulator.add(x))    println(accumulator.value)  &#125;&#125;</code></pre><p>While this code used the built-in support for accumulators of type Long, programmers can also create their own types by subclassing <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.util.AccumulatorV2" target="_blank" rel="noopener">AccumulatorV2</a>. The AccumulatorV2 abstract class has several methods which one has to override: <code>reset</code> for resetting the accumulator to zero, <code>add</code> for adding another value into the accumulator, <code>merge</code> for merging another same-type accumulator into this one. Other methods that must be overridden are contained in the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.util.AccumulatorV2" target="_blank" rel="noopener">API documentation</a>. For example, supposing we had a <code>MyVector</code> class representing mathematical vectors, we could write:</p><p>虽然此代码使用了对Long类型的累加器的内置支持，但程序员也可以通过对<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.util.AccumulatorV2" target="_blank" rel="noopener">AccumulatorV2</a>进行子类化来创建自己的类型。AccumulatorV2抽象类具有几种必须被覆盖的方法：<code>reset</code>将累加器重置为零，<code>add</code>将另一个值添加到累加器，<code>merge</code>将另一个相同类型的累加器合并到该累加器中 。<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.util.AccumulatorV2" target="_blank" rel="noopener">API文档</a>中包含其他必须重写的方法。例如，假设我们有一个<code>MyVector</code>代表数学向量的类，我们可以这样写：</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorAccumulatorV2</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">AccumulatorV2</span>[<span class="hljs-type">MyVector</span>, <span class="hljs-type">MyVector</span>] </span>&#123;  <span class="hljs-keyword">private</span> <span class="hljs-keyword">val</span> myVector: <span class="hljs-type">MyVector</span> = <span class="hljs-type">MyVector</span>.createZeroVector  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset</span></span>(): <span class="hljs-type">Unit</span> = &#123;    myVector.reset()  &#125;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span></span>(v: <span class="hljs-type">MyVector</span>): <span class="hljs-type">Unit</span> = &#123;    myVector.add(v)  &#125;  ...&#125;<span class="hljs-comment">// Then, create an Accumulator of this type:</span><span class="hljs-keyword">val</span> myVectorAcc = <span class="hljs-keyword">new</span> <span class="hljs-type">VectorAccumulatorV2</span><span class="hljs-comment">// Then, register it into spark context:</span>sc.register(myVectorAcc, <span class="hljs-string">"MyVectorAcc1"</span>)</code></pre><p>Note that, when programmers define their own type of AccumulatorV2, the resulting type can be different than that of the elements added.</p><p>For accumulator updates performed inside <strong>actions only</strong>, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.</p><p>Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like <code>map()</code>. The below code fragment demonstrates this property:</p><p>请注意，当程序员定义自己的AccumulatorV2类型时，结果类型可能与所添加元素的类型不同。</p><p>对于<strong>仅</strong>在<strong>操作</strong>内部<strong>执行的</strong>累加器更新，Spark保证每个任务对累加器的更新将仅应用一次，即重新启动的任务将不会更新该值。在转换中，用户应注意，如果重新执行任务或作业阶段，则可能不止一次应用每个任务的更新。</p><p>累加器不会更改Spark的惰性评估模型。如果在RDD上的操作中对其进行更新，则仅当将RDD计算为操作的一部分时才更新其值。因此，当在类似的惰性转换中进行累加器更新时，不能保证执行更新<code>map()</code>。下面的代码片段演示了此属性：</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> accum = sc.longAccumulatordata.map &#123; x =&gt; accum.add(x); x &#125;<span class="hljs-comment">// Here, accum is still 0 because no actions have caused the map operation to be computed.</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive编程指南——笔记（二）</title>
    <link href="/2020/07/11/Hive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E2%80%94%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2020/07/11/Hive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E2%80%94%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="Hive编程指南—笔记-二"><a href="#Hive编程指南—笔记-二" class="headerlink" title="Hive编程指南—笔记(二)"></a>Hive编程指南—笔记(二)</h1><h2 id="5-HiveQL：数据操作"><a href="#5-HiveQL：数据操作" class="headerlink" title="5. HiveQL：数据操作"></a>5. HiveQL：数据操作</h2><h3 id="5-1-向管理表中装载数据"><a href="#5-1-向管理表中装载数据" class="headerlink" title="5.1 向管理表中装载数据"></a>5.1 向管理表中装载数据</h3><ol><li><p>Hive不支持行级的插入，删除，更新</p></li><li><pre><code>LOAD DATA LOCAL INPATH &apos;${env:HOME}/california-employees&apos;OVERWRITE INTO TABLE employeesPARTITION (country = &apos;US&apos;, state = &apos;CA&apos;);<pre><code class="hljs angelscript"><span class="hljs-number">3.</span> local：本地文件系统，不加local指的是hdfs，分布式文件系统<span class="hljs-number">4.</span> overwrite：重写，删除原有数据，重新插入新的数据，<span class="hljs-number">5.</span> 不加overwrite时，如果load的数据文件和已有的同名，hive会在文件名后面加后缀，不会覆盖原有的数据<span class="hljs-number">6.</span> 一般跟的是文件夹，默认会将文件夹中所有的文件都装载到表里<span class="hljs-number">7.</span> 需要装载的数据文件格式需要跟建表时候指定的格式一样### <span class="hljs-number">5.2</span> 通过查询语句向表中插入数据<span class="hljs-number">1.</span></code></pre>INSERT OVERWRITE TABLE employeesPARTITION (country = &apos;US&apos;, state = &apos;OR&apos;)SELECT * FROM staged_employees seWHERE se.cnty = &apos;US&apos; AND se.st = &apos;OR&apos;;<pre><code class="hljs lua"><span class="hljs-number">2.</span> 往一个表里需要<span class="hljs-built_in">insert</span>多个<span class="hljs-built_in">select</span>语句可以写在一起</code></pre>FROM staged_employees seINSERT OVERWRITE TABLE employees　PARTITION (country = &apos;US&apos;, state = &apos;OR&apos;)　SELECT * WHERE se.cnty = &apos;US&apos; AND se.st = &apos;OR&apos;INSERT OVERWRITE TABLE employees　PARTITION (country = &apos;US&apos;, state = &apos;CA&apos;)　SELECT * WHERE se.cnty = &apos;US&apos; AND se.st = &apos;CA&apos;INSERT OVERWRITE TABLE employees　PARTITION (country = &apos;US&apos;, state = &apos;IL&apos;)　SELECT * WHERE se.cnty = &apos;US&apos; AND se.st = &apos;IL&apos;;<pre><code class="hljs markdown"><span class="hljs-bullet">3. </span>动态分区插入：<span class="hljs-bullet">   - </span>如果需要创建非常多的分区，那么用户就需要写非常多的SQL<span class="hljs-bullet">   - </span>Hive提供了一个动态分区功能，其可以基于查询参数推断出需要创建的分区名称   -</code></pre>  INSERT OVERWRITE TABLE employees  PARTITION (country, state)  SELECT ..., se.cnty, se.st  FROM staged_employees se;  <pre><code class="hljs pf">- 是为了强调源表字段值和输出分区值之间的关系是根据位置而不是根据命名来匹配的。- 用户也可以混合使用动态和静态分区。如下这个例子中指定了country字段的值为静态的US，而分区字段<span class="hljs-keyword">state</span>是动态值：</code></pre>  INSERT OVERWRITE TABLE employees  PARTITION (country = &apos;US&apos;, state)  SELECT ..., se.cnty, se.st  FROM staged_employees se  WHERE se.cnty = &apos;US&apos;;  <pre><code class="hljs asciidoc"><span class="hljs-code">  静态分区键必须出现在动态分区键之前。</span><span class="hljs-bullet">- </span>动态分区功能默认情况下没有开启。开启后，默认是以“严格”模式执行的，在这种模式下要求至少有一列分区字段是静态的。这有助于阻止因设计错误导致查询产生大量的分区。</code></pre>  hive&gt; set hive.exec.dynamic.partition=true;  hive&gt; set hive.exec.dynamic.partition.mode=nonstrict;  hive&gt; set hive.exec.max.dynamic.partitions.pernode=1000;  <pre><code class="hljs clean">### <span class="hljs-number">5.3</span> 单个查询语句中创建表并加载数据<span class="hljs-number">1.</span> 创建表并装载数据</code></pre>CREATE TABLE ca_employeesAS SELECT name, salary, addressFROM employeesWHERE se.state = &apos;CA&apos;;<pre><code class="hljs angelscript">   适用于从一个宽表中取出部分数据### <span class="hljs-number">5.4</span> 导出数据<span class="hljs-number">1.</span> 如果数据格式正好是想要的，把文件直接从hdfs上拷贝下来<span class="hljs-number">2.</span> 如果不是的话，可以把数据导入来</code></pre>INSERT OVERWRITE LOCAL DIRECTORY &apos;/tmp/ca_employees&apos;SELECT name, salary, addressFROM employeesWHERE se.state = &apos;CA&apos;;<pre><code class="hljs angelscript"><span class="hljs-number">3.</span> 不管在源表中数据实际是怎么存储的，Hive会将所有的字段序列化成字符串写入到文件中。Hive使用和Hive内部存储的表相同的编码方式来生成输出文件。<span class="hljs-number">4.</span> 和向表中插入数据一样，用户也是可以通过如下方式指定多个输出文件夹目录的：</code></pre>FROM staged_employees seINSERT OVERWRITE DIRECTORY &apos;/tmp/or_employees&apos;　SELECT * WHERE se.cty = &apos;US&apos; and se.st = &apos;OR&apos;INSERT OVERWRITE DIRECTORY &apos;/tmp/ca_employees&apos;　SELECT * WHERE se.cty = &apos;US&apos; and se.st = &apos;CA&apos;INSERT OVERWRITE DIRECTORY &apos;/tmp/il_employees&apos;　SELECT * WHERE se.cty = &apos;US&apos; and se.st = &apos;IL&apos;;</code></pre></li></ol><h2 id="6-HiveQL：查询"><a href="#6-HiveQL：查询" class="headerlink" title="6. HiveQL：查询"></a>6. HiveQL：查询</h2><h3 id="6-1-SELECT…FROM语句"><a href="#6-1-SELECT…FROM语句" class="headerlink" title="6.1 SELECT…FROM语句"></a>6.1 SELECT…FROM语句</h3><h4 id="6-1-1-使用正则表达式指定列"><a href="#6-1-1-使用正则表达式指定列" class="headerlink" title="6.1.1 使用正则表达式指定列"></a>6.1.1 使用正则表达式指定列</h4><ol><li>SELECT symbol, ‘price.*’ FROM stocks;</li></ol><h4 id="6-1-2-使用列值进行计算"><a href="#6-1-2-使用列值进行计算" class="headerlink" title="6.1.2 使用列值进行计算"></a>6.1.2 使用列值进行计算</h4><h4 id="6-1-3-算数表达式"><a href="#6-1-3-算数表达式" class="headerlink" title="6.1.3 算数表达式"></a>6.1.3 算数表达式</h4><ol><li>和java一样</li></ol><h4 id="6-1-4-使用函数"><a href="#6-1-4-使用函数" class="headerlink" title="6.1.4 使用函数"></a>6.1.4 使用函数</h4><ol><li>函数很多</li></ol><h4 id="6-1-5-LIMIT语句"><a href="#6-1-5-LIMIT语句" class="headerlink" title="6.1.5 LIMIT语句"></a>6.1.5 LIMIT语句</h4><h4 id="6-1-6-列别名"><a href="#6-1-6-列别名" class="headerlink" title="6.1.6 列别名"></a>6.1.6 列别名</h4><h4 id="6-1-7-嵌套SELECT语句"><a href="#6-1-7-嵌套SELECT语句" class="headerlink" title="6.1.7 嵌套SELECT语句"></a>6.1.7 嵌套SELECT语句</h4><ol><li>嵌套子句需要起别名</li></ol><h4 id="6-1-8-CASW…WHEN…THEN句式"><a href="#6-1-8-CASW…WHEN…THEN句式" class="headerlink" title="6.1.8 CASW…WHEN…THEN句式"></a>6.1.8 CASW…WHEN…THEN句式</h4><h4 id="6-1-9-什么情况下Hive可以避免进行MapReduce"><a href="#6-1-9-什么情况下Hive可以避免进行MapReduce" class="headerlink" title="6.1.9 什么情况下Hive可以避免进行MapReduce"></a>6.1.9 什么情况下Hive可以避免进行MapReduce</h4><ol><li>Hive中对某些情况的查询可以不必使用MapReduce，也就是所谓的本地模式</li><li>SELECT * 不需要</li><li>where 后面跟的是分区字段 不需要</li><li>如果属性hive.exec.mode.local.auto 的值设置为true的话，Hive还会尝试使用本地模式执行其他的操作：set hive.exec.mode.local.auto=true;</li></ol><h3 id="6-2-Where语句"><a href="#6-2-Where语句" class="headerlink" title="6.2 Where语句"></a>6.2 Where语句</h3><h4 id="6-2-1-谓词操作符"><a href="#6-2-1-谓词操作符" class="headerlink" title="6.2.1 谓词操作符"></a>6.2.1 谓词操作符</h4><ol><li>A [NOT] BETWEEN B AND C <ul><li>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于或等于B，而且小于或等于C，则结果为TRUE，反之为FALSE.如果使用NOT关键字则可达到相反的效果（Hive 0.9.0版本中新增）</li></ul></li></ol><h4 id="6-2-2-关于浮点数比较"><a href="#6-2-2-关于浮点数比较" class="headerlink" title="6.2.2 关于浮点数比较"></a>6.2.2 关于浮点数比较</h4><ol><li>浮点数比较很容易出现问题</li><li>解决办法<ol><li>保存成string类型</li><li>比较的时候强转成相同的数据类型之后在进行比较</li><li>设计到前相关的数据，不要保存成double类型</li><li>建表的时候，跟钱相关的保存成decimal类型，其余字段尽可能保存成string类型</li></ol></li></ol><h4 id="6-2-3-LIke和RLike"><a href="#6-2-3-LIke和RLike" class="headerlink" title="6.2.3 LIke和RLike"></a>6.2.3 LIke和RLike</h4><ol><li>like通过字符串的开头或结尾，以及指定特定的子字符串，或当子字符串出现在字符串内的任何位置时进行匹配。</li><li>Rlike可以通过java中的正则表达式进行匹配，没进行过测试，和rep有什么区别</li></ol><h3 id="6-3-Group-By语句"><a href="#6-3-Group-By语句" class="headerlink" title="6.3 Group By语句"></a>6.3 Group By语句</h3><ol><li><p>关键字顺序</p><p>where—&gt;group by—&gt;having</p></li><li><p>having 可以用子查询实现一样的效果，不知道哪个效率更好</p></li></ol><h3 id="6-4-JOIN语句"><a href="#6-4-JOIN语句" class="headerlink" title="6.4 JOIN语句"></a>6.4 JOIN语句</h3><ol><li>默认是inner join</li><li>只支持等值连接</li><li>当对3个或者更多个表进行JOIN连接时，如果每个ON子句都使用相同的连接键的话，那么只会产生一个MapReduce job。</li><li>Hive同时假定查询中最后一个表是最大的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。因此，用户需要保证连续查询中的表的大小从左到右是依次增加的。</li><li>左半开连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右边表满足ON语句中的判定条件</li><li>内连接，外接连（全连接，左连接，右连接，左半开连接）</li><li>Hive不支持右半开连接（RIGHT SEMI-JOIN）。</li><li>SEMI-JOIN比通常的INNER JOIN要更高效，原因如下：对于左表中一条指定的记录，在右边表中一旦找到匹配的记录，Hive就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。</li><li></li></ol><h3 id="6-5-Order-by和Sort-by"><a href="#6-5-Order-by和Sort-by" class="headerlink" title="6.5 Order by和Sort by"></a>6.5 Order by和Sort by</h3><ol><li>order by 全局有序，一个reducer</li><li>sort by 局部有序，每个reducer内有序</li></ol><h3 id="6-6-含有SORT-by和Distribute-by"><a href="#6-6-含有SORT-by和Distribute-by" class="headerlink" title="6.6 含有SORT by和Distribute by"></a>6.6 含有SORT by和Distribute by</h3><ol><li><h3 id="6-7-Cluster-by"><a href="#6-7-Cluster-by" class="headerlink" title="6.7 Cluster by"></a>6.7 Cluster by</h3></li></ol><h3 id="6-8-类型转换"><a href="#6-8-类型转换" class="headerlink" title="6.8 类型转换"></a>6.8 类型转换</h3><h3 id="6-9-抽样查询"><a href="#6-9-抽样查询" class="headerlink" title="6.9 抽样查询"></a>6.9 抽样查询</h3><h3 id="6-10-Union-all"><a href="#6-10-Union-all" class="headerlink" title="6.10 Union all"></a>6.10 Union all</h3>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>读书笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive编程指南——笔记（一）</title>
    <link href="/2020/06/23/Hive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E2%80%94%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2020/06/23/Hive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E2%80%94%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="3-数据类型和文件格式"><a href="#3-数据类型和文件格式" class="headerlink" title="3. 数据类型和文件格式"></a>3. 数据类型和文件格式</h2><h3 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h3><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><ul><li>hive中的所有数据类型是对java中接口的实现，具体细节和java中的数据类型一样</li><li>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</li></ul><h2 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h2><p>表6-2</p><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct(‘John’，’Doe’)</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map(‘first’，’JOIN’，’last’，’Doe’)</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array(‘John’，’Doe’)</td></tr></tbody></table><ul><li>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</li><li>例子</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">test</span>(<span class="hljs-keyword">name</span> <span class="hljs-keyword">string</span>,friends <span class="hljs-built_in">array</span>&lt;<span class="hljs-keyword">string</span>&gt;,children <span class="hljs-keyword">map</span>&lt;<span class="hljs-keyword">string</span>, <span class="hljs-built_in">int</span>&gt;,address <span class="hljs-keyword">struct</span>&lt;street:<span class="hljs-keyword">string</span>, city:<span class="hljs-keyword">string</span>&gt;)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span>collection items <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'_'</span><span class="hljs-keyword">map</span> <span class="hljs-keyword">keys</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">':'</span><span class="hljs-keyword">lines</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\n'</span>;</code></pre><p>row format delimited fields terminated by ‘,’ – 列分隔符</p><p>collection items terminated by ‘_’     –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p>map keys terminated by ‘:’                – MAP中的key与value的分隔符</p><p>lines terminated by ‘\n’;                  – 行分隔符</p><p>存储的数据：</p><pre><code class="hljs json">&#123;    <span class="hljs-attr">"name"</span>: <span class="hljs-string">"songsong"</span>,    <span class="hljs-attr">"friends"</span>: [<span class="hljs-string">"bingbing"</span> , <span class="hljs-string">"lili"</span>] ,       <span class="hljs-comment">//列表Array, </span>    <span class="hljs-attr">"children"</span>: &#123;                      <span class="hljs-comment">//键值Map,</span>        <span class="hljs-attr">"xiao song"</span>: <span class="hljs-number">18</span> ,        <span class="hljs-attr">"xiaoxiao song"</span>: <span class="hljs-number">19</span>    &#125;    <span class="hljs-string">"address"</span>: &#123;                      <span class="hljs-comment">//结构Struct,</span>        <span class="hljs-attr">"street"</span>: <span class="hljs-string">"hui long guan"</span> ,        <span class="hljs-attr">"city"</span>: <span class="hljs-string">"beijing"</span>     &#125;&#125;</code></pre><h3 id="3-3-文本文件数据编码"><a href="#3-3-文本文件数据编码" class="headerlink" title="3.3 文本文件数据编码"></a>3.3 文本文件数据编码</h3><table><thead><tr><th>分隔符</th><th>描述</th></tr></thead><tbody><tr><td>\n</td><td>对于文本文件来说，每行都是一条记录，因此换行符可以分割记录</td></tr><tr><td>^A（Ctrl+A）</td><td>用于分隔字段（列）。在CREATE TABLE语句中可以使用八进制编码\001表示</td></tr><tr><td>^B</td><td>用于分隔ARRARY或者STRUCT中的元素，或用于MAP中键-值对之间的分隔。在CREATE TABLE 语句中可以使用八进制编码\002表示</td></tr><tr><td>^C</td><td>用于MAP中键和值之间的分隔。在CREATE TABLE 语句中可以使用八进制编码\003表示</td></tr></tbody></table><pre><code class="hljs cos">CREATE TABLE employees (　name　　　　　　　STRING,　salary　　　　　　FLOAT,　subordinates 　　ARRAY&lt;STRING&gt;,　deductions　　　　MAP&lt;STRING, FLOAT&gt;,　address　　　　　STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;)<span class="hljs-comment">;</span>&#123;　<span class="hljs-string">"name"</span>: <span class="hljs-string">"John Doe"</span>,　<span class="hljs-string">"salary"</span>: <span class="hljs-number">100000.0</span>,　<span class="hljs-string">"subordinates"</span>: [<span class="hljs-string">"Mary Smith"</span>, <span class="hljs-string">"Todd Jones"</span>],　<span class="hljs-string">"deductions"</span>: &#123;　　<span class="hljs-string">"Federal Taxes"</span>: 　.<span class="hljs-number">2</span>,　　<span class="hljs-string">"State Taxes"</span>: 　　.<span class="hljs-number">05</span>,　　<span class="hljs-string">"Insurance"</span>: 　　　.<span class="hljs-number">1</span>　&#125;,　<span class="hljs-string">"address"</span>: &#123;　　<span class="hljs-string">"street"</span>: <span class="hljs-string">"1 Michigan Ave."</span>,　　<span class="hljs-string">"city"</span>: <span class="hljs-string">"Chicago"</span>,　　<span class="hljs-string">"state"</span>: <span class="hljs-string">"IL"</span>,　　<span class="hljs-string">"zip"</span>: <span class="hljs-number">60600</span>　&#125;&#125;John Doe<span class="hljs-symbol">^A100000</span><span class="hljs-number">.0</span><span class="hljs-symbol">^AMary</span> Smith<span class="hljs-symbol">^BTodd</span> Jones<span class="hljs-symbol">^AFederal</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.2</span><span class="hljs-symbol">^BStateTaxes</span><span class="hljs-symbol">^C</span><span class="hljs-number">.05</span>^ BInsurance<span class="hljs-symbol">^C</span><span class="hljs-number">.1</span><span class="hljs-symbol">^A1</span> Michigan Ave.<span class="hljs-symbol">^BChicago</span><span class="hljs-symbol">^BIL</span><span class="hljs-symbol">^B60600</span>Mary Smith<span class="hljs-symbol">^A80000</span><span class="hljs-number">.0</span><span class="hljs-symbol">^ABill</span> King<span class="hljs-symbol">^AFederal</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.2</span><span class="hljs-symbol">^BState</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.05</span><span class="hljs-symbol">^BInsurance</span>^ C<span class="hljs-number">.1</span><span class="hljs-symbol">^A100</span> Ontario St.<span class="hljs-symbol">^BChicago</span><span class="hljs-symbol">^BIL</span><span class="hljs-symbol">^B60601</span>Todd Jones<span class="hljs-symbol">^A70000</span><span class="hljs-number">.0</span><span class="hljs-symbol">^AFederal</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.15</span><span class="hljs-symbol">^BState</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.03</span><span class="hljs-symbol">^BInsurance</span><span class="hljs-symbol">^C</span><span class="hljs-number">.1</span><span class="hljs-symbol">^A200</span>   Chicago Ave.<span class="hljs-symbol">^BOak</span> Park<span class="hljs-symbol">^BIL</span><span class="hljs-symbol">^B60700</span>Bill King<span class="hljs-symbol">^A60000</span><span class="hljs-number">.0</span><span class="hljs-symbol">^AFederal</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.15</span><span class="hljs-symbol">^BState</span>Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.03</span><span class="hljs-symbol">^BInsurance</span><span class="hljs-symbol">^C</span><span class="hljs-number">.1</span><span class="hljs-symbol">^A300</span> Obscure Dr.<span class="hljs-symbol">^BObscuria</span><span class="hljs-symbol">^BIL</span><span class="hljs-symbol">^B60100</span></code></pre><h3 id="3-4-读时模式"><a href="#3-4-读时模式" class="headerlink" title="3.4 读时模式"></a>3.4 读时模式</h3><p>不懂</p><h2 id="4-HiveQL：数据定义"><a href="#4-HiveQL：数据定义" class="headerlink" title="4. HiveQL：数据定义"></a>4. HiveQL：数据定义</h2><h3 id="4-1-Hive中的数据库"><a href="#4-1-Hive中的数据库" class="headerlink" title="4.1 Hive中的数据库"></a>4.1 Hive中的数据库</h3><ol><li>数据库类似命名空间或者目录</li><li>创建 create database [if not exists] database_name LOCATION … COMMENT…  WITH DBPROPERTIES()<ol><li>location 指定位置</li><li>comment 添加注释</li><li>WITH DBPROPERTIES 添加属性</li></ol></li><li>查看数据库 show database [like …]</li><li>查看数据的详细信息 describe database_name</li><li>使用数据库use database_name</li><li>查看当前正在使用的数据库 SELECT current_database()</li><li>删除数据库 drop database if exists database_name [CASCADE]</li></ol><h3 id="4-2-修改数据库"><a href="#4-2-修改数据库" class="headerlink" title="4.2 修改数据库"></a>4.2 修改数据库</h3><ol><li>修改数据库，只能修改DBPROPERTIES，数据库名字和存储位置都不能修改，alter database  database_name SET DBPROPERTIES (‘edited-by’ = ‘Joe Dba’);</li><li>没有办法可以删除或者“重置”数据库属性</li></ol><h3 id="4-3-创建表"><a href="#4-3-创建表" class="headerlink" title="4.3 创建表"></a>4.3 创建表</h3><ol><li>创建表create table if not exists table_name()</li><li>if not exists 只判断有没有同名的表存在，不去比较属性是不是不一样，如果是想修改表的属性，需要先删除原来的表，再重建新表</li><li>Hive会自动增加两个表属性：一个是last_modified_by，其保存着最后修改这个表的用户的用户名；另一个是last_modified_time，其保存着最后一次修改<br>的新纪元时间秒。</li><li>查看表结构 desc table_name，只输出包含有列描述信息的表结构信息</li><li>查看详细的表结构信息，DESCRIBE EXTENDED table_name，展示结构很乱</li><li>查看详细的表结构信息，DESCRIBE formatted table_name，展示结构很清晰</li><li>SHOW TBLPROPERTIES table_name命令，用于列举出某个表的TBLPROPERTIES属性<br>信息。</li><li>拷贝一个已存在的表结构，不包含数据，CREATE TABLE IF NOT EXISTS mydb.employees2<br>LIKE mydb.employees;</li><li>查看所有的表：show tables [like …]；支持简单正则</li><li>指定查看摸个数据库下的表：show tables in database_name;</li></ol><h4 id="4-3-1-管理表"><a href="#4-3-1-管理表" class="headerlink" title="4.3.1 管理表"></a>4.3.1 管理表</h4><ol><li>hive控制着数据的生命周期，删除表的时候，表中的数据也会被删除</li></ol><h4 id="4-3-2-外部表"><a href="#4-3-2-外部表" class="headerlink" title="4.3.2 外部表"></a>4.3.2 外部表</h4><ol><li>CREATE EXTERNAL TABLE IF NOT EXISTS table_name() ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’  LOCATION ‘/data/stocks’;</li><li>hive删除外部表时候，表中的数据不会被删除</li><li>查看是外部表还是内部表：DESCRIBE EXTENDED table_name中tableType:</li></ol><h3 id="4-4-分区表、管理表"><a href="#4-4-分区表、管理表" class="headerlink" title="4.4 分区表、管理表"></a>4.4 分区表、管理表</h3><ol><li>分区表，更快的查询</li><li>存储，按照分区字段分目录存储</li><li>分区字段使用起来和普通字段是一样的</li><li>如果表中的数据太多，select代价很大，可以开启hive 的严格模式，select必须要加上where条件，set hive.mapred.mode=strict; set hive.mapred.mode=nonstrict;</li></ol><blockquote><p>如果表中的数据以及分区个数都非常大的话，执行这样一个包含有所有<br>分区的查询可能会触发一个巨大的MapReduce任务。一个高度建议的安全措施就是<br>将Hive设置为“strict(严格)”模式，这样如果对分区表进行查询而WHERE子句没<br>有加分区过滤的话，将会禁止提交这个任务。用户也可以按照下面的语句将属性值<br>设置为“nostrict(非严格)”</p></blockquote><ol start="5"><li>查看分区show partitions table_name partition(’‘);</li></ol><h4 id="4-4-1-外部分区表"><a href="#4-4-1-外部分区表" class="headerlink" title="4.4.1 外部分区表"></a>4.4.1 外部分区表</h4><ol><li>创建分区，ALTER TABLE … ADD PARTITION</li><li>往分区内移入数据，hdfs拷贝数据到对应的目录下</li><li>查看到分区数据所在的路径： DESCRIBE EXTENDED log_messages PARTITION (year=2012, month=1, day=2);</li></ol><h4 id="4-4-2-自定义表的存储格式"><a href="#4-4-2-自定义表的存储格式" class="headerlink" title="4.4.2 自定义表的存储格式"></a>4.4.2 自定义表的存储格式</h4><ol><li>hive默认存储格式：text</li><li>创建表的时候指定：STORED AS TEXTFILE;</li><li>记录的解析是由序列化器/反序列化器（或者缩写为SerDe）来控制的。</li><li>ROW FORMAT SERDE …指定SerDe</li><li>STORED AS INPUTFORMAT … OUTPUTFORMAT …子句分别指定了用于输入格式和输出格式的Java类。</li><li><strong>Hive使用一个inputformat对象将输入流分割成记录，然后使用一个outputformat对象来将记录格式化为输出流（例如查询的输出结果），再使用一个SerDe在读数据时将记录解析成列，在写数据时将列编码成记录。</strong>—有疑问</li></ol><h3 id="4-5-删除表"><a href="#4-5-删除表" class="headerlink" title="4.5 删除表"></a>4.5 删除表</h3><ol><li>drop table if exists table_name</li><li>如果开启回收站功能，会将删除的数据移到用户根目录下的.Trash目录下，即hdfs中的/user/$USER/.Trash目录</li></ol><h3 id="4-6-修改表"><a href="#4-6-修改表" class="headerlink" title="4.6 修改表"></a>4.6 修改表</h3><p>不怎么常用，一般会选择重新建表</p><h4 id="4-6-2-增加，修改，删除分区"><a href="#4-6-2-增加，修改，删除分区" class="headerlink" title="4.6.2 增加，修改，删除分区"></a>4.6.2 增加，修改，删除分区</h4><ol><li>增加分区：ALTER TABLE log_messages ADD IF NOT EXISTS PARTITION (year = 2011, month = 1, day = 1) LOCATION ‘/logs/2011/01/01’</li><li>修改分区路径：ALTER TABLE log_messages PARTITION(year = 2011, month = 12, day = 2) SET LOCATION ‘s3n://ourbucket/logs/2011/01/02’;</li><li>删除分区：ALTER TABLE log_messages DROP IF EXISTS PARTITION(year = 2011, month = 12, day = 2);</li></ol>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>读书笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL中字符串相关的部分函数（一）</title>
    <link href="/2020/06/23/Mysql%E4%B8%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E5%85%B3%E7%9A%84%E9%83%A8%E5%88%86%E5%87%BD%E6%95%B0/"/>
    <url>/2020/06/23/Mysql%E4%B8%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E5%85%B3%E7%9A%84%E9%83%A8%E5%88%86%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-trim"><a href="#1-trim" class="headerlink" title="1. trim"></a>1. trim</h2><p><strong>TRIM([{BOTH | LEADING | TRAILING} [remstr] FROM] str)</strong> </p><p>作用：去掉指定的字符</p><ol><li><p>不指定指定字符，默认去掉空格</p><ul><li><p>默认去掉左右空格： select trim(‘  a   ‘);   –’a’</p></li><li><p>去掉左边空格：select ltrim(‘   a  ‘);  –’a  ‘</p></li><li><p>去掉右边空格：select rtrim(‘  a  ‘);  –’  a’</p></li></ul></li><li><p>指定字符</p><ul><li>去掉两边字符：select trim(both ‘|’  from ‘|abc|’); –’abc’</li><li>去掉左边字符：select trim(LEADING ‘|’  from ‘|abc|’); –’abc|’</li><li>去掉右边字符：select trim(TRAILING ‘|’  from ‘|abc|’); –’|abc’</li></ul></li></ol><h2 id="2-instr"><a href="#2-instr" class="headerlink" title="2. instr"></a>2. instr</h2><p><strong>instr(str,substr)</strong> </p><p>作用：返回substr第一次在str中出现的位置</p><p>select instr(‘aaa_0001’ ,  ‘_’); –4</p><h2 id="3-substr"><a href="#3-substr" class="headerlink" title="3. substr"></a>3. substr</h2><p><strong>substr(str,pos[,len])</strong></p><p>作用：从字符串中的指定位置pos开始取一个字符串返回</p><p>select substr(‘aaa_0001’ ,  4); –’_0001’</p><h2 id="4-substring"><a href="#4-substring" class="headerlink" title="4. substring"></a>4. substring</h2><p><strong>substring(str,n,len)</strong></p><p>作用：获取子串，从n开始截取str中长度为len的子串</p><p>select substring(‘aaa_0001’, 1, 3); –’aaa’</p>]]></content>
    
    
    <categories>
      
      <category>MySQL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive常用函数（一）</title>
    <link href="/2020/06/18/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
    <url>/2020/06/18/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-concat：拼接字符串"><a href="#1-concat：拼接字符串" class="headerlink" title="1. concat：拼接字符串"></a>1. concat：拼接字符串</h2><p> <strong>concat(string|binary A, string|binary B…)</strong></p><p> <strong>作用</strong>：将字符串按顺序拼接成一个字符串</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat</span>(<span class="hljs-string">'a'</span>,<span class="hljs-string">'_'</span>,<span class="hljs-string">'b'</span>);  <span class="hljs-comment">-- a_b</span></code></pre><p> <strong>注意</strong>：如果有任何一个参数为null，返回结果为null </p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat</span>(<span class="hljs-string">'a'</span>,<span class="hljs-string">'_'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-literal">null</span>);  <span class="hljs-comment">-- NULL</span></code></pre><h2 id="2-concat-ws：指定拼接字符串拼接"><a href="#2-concat-ws：指定拼接字符串拼接" class="headerlink" title="2. concat_ws：指定拼接字符串拼接"></a>2. concat_ws：指定拼接字符串拼接</h2><p><strong>concat_ws(string SEP, string A, string B…)</strong></p><p>concat_ws是concat的特殊形式，可以自定义分隔符SEP</p><p>select concat_ws(‘_’,’a’,’b’);    – a_b</p><p><strong>注意</strong>：</p><ol><li>分隔符可以任何参数，字符串，特殊符号都可以</li><li>分隔符为null时，结果为null</li><li>concat_ws会忽略其他除分隔符外其他为null的参数，不会忽略空字符串</li></ol><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat_ws</span>(<span class="hljs-string">'W'</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>);<span class="hljs-comment">-- aWb</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat_ws</span>(<span class="hljs-literal">null</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>);<span class="hljs-comment">-- NULL</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat_ws</span>(<span class="hljs-string">'#'</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">' '</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">''</span>,<span class="hljs-string">'c'</span>);<span class="hljs-comment">-- a# #b##c</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat_ws</span>(<span class="hljs-string">'|'</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-literal">null</span>,<span class="hljs-string">'c'</span>,<span class="hljs-literal">null</span>,<span class="hljs-string">'d'</span>);<span class="hljs-comment">-- a|b|c|d</span></code></pre><h2 id="3-nvl：空值处理"><a href="#3-nvl：空值处理" class="headerlink" title="3. nvl：空值处理"></a>3. nvl：空值处理</h2><p><strong>nvl(T value, T default_value)</strong></p><p>如果value为null，返回default_value,否则返回value，开发中十分常用</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> nvl(<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>);<span class="hljs-comment">-- a</span><span class="hljs-keyword">select</span> nvl(<span class="hljs-literal">null</span>, <span class="hljs-string">'b'</span>);<span class="hljs-comment">-- b</span></code></pre><p><strong>补充</strong>：去除为null和空’’的值，select nvl(id,’’) != ‘’;</p><h2 id="4-datediff：计算两个日期的差值"><a href="#4-datediff：计算两个日期的差值" class="headerlink" title="4.  datediff：计算两个日期的差值"></a>4.  datediff：计算两个日期的差值</h2><p><strong>datediff(string enddate, string startdate)</strong></p><p>返回值类型：int</p><p>日期比较函数，返回结束日期减去开始日期的天数</p><pre><code class="hljs sql">hive&gt; select datediff('2020-02-02','2020-02-01');  --返回1</code></pre><p>注：传入时间的格式必须是YYYY-MM-DD形式</p><pre><code class="hljs sql">hive&gt; select datediff('20200202','20200201');  --返回null</code></pre><h2 id="5-cast：显式转换"><a href="#5-cast：显式转换" class="headerlink" title="5. cast：显式转换"></a>5. cast：显式转换</h2><p><strong>类型转换：</strong></p><ol><li>任何类型都可以自动转成一种范围更大的类型，TINYINT,SMALLINT,INT,BIGINT,FLOAT和STRING都可以隐式地转换成DOUBLE；</li><li>BOOLEAN类型不能转换为其他任何数据类型。</li><li>从小范围往大范围转，不能自动完成，需要加cast关键字做强制类型转换。</li><li>转换失败返回null。</li></ol><p><strong>常用：</strong></p><p>1.获取当前日期：</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">cast</span> (<span class="hljs-keyword">current_date</span> <span class="hljs-keyword">as</span> <span class="hljs-keyword">string</span>)； <span class="hljs-comment">-- 2020-06-12</span></code></pre><p>2.string转数字类型，参与运算</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span>  <span class="hljs-keyword">max</span>(<span class="hljs-keyword">cast</span>(code <span class="hljs-keyword">as</span> <span class="hljs-built_in">int</span>)); <span class="hljs-comment">-- code 是string类型的一串数字 例：110911893303 ，2000098777，</span><span class="hljs-comment">--为取到code的最大值，如果不强转成int，按string比较，会取到的最大值是2000098777</span></code></pre><h2 id="6-case-when-then-else-end"><a href="#6-case-when-then-else-end" class="headerlink" title="6. case when then else end"></a>6. case when then else end</h2><p><b>CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END<b/></p><p>解释：When a = true, returns b; when c = true, returns d; else returns e.</p><p>类似java中的switch case语句和scala中的模式匹配</p><p><strong>示例：</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> age <span class="hljs-keyword">from</span> student;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142231143.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age &lt; <span class="hljs-number">18</span> <span class="hljs-keyword">then</span> <span class="hljs-string">"少年"</span> <span class="hljs-keyword">when</span> age &lt; <span class="hljs-number">60</span> <span class="hljs-keyword">then</span> <span class="hljs-string">"成年人"</span>  <span class="hljs-keyword">else</span> <span class="hljs-string">"老年人"</span> <span class="hljs-keyword">end</span> <span class="hljs-keyword">from</span> student;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142313352.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3gxMjM0NV8=,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><h2 id="7-sum-case-when"><a href="#7-sum-case-when" class="headerlink" title="7. sum case when"></a>7. sum case when</h2><p><strong>对是否满足条件进行计数</strong></p><p>统计各个年龄段人的数量：</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age&lt; <span class="hljs-number">18</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span> teenager, <span class="hljs-keyword">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age&lt; <span class="hljs-number">60</span> <span class="hljs-keyword">and</span> age&gt;=<span class="hljs-number">18</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span> adult, <span class="hljs-keyword">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age&gt;= <span class="hljs-number">60</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span> elderly <span class="hljs-keyword">from</span> student;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142451709.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><h2 id="8-row-number-over"><a href="#8-row-number-over" class="headerlink" title="8. row_number() over()"></a>8. row_number() over()</h2><p><strong>row_number() over()可以给每个行数据根据分组排序形成一个序号</strong></p><p>示例：</p><p>取最近的一条订单号</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> test_row_number_over;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142740868.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">no</span>,paydate,rn <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-keyword">no</span>,paydate,row_number() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">distribute</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">no</span> <span class="hljs-keyword">sort</span> <span class="hljs-keyword">by</span> paydate <span class="hljs-keyword">desc</span> ) <span class="hljs-keyword">as</span> rn <span class="hljs-keyword">from</span> test_row_number_over) t ;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142826546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3gxMjM0NV8=,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">no</span>,paydate <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-keyword">no</span>,paydate,row_number() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">distribute</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">no</span> <span class="hljs-keyword">sort</span> <span class="hljs-keyword">by</span> paydate <span class="hljs-keyword">desc</span> ) <span class="hljs-keyword">as</span> rn <span class="hljs-keyword">from</span> test_row_number_over) t <span class="hljs-keyword">where</span> t.rn=<span class="hljs-number">1</span>;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142933642.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><h2 id="9-greatest：多列中取到最大值"><a href="#9-greatest：多列中取到最大值" class="headerlink" title="9. greatest：多列中取到最大值"></a>9. greatest：多列中取到最大值</h2><pre><code class="hljs sql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> test_greatest;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612143100732.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">greatest</span>(a,b,c) <span class="hljs-keyword">from</span> test_greatest; <span class="hljs-comment">-- 3 7</span></code></pre><p><strong>注：</strong> 如果含有string类型的列，返回null</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">greatest</span>(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-string">"a"</span>); <span class="hljs-comment">--null</span></code></pre><h2 id="10-collect-list：列转行"><a href="#10-collect-list：列转行" class="headerlink" title="10.  collect_list：列转行"></a>10.  collect_list：列转行</h2><p>Hive中collect相关的函数有collect_list和collect_set。</p><p>它们都是将分组中的某列转为一个数组返回，不同的是collect_list不去重而collect_set去重。</p><p>示例：</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> test_collect_list;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612143327465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3gxMjM0NV8=,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>1. collect_list 不去重</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">name</span>,collect_list(video) <span class="hljs-keyword">from</span> test_collect_list <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span>;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612143348422.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>2. collect_set 去重</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">name</span>,collect_set(video) <span class="hljs-keyword">from</span> test_collect_list <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span>;</code></pre><p><img src="https://img-blog.csdnimg.cn/202006121434126.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>3. concat_ws修改拼接格式</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">name</span>,<span class="hljs-keyword">concat_ws</span>(<span class="hljs-string">','</span>,collect_list(video)) <span class="hljs-keyword">from</span> test_collect_list <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span>;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612143438521.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>函数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Impala入门</title>
    <link href="/2020/06/16/impala%E5%85%A5%E9%97%A8/"/>
    <url>/2020/06/16/impala%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Impala简介"><a href="#1-Impala简介" class="headerlink" title="1. Impala简介"></a>1. Impala简介</h2><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h3><ol><li>Impala提供对存储在HDFS、Hbase或Amazon Simple Storage Service（S3）数据的高性能、低延迟的交互式SQL查询功能。</li><li>基于Hive，使用Hive的元数据，类似Hive的SQL，JDBC和用户操作界面。Impala和Hive的定位不同，Impala查询速度快，可以做为大数据查询的补充，Hive更适合做需要长时间运行的跑批操作。</li><li>Impala由Cloudera公司推出，2017年11份从Apache Incubator毕业，是CDH平台首选的PB级大数据实时查询分析引擎。</li></ol><h3 id="1-2-优缺点"><a href="#1-2-优缺点" class="headerlink" title="1.2 优缺点"></a>1.2 优缺点</h3><h4 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h4><ol><li>使用SQL查询，使用上手快。</li><li>兼容Hive，可以访问hive的metastore，对hive数据直接做数据分析。</li><li>基于内存运算，不需要把中间结果写入磁盘，省掉了大量的I/O开销。</li><li>无需转换为Mapreduce，直接访问存储在HDFS，HBase中的数据进行作业调度，速度快。</li><li>使用了支持Data locality的I/O调度机制，尽可能地将数据和计算分配在同一台机器上进行，减少了网络开销。</li><li>支持各种文件格式，如TEXTFILE 、SEQUENCEFILE 、RCFile、Parquet。</li></ol><h4 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h4><ol><li>对内存的依赖大，且完全依赖于hive。</li><li>实践中，分区超过1万，性能严重下降。</li><li>只能读取文本文件，而不能直接读取自定义二进制文件。</li><li>每当新的记录/文件被添加到HDFS中的数据目录时，该表需要被刷新。</li></ol><h3 id="1-3-Impala架构"><a href="#1-3-Impala架构" class="headerlink" title="1.3 Impala架构"></a>1.3 Impala架构</h3><p>Impala自身包含三个模块：Impalad、Statestore和Catalog，除此之外它还依赖Hive Metastore和HDFS。</p><h4 id="1-3-1-核心组件"><a href="#1-3-1-核心组件" class="headerlink" title="1.3.1 核心组件"></a>1.3.1 核心组件</h4><ol><li><p>Impalad：</p><ul><li>接收client的请求、Query执行并返回给中心协调节点</li><li>子节点上的守护进程，负责向statestore保持通信，汇报工作。</li><li>具体细节：<ul><li>query planner：翻译sql，生成计划</li><li>query coordinator：协调器，分配任务给query executor</li><li>query executor：执行查询任务</li></ul></li></ul></li><li><p>Catalog：</p><ul><li><p>分发表的元数据信息到各个impalad中；</p></li><li><p>接收来自statestore的所有请求。</p></li></ul></li><li><p>Statestore：</p><ul><li><p>负责收集分布在集群中各个impalad进程的资源信息、各节点健康状况，同步节点信息；</p></li><li><p>负责query的协调调度。</p></li></ul></li></ol><h4 id="1-3-2-执行流程"><a href="#1-3-2-执行流程" class="headerlink" title="1.3.2 执行流程"></a>1.3.2 执行流程</h4><p><img src="C:%5CUsers%5C16336%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200616172905838.png" srcset="/img/loading.gif" alt="image-20200616172905838"></p><ol><li>由Client发送一个执行SQL到任意一台Impalad的Query Planner</li><li>由Query Planner 把SQL发向Query Coordinator</li><li>由Query Coordinator 来调度分配任务到Impalad的所有节点</li><li>各个Impalad节点的Query Executor 进行执行SQL工作</li><li>执行SQL结束以后，将结果返回给Query Coordinator</li><li>再由Query Coordinator 将结果返回给Client</li></ol><h2 id="2-操作命令"><a href="#2-操作命令" class="headerlink" title="2. 操作命令"></a>2. 操作命令</h2><h3 id="2-1-外部shell命令"><a href="#2-1-外部shell命令" class="headerlink" title="2.1 外部shell命令"></a>2.1 外部shell命令</h3><table><thead><tr><th align="left">选项</th><th>描述</th></tr></thead><tbody><tr><td align="left">-h,  –help</td><td>显示帮助信息</td></tr><tr><td align="left">-v  or –version</td><td>显示版本信息</td></tr><tr><td align="left">-i  hostname, –impalad=hostname</td><td>指定连接运行  impalad 守护进程的主机。默认端口是 21000。</td></tr><tr><td align="left">-q query, –query=query</td><td>从命令行中传递一个shell  命令。执行完这一语句后 shell 会立即退出。</td></tr><tr><td align="left">-f query_file, –query_file= query_file</td><td>传递一个文件中的 SQL  查询。文件内容必须以分号分隔</td></tr><tr><td align="left">-o filename or –output_file filename</td><td>保存所有查询结果到指定的文件。通常用于保存在命令行使用 -q 选项执行单个查询时的查询结果。</td></tr><tr><td align="left">-c</td><td>查询执行失败时继续执行</td></tr><tr><td align="left">-d default_db or  –database=default_db</td><td>指定启动后使用的数据库，与建立连接后使用use语句选择数据库作用相同，如果没有指定，那么使用default数据库</td></tr><tr><td align="left">-r or –refresh_after_connect</td><td>建立连接后刷新  Impala 元数据</td></tr><tr><td align="left">-p, –show_profiles</td><td>对 shell 中执行的每一个查询，显示其查询执行计划</td></tr><tr><td align="left">-B（–delimited）</td><td>去格式化输出</td></tr><tr><td align="left">–output_delimiter=character</td><td>指定分隔符</td></tr><tr><td align="left">–print_header</td><td>打印列名</td></tr></tbody></table><h3 id="2-2-内部shell命令"><a href="#2-2-内部shell命令" class="headerlink" title="2.2 内部shell命令"></a>2.2 内部shell命令</h3><table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>help</td><td>显示帮助信息</td></tr><tr><td>explain  <sql></td><td>显示执行计划</td></tr><tr><td>profile</td><td>(查询完成后执行） 查询最近一次查询的底层信息</td></tr><tr><td>shell <shell></td><td>不退出impala-shell执行shell命令</td></tr><tr><td>version</td><td>显示版本信息（同于impala-shell  -v）</td></tr><tr><td>connect</td><td>连接impalad主机，默认端口21000（同于impala-shell -i）</td></tr><tr><td>refresh <tablename></td><td>增量刷新元数据库</td></tr><tr><td>invalidate metadata</td><td>全量刷新元数据库（慎用）（同于 impala-shell -r）</td></tr><tr><td>history</td><td>历史命令</td></tr></tbody></table><h2 id="3-数据类型"><a href="#3-数据类型" class="headerlink" title="3. 数据类型"></a>3. 数据类型</h2><table><thead><tr><th>Hive数据类型</th><th>Impala数据类型</th><th>长度</th></tr></thead><tbody><tr><td>TINYINT</td><td>TINYINT</td><td>1byte有符号整数</td></tr><tr><td>SMALINT</td><td>SMALINT</td><td>2byte有符号整数</td></tr><tr><td>INT</td><td>INT</td><td>4byte有符号整数</td></tr><tr><td>BIGINT</td><td>BIGINT</td><td>8byte有符号整数</td></tr><tr><td>BOOLEAN</td><td>BOOLEAN</td><td>布尔类型，true或者false</td></tr><tr><td>FLOAT</td><td>FLOAT</td><td>单精度浮点数</td></tr><tr><td>DOUBLE</td><td>DOUBLE</td><td>双精度浮点数</td></tr><tr><td>STRING</td><td>STRING</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td></tr><tr><td>TIMESTAMP</td><td>TIMESTAMP</td><td>时间类型</td></tr><tr><td>BINARY</td><td>不支持</td><td>字节数组</td></tr></tbody></table><p>注意：Impala虽然支持array，map，struct复杂数据类型，但是支持并不完全，一般处理方法，将复杂类型转化为基本类型，通过hive创建表。</p><h2 id="4-DDL操作"><a href="#4-DDL操作" class="headerlink" title="4. DDL操作"></a>4. DDL操作</h2><h2 id="5-DML操作"><a href="#5-DML操作" class="headerlink" title="5. DML操作"></a>5. DML操作</h2>]]></content>
    
    
    <categories>
      
      <category>Impala</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Impala</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive Create Table</title>
    <link href="/2020/06/12/Hive_CreateDropTruncate_Table/"/>
    <url>/2020/06/12/Hive_CreateDropTruncate_Table/</url>
    
    <content type="html"><![CDATA[<h3 id="Create-Table"><a href="#Create-Table" class="headerlink" title="Create Table"></a>Create Table</h3><pre><code class="hljs mysql">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])]  [COMMENT table_comment]  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)     [STORED AS DIRECTORIES]  [   [ROW FORMAT row_format]    [STORED AS file_format]     | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)  ]  [LOCATION hdfs_path]  [TBLPROPERTIES (property_name&#x3D;property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</code></pre><ol><li><p>CREATE TABLE 创建一个指定名字的表，如果一个同名的表或者视图已经存在，则会抛出异常，加上IF NOT EXISTS时，如果表已经存在，Hive就会忽略掉后面的执行语句，而且不会有任何提示。</p><ul><li><p>IF NOT EXISTS只比较表名，不比较表的模式。</p><blockquote><p>如果用户使用了IF NOT EXISTS，当用户所指定的表的模式和已经存在的这个表的模式不同的话，Hive不会为此做出提示。如果用户的意图是使这个表具有重新指定的那个新的模式的话，那么就需要先删除这个表，也就是丢弃之前的数据，然后再重建这张表。用户可以考虑使用一个或多个ALTER TABLE语句来修改已经存在的表的结构。</p></blockquote></li></ul></li><li><p>表名和列名不区分大小写，但SerDe和属性名区分大小写。</p><ol><li>在Hive 0.12和更早版本中，表和列名称中仅允许使用字母数字和下划线。在Hive 0.13之后，列名可以使用任何Unicode字符，但是点(.)和冒号(:)在查询时候会报错，所以在Hive 1.2.0中不允许使用它们</li><li>反引号（``）中指定的任何列名均按字面意义处理。在反引号字符串中，使用双反引号（```）表示反引号字符。反引号还可以将保留关键字用于表和列标识符。</li><li>如果要限制列名只能为字母数字和下划线字符，可以将将配置属性设置<code>hive.support.quoted.identifiers</code>为<code>none</code>。在此配置中，带反引号的名称被解释为正则表达式。</li></ol></li><li><p>不加EXTERNAL创建的表是管理表，hive管理自己的数据，要确认一个表是管理表还是外部表，请在DESCRIBE EXTENDED table_name的输出中查找tableType 。</p></li><li><p>TBLPROPERTIES可以用键值对形式，给表添加额外的一些说明。</p><p>常见用法：</p><ul><li><p>指定压缩格式：TBLPROPERTIES（“ orc.compress” =“ ZLIB”）或（“ orc.compress” =“ SNAPPY”）或（“ orc.compress” =“ NONE”</p><blockquote><p>Hive会自动增加两个表属性：一个是last_modified_by，其保存着最后修改这个表的用户的用户名；另一个是last_modified_time，其保存着最后一次修改的新纪元时间秒。</p></blockquote></li><li><p>查看TBLPROPERTIES属性：show TBLPROPERTIES table_name；</p></li></ul></li><li><p>查看表的详细信息</p><ul><li>desc  [extended|formatted]  table_name;</li><li>formatted打印的表信息比extended的更加详细，而且阅读性更强。</li></ul></li><li><p>查看某一列的详细信息</p><ul><li>desc  [extended|formatted]  table_name.column_name;</li></ul></li></ol><h2 id="Managed-and-External-Tables"><a href="#Managed-and-External-Tables" class="headerlink" title="Managed and External Tables"></a>Managed and External Tables</h2><p>hive的表从本质上分两种类型：</p><ul><li>管理表</li><li>外部表</li></ul><h3 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h3><ol><li>默认创建的表，或者是Managed修饰的表是管理表，元数据，文件，统计信息等由Hive自己管理，即Hive控制着数据的生命周期，当我们删除一个管理表时候，表中的数据也会被删除。</li><li>管理表的数据存储在属性<code>hive.metastore.warehouse.dir</code>指定的路径下，默认情况下存储在类似/user/hive/warehouse/databasename.db/tablename/的路径下。在创建表的时候，如果指定location的位置，就可以覆盖默认位置。</li></ol><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><ol><li>外部表描述外部文件的元数据信息，外部表的文件可以由Hive之外的工具管理，外部表可以访问存储在例如Azure存储卷（ASV）或远程HDFS位置的源中的数据。如果更改了外部表的结构或分区，则可以使用MSCK REPAIR TABLE table_name语句刷新元数据信息。</li><li>删除外部表不会删除表中的数据，只会删除掉描述表的元数据信息。</li></ol><h3 id="区别与应用"><a href="#区别与应用" class="headerlink" title="区别与应用"></a>区别与应用</h3><ol><li>管理表拥有对表中数据的管理权，而外部表没有，删除管理表，元数据信息和表中的数据都会被删除，删除外部表只删除元数据信息，表中的数据不会被删除。</li><li>需要Hive需要管理表的生命周期或生成临时表时候，使用管理表，管理表不适合和其他工具共享数据。</li><li>当表中的文件已经存在时候，使用外部表，即使表被删除，文件也会保留。</li></ol><h3 id="管理表和外部表转换"><a href="#管理表和外部表转换" class="headerlink" title="管理表和外部表转换"></a>管理表和外部表转换</h3><ol><li><p>内部表转外部表</p><p>alter table table_name set tblproperties(‘EXTERNAL’=’TRUE’);</p></li><li><p>外部表转内部表</p><p>alter table table_name set tblproperties(‘EXTERNAL’=’FALSE’);</p></li><li><p>查看是外部表还是管理表</p><pre><code class="hljs gams">hive&gt; desc formatted table_name;<span class="hljs-keyword">Table</span> Type:       MANAGED_TABLE    //管理表<span class="hljs-keyword">Table</span> Type:       EXTERNAL_TABLE   //外部表</code></pre></li></ol><p><strong>注：</strong>(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写</p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/06/12/hello-world/"/>
    <url>/2020/06/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="hljs bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="hljs bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="hljs bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
