<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>spark简介</title>
    <link href="/2020/07/17/spark%E7%AE%80%E4%BB%8B/"/>
    <url>/2020/07/17/spark%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark系列（一）-Spark入门"><a href="#Spark系列（一）-Spark入门" class="headerlink" title="Spark系列（一） Spark入门"></a>Spark系列（一） Spark入门</h1><h2 id="1-Spark是什么？"><a href="#1-Spark是什么？" class="headerlink" title="1. Spark是什么？"></a>1. Spark是什么？</h2><h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>Apache Spark is a unified analytics engine for large-scale data processing.（Apache Spark是用于大规模数据处理的统一分析引擎。）</p><h3 id="1-2-介绍"><a href="#1-2-介绍" class="headerlink" title="1.2 介绍"></a>1.2 介绍</h3><p>Spark支持多种常用的编程语言（Python，Java，Scala和R），提供支持SQL、流处理、机器学习等多种任务的软件库，能单机运行，也可以集群运行。</p><p>spark提供的组件的软件库如下：</p><p><img src="https://s1.ax1x.com/2020/07/07/UknygH.png" srcset="/img/loading.gif" alt=""></p><h3 id="1-3-特点"><a href="#1-3-特点" class="headerlink" title="1.3 特点"></a>1.3 特点</h3><ol><li>相比较MapReduce，基于内存分析快速</li><li>使用简单，提供多种API</li><li>提供统一解决方案，sparkSQL，Spark Streaming，Spark MLlibdeng</li></ol><h2 id="2-RDD"><a href="#2-RDD" class="headerlink" title="2. RDD"></a>2. RDD</h2><p>spark的主要抽象称为弹性分布式数据集（RDD），可以通过读取文件创建RDD。</p><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> textFile = sc.textFile(<span class="hljs-string">"README.md"</span>)textFile: org.apache.spark.rdd.<span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = <span class="hljs-type">README</span>.md <span class="hljs-type">MapPartitionsRDD</span>[<span class="hljs-number">1</span>] at textFile at &lt;console&gt;:<span class="hljs-number">25</span></code></pre><p>RDD具有两种算子，行动算子（有返回值）和转换算子（返回指向新RDD的指针）</p><p><strong>行动算子：</strong></p><pre><code class="hljs scala">scala&gt; textFile.count() <span class="hljs-comment">// Number of items in this RDD</span>res0: <span class="hljs-type">Long</span> = <span class="hljs-number">126</span> <span class="hljs-comment">// May be different from yours as README.md will change over time, similar to other outputs</span>scala&gt; textFile.first() <span class="hljs-comment">// First item in this RDD</span>res1: <span class="hljs-type">String</span> = # <span class="hljs-type">Apache</span> <span class="hljs-type">Spark</span></code></pre><p><strong>转换算子：</strong></p><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> linesWithSpark = textFile.filter(line =&gt; line.contains(<span class="hljs-string">"Spark"</span>))linesWithSpark: org.apache.spark.rdd.<span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = <span class="hljs-type">MapPartitionsRDD</span>[<span class="hljs-number">2</span>] at filter at &lt;console&gt;:<span class="hljs-number">27</span></code></pre><p>两种算子同时使用：</p><pre><code class="hljs scala">scala&gt; textFile.filter(line =&gt; line.contains(<span class="hljs-string">"Spark"</span>)).count() <span class="hljs-comment">// How many lines contain "Spark"?</span>res3: <span class="hljs-type">Long</span> = <span class="hljs-number">15</span></code></pre><p>Spark还支持将数据集提取到群集范围的内存中缓存中，当重复访问数据时，例如查询小的“热”数据集或运行迭代算法（如PageRank）时，这非常有用。</p><pre><code class="hljs scala">scala&gt; linesWithSpark.cache()res7: linesWithSpark<span class="hljs-class">.<span class="hljs-keyword">type</span> </span>= <span class="hljs-type">MapPartitionsRDD</span>[<span class="hljs-number">2</span>] at filter at &lt;console&gt;:<span class="hljs-number">27</span>scala&gt; linesWithSpark.count()res8: <span class="hljs-type">Long</span> = <span class="hljs-number">15</span>scala&gt; linesWithSpark.count()res9: <span class="hljs-type">Long</span> = <span class="hljs-number">15</span></code></pre><h2 id="3-Wordcount例子"><a href="#3-Wordcount例子" class="headerlink" title="3. Wordcount例子"></a>3. Wordcount例子</h2><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Wordcount</span></span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;      <span class="hljs-comment">// 创建saprkcontext</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"quickStart"</span>).setMaster(<span class="hljs-string">"local"</span>)    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)        <span class="hljs-comment">// 读取文件</span>    <span class="hljs-keyword">val</span> logData = sc.textFile(<span class="hljs-string">"D:\\testfile"</span>,<span class="hljs-number">2</span>).cache()        <span class="hljs-comment">// 计算包含a，b的行数</span>    <span class="hljs-keyword">val</span> aNum = logData.filter(_.contains(<span class="hljs-string">"a"</span>)).count()    <span class="hljs-keyword">val</span> bNum = logData.filter(_.contains(<span class="hljs-string">"b"</span>)).count()    println(<span class="hljs-string">s"a=<span class="hljs-subst">$aNum</span> b=<span class="hljs-subst">$bNum</span>"</span>)    <span class="hljs-comment">// 包含最多单词的行的单词数</span>    <span class="hljs-keyword">val</span> bigline = logData.map(_.split(<span class="hljs-string">" "</span>).size)    .reduce((a, b) =&gt; <span class="hljs-keyword">if</span> (a &gt; b) a <span class="hljs-keyword">else</span> b)    println(bigline)    <span class="hljs-comment">// wordcount</span>    <span class="hljs-keyword">val</span> wc = logData.flatMap(line =&gt; line.split(<span class="hljs-string">" "</span>))                    .map(word =&gt; (word,<span class="hljs-number">1</span>))                    .reduceByKey((a,b)=&gt; a+b)    wc.foreach(println)    sc.stop()  &#125;&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>spark编程指南</title>
    <link href="/2020/07/17/spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
    <url>/2020/07/17/spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark系列（二）——Spark编程指南"><a href="#Spark系列（二）——Spark编程指南" class="headerlink" title="Spark系列（二）——Spark编程指南"></a>Spark系列（二）——Spark编程指南</h1><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><blockquote><p>At a high level, every Spark application consists of a <em>driver program</em> that runs the user’s <code>main</code> function and executes various <em>parallel operations</em> on a cluster. The main abstraction Spark provides is a <em>resilient distributed dataset</em> (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to <em>persist</em> an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.</p><p>A second abstraction in Spark is <em>shared variables</em> that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: <em>broadcast variables</em>, which can be used to cache a value in memory on all nodes, and <em>accumulators</em>, which are variables that are only “added” to, such as counters and sums.</p><p>This guide shows each of these features in each of Spark’s supported languages. It is easiest to follow along with if you launch Spark’s interactive shell – either <code>bin/spark-shell</code> for the Scala shell or <code>bin/pyspark</code> for the Python one.</p><p>在较高级别上，每个Spark应用程序都包含一个<em>驱动程序</em>，该<em>程序</em>运行用户的<code>main</code>功能并在集群上执行各种<em>并行操作</em>。Spark提供的主要抽象是<em>弹性分布式数据集</em>（RDD），它是跨集群节点划分的元素的集合，可以并行操作。通过从Hadoop文件系统（或任何其他Hadoop支持的文件系统）中的文件或驱动程序中现有的Scala集合开始并进行转换来创建RDD。用户还可以要求Spark将RDD <em>持久</em>存储在内存中，从而使其能够在并行操作中高效地重用。最后，RDD会自动从节点故障中恢复。</p><p>Spark中的第二个抽象是可以在并行操作中使用的<em>共享变量</em>。默认情况下，当Spark作为一组任务在不同节点上并行运行一个函数时，它会将函数中使用的每个变量的副本传送给每个任务。有时，需要在任务之间或任务与驱动程序之间共享变量。Spark支持两种类型的共享变量：<em>广播变量</em>（可用于在所有节点上的内存中缓存值）和<em>累加器（accumulator）</em>，这些变量仅被“添加”到其上，例如计数器和总和。</p></blockquote><h2 id="2-Linking-with-Spark"><a href="#2-Linking-with-Spark" class="headerlink" title="2. Linking with Spark"></a>2. Linking with Spark</h2><ol><li><p>spark的版本与scala的版本要兼容，可以在maven依赖项中体现</p><pre><code class="hljs angelscript">groupId = org.apache.sparkartifactId = spark-core_2<span class="hljs-number">.11</span> (spark版本<span class="hljs-number">2.1</span><span class="hljs-number">.2</span>，scala版本<span class="hljs-number">2.11</span>)version = <span class="hljs-number">2.1</span><span class="hljs-number">.2</span></code></pre></li><li><p>spark和scala的兼容情况</p><ul><li>spark1.2.x 到 spark2.2.x 兼容 scala2.10，scala2.11；</li><li>spark2.3.x 兼容 scala2.11；</li><li>spark2.4.x 兼容 scala2.11，scala2.12；</li><li>spark3.0.0 兼容 scala2.12</li></ul></li></ol><h2 id="3-Initializing-Spark"><a href="#3-Initializing-Spark" class="headerlink" title="3. Initializing Spark"></a>3. Initializing Spark</h2><p>创建spark程序的第一步需要先创建SparkContext对象，SparkContext告诉Spark如何访问集群。创建SparkContext需要先创建SparkConf对象，SparkConf对象包含一些描述信息</p><pre><code class="hljs scala"><span class="hljs-comment">// 创建saprkcontext</span><span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"quickStart"</span>).setMaster(<span class="hljs-string">"local"</span>)<span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)</code></pre><p>每个JVM只能激活一个SparkContext，启动新的SparkContext之前需要先stop()开启的SparkConte</p><h2 id="4-Using-the-Shell"><a href="#4-Using-the-Shell" class="headerlink" title="4. Using the Shell"></a>4. Using the Shell</h2><ol><li>在shell中，spark提供SparkContext sc, 创建自己的SparkContext不生效。</li><li>可以指定参数<ul><li>–master 设置运行模式</li><li>–jars 指定jar包</li><li>–packages 添加maven依赖（？）</li></ul></li></ol><h2 id="5-Resilient-Distributed-Datasets-RDDs"><a href="#5-Resilient-Distributed-Datasets-RDDs" class="headerlink" title="5. Resilient Distributed Datasets (RDDs)"></a>5. Resilient Distributed Datasets (RDDs)</h2><p>弹性分布式数据集（RDD）：可并行操作的容错数据集</p><p>创建方式：</p><ol><li><em>parallelizing</em> 一个存在的集合</li><li>引用外部存储系统中的数据集，hdfs，hbase等</li></ol><h3 id="5-1-Parallelized-Collections"><a href="#5-1-Parallelized-Collections" class="headerlink" title="5.1 Parallelized Collections"></a>5.1 Parallelized Collections</h3><p>Parallelized collections are created by calling <code>SparkContext</code>’s <code>parallelize</code> method on an existing collection in your driver program (a Scala <code>Seq</code>)，复制集合中的元素形成可并行操作分布式数据集。</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> data = <span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<span class="hljs-keyword">val</span> distData = sc.parallelize(data)</code></pre><p>One important parameter for parallel collections is the number of <em>partitions</em> to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to <code>parallelize</code> (e.g. <code>sc.parallelize(data, 10)</code>). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.</p><h3 id="5-2-External-Datasets"><a href="#5-2-External-Datasets" class="headerlink" title="5.2 External Datasets"></a>5.2 External Datasets</h3><p>Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, <a href="http://wiki.apache.org/hadoop/AmazonS3" target="_blank" rel="noopener">Amazon S3</a>, etc. Spark supports text files, <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank" rel="noopener">SequenceFiles</a>, and any other Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html" target="_blank" rel="noopener">InputFormat</a>.Spark可以从hadoop支持的任何存储源创建分布式数据集。</p><p>Text file RDDs can be created using <code>SparkContext</code>’s <code>textFile</code> method.这个方法需要提供文件URI </p><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> distFile = sc.textFile(<span class="hljs-string">"data.txt"</span>)distFile: org.apache.spark.rdd.<span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = data.txt <span class="hljs-type">MapPartitionsRDD</span>[<span class="hljs-number">10</span>] at textFile at &lt;console&gt;:<span class="hljs-number">26</span></code></pre><p>Some notes on reading files with Spark:</p><ul><li>If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.<ul><li>本地文件系统，注意别的工作节点是否能访问到。</li></ul></li><li>All of Spark’s file-based input methods, including <code>textFile</code>, support running on directories, compressed files, and wildcards as well. For example, you can use <code>textFile(&quot;/my/directory&quot;)</code>, <code>textFile(&quot;/my/directory/*.txt&quot;)</code>, and <code>textFile(&quot;/my/directory/*.gz&quot;)</code>.<ul><li>支持读文件夹，压缩格式，和通配符选择文件</li></ul></li><li>The <code>textFile</code> method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.<ul><li>可以传入第二个参数，手动设置分区个数，默认情况下，文件的一个块创建一个分区，可以传入一个更大的值，但是分区数不能小于文件块的数量。</li></ul></li></ul><p>Apart from text files, Spark’s Scala API also supports several other data formats:</p><ul><li><code>SparkContext.wholeTextFiles</code> lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with <code>textFile</code>, which would return one record per line in each file.<ul><li><code>SparkContext.wholeTextFiles</code>合并小文件，每一行书文件名和文件内容，类似hadoop中的tar文件</li></ul></li><li>For <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank" rel="noopener">SequenceFiles</a>, use SparkContext’s <code>sequenceFile[K, V]</code> method where <code>K</code> and <code>V</code> are the types of key and values in the file. These should be subclasses of Hadoop’s <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html" target="_blank" rel="noopener">Writable</a> interface, like <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/IntWritable.html" target="_blank" rel="noopener">IntWritable</a> and <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Text.html" target="_blank" rel="noopener">Text</a>. In addition, Spark allows you to specify native types for a few common Writables; for example, <code>sequenceFile[Int, String]</code> will automatically read IntWritables and Texts.<ul><li><code>sequenceFile[K, V]</code>操作<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank" rel="noopener">SequenceFiles</a>文件，这是Hadoop的<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html" target="_blank" rel="noopener">Writable</a>接口的子类，例如<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/IntWritable.html" target="_blank" rel="noopener">IntWritable</a>和<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Text.html" target="_blank" rel="noopener">Text</a>。没用过，不太理解。</li></ul></li><li><h2 id="For-other-Hadoop-InputFormats-you-can-use-the-SparkContext-hadoopRDD-method-which-takes-an-arbitrary-JobConf-and-input-format-class-key-class-and-value-class-Set-these-the-same-way-you-would-for-a-Hadoop-job-with-your-input-source-You-can-also-use-SparkContext-newAPIHadoopRDD-for-InputFormats-based-on-the-“new”-MapReduce-API-org-apache-hadoop-mapreduce"><a href="#For-other-Hadoop-InputFormats-you-can-use-the-SparkContext-hadoopRDD-method-which-takes-an-arbitrary-JobConf-and-input-format-class-key-class-and-value-class-Set-these-the-same-way-you-would-for-a-Hadoop-job-with-your-input-source-You-can-also-use-SparkContext-newAPIHadoopRDD-for-InputFormats-based-on-the-“new”-MapReduce-API-org-apache-hadoop-mapreduce" class="headerlink" title="For other Hadoop InputFormats, you can use the SparkContext.hadoopRDD method, which takes an arbitrary JobConf and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use SparkContext.newAPIHadoopRDD for InputFormats based on the “new” MapReduce API (org.apache.hadoop.mapreduce)."></a>For other Hadoop InputFormats, you can use the <code>SparkContext.hadoopRDD</code> method, which takes an arbitrary <code>JobConf</code> and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use <code>SparkContext.newAPIHadoopRDD</code> for InputFormats based on the “new” MapReduce API (<code>org.apache.hadoop.mapreduce</code>).</h2></li><li><code>RDD.saveAsObjectFile</code> and <code>SparkContext.objectFile</code> support saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.</li></ul><h3 id="5-3-RDD-Operations"><a href="#5-3-RDD-Operations" class="headerlink" title="5.3 RDD Operations"></a>5.3 RDD Operations</h3><p>RDDs support two types of operations: <em>transformations</em>, which create a new dataset from an existing one, and <em>actions</em>, which return a value to the driver program after running a computation on the dataset. For example, <code>map</code> is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, <code>reduce</code> is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel <code>reduceByKey</code> that returns a distributed dataset). 为什么 <code>reduce(_+_)</code>会执行累加？</p><ul><li>RDD提供两种操作<ul><li>转换：根据一个现有的RDD创建一个新的RDD</li><li>行动：对数据集计算之后返回给driver一个值</li></ul></li></ul><p>All transformations in Spark are <em>lazy</em>, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through <code>map</code> will be used in a <code>reduce</code> and return only the result of the <code>reduce</code> to the driver, rather than the larger mapped dataset.</p><ul><li>spark的转换是惰性的，只有driver需要action返回数据时候，才会触发transformmations进行计算。</li></ul><p>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also <em>persist</em> an RDD in memory using the <code>persist</code> (or <code>cache</code>) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.</p><ul><li>默认情况下每个action会去触发transform RDD，可以把转换的RDD缓存起来，用persist，cache缓存在内存中。也可以持久化到磁盘中。</li></ul><h3 id="5-4-Basics"><a href="#5-4-Basics" class="headerlink" title="5.4 Basics"></a>5.4 Basics</h3><pre><code class="hljs scala"><span class="hljs-keyword">val</span> lines = sc.textFile(<span class="hljs-string">"data.txt"</span>)<span class="hljs-keyword">val</span> lineLengths = lines.map(s =&gt; s.length)<span class="hljs-keyword">val</span> totalLength = lineLengths.reduce((a, b) =&gt; a + b)</code></pre><p>The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: <code>lines</code> is merely a pointer to the file. The second line defines <code>lineLengths</code> as the result of a <code>map</code> transformation. Again, <code>lineLengths</code> is <em>not</em> immediately computed, due to laziness. Finally, we run <code>reduce</code>, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program.。此时，Spark将计算分解为任务，以在不同的机器上运行，每台机器都运行其映射的一部分和局部归约，仅将其答案返回给驱动程序。</p><p>If we also wanted to use <code>lineLengths</code> again later, we could add:</p><pre><code class="hljs scala">lineLengths.persist()</code></pre><p>before the <code>reduce</code>, which would cause <code>lineLengths</code> to be saved in memory after the first time it is computed.</p><h3 id="5-5-Passing-Functions-to-Spark"><a href="#5-5-Passing-Functions-to-Spark" class="headerlink" title="5.5 Passing Functions to Spark"></a>5.5 Passing Functions to Spark</h3><p>Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are two recommended ways to do this:  Spark的API在很大程度上依赖于在驱动程序中传递函数以在集群上运行</p><ul><li><p><a href="http://docs.scala-lang.org/tutorials/tour/anonymous-function-syntax.html" target="_blank" rel="noopener">Anonymous function syntax</a>, which can be used for short pieces of code.</p></li><li><p>Static methods in a global singleton object. For example, you can define <code>object MyFunctions</code> and then pass <code>MyFunctions.func1</code>, as follows:</p><ul><li><p>伴生对象中的静态方法，调用是半生对象.方法名</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">MyFunctions</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func1</span></span>(s: <span class="hljs-type">String</span>): <span class="hljs-type">String</span> = &#123; ... &#125;&#125;myRdd.map(<span class="hljs-type">MyFunctions</span>.func1)</code></pre></li></ul></li><li><p>Note that while it is also possible to pass a reference to a method in a class instance (as opposed to a singleton object), this requires sending the object that contains that class along with the method. For example, consider:</p><ul><li>类中定义方法，传递时候需要类的对象和方法一起传递</li></ul><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyClass</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func1</span></span>(s: <span class="hljs-type">String</span>): <span class="hljs-type">String</span> = &#123; ... &#125;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doStuff</span></span>(rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = &#123; rdd.map(func1) &#125;&#125;</code></pre></li></ul><p>Here, if we create a new <code>MyClass</code> instance and call <code>doStuff</code> on it, the <code>map</code> inside there references the <code>func1</code> method <em>of that <code>MyClass</code> instance</em>, so the whole object needs to be sent to the cluster. It is similar to writing <code>rdd.map(x =&gt; this.func1(x))</code>.</p><p>在这里，如果我们创建一个新的<code>MyClass</code>实例，并调用<code>doStuff</code>就可以了，<code>map</code>里面有引用的 <code>func1</code>方法<em>是的<code>MyClass</code>实例</em>，所以整个对象需要被发送到群集。它类似于写作<code>rdd.map(x =&gt; this.func1(x))</code>。</p><p><strong>* * * * * 不理解</strong></p><p>In a similar way, accessing fields of the outer object will reference the whole object:</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyClass</span> </span>&#123;  <span class="hljs-keyword">val</span> field = <span class="hljs-string">"Hello"</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doStuff</span></span>(rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = &#123; rdd.map(x =&gt; field + x) &#125;&#125;</code></pre><p>is equivalent to writing <code>rdd.map(x =&gt; this.field + x)</code>, which references all of <code>this</code>. To avoid this issue, the simplest way is to copy <code>field</code> into a local variable instead of accessing it externally:</p><pre><code class="hljs scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doStuff</span></span>(rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = &#123;  <span class="hljs-keyword">val</span> field_ = <span class="hljs-keyword">this</span>.field  rdd.map(x =&gt; field_ + x)&#125;</code></pre><h3 id="5-6-Understanding-closures"><a href="#5-6-Understanding-closures" class="headerlink" title="5.6 Understanding closures"></a>5.6 Understanding closures</h3><p>One of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses <code>foreach()</code> to increment a counter, but similar issues can occur for other operations as well.</p><p>跨集群执行代码时变量和方法的作用范围和生命周期，修改超出其范围的变量的RDD操作可能经常引起混乱</p><h4 id="5-6-1-Example"><a href="#5-6-1-Example" class="headerlink" title="5.6.1 Example"></a>5.6.1 Example</h4><p>Consider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in <code>local</code> mode (<code>--master = local[n]</code>) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):</p><pre><code class="hljs scala"><span class="hljs-keyword">var</span> counter = <span class="hljs-number">0</span><span class="hljs-keyword">var</span> rdd = sc.parallelize(data)<span class="hljs-comment">// Wrong: Don't do this!!</span>rdd.foreach(x =&gt; counter += x)println(<span class="hljs-string">"Counter value: "</span> + counter)</code></pre><p>没有报错，为什么累加之后counter还为0</p><h4 id="5-6-2-Local-vs-cluster-modes"><a href="#5-6-2-Local-vs-cluster-modes" class="headerlink" title="5.6.2 Local vs. cluster modes"></a>5.6.2 Local vs. cluster modes</h4><p>The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s <strong>closure</strong>. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case <code>foreach()</code>). This closure is serialized and sent to each executor.</p><p>上面的代码的行为是未定义的，可能无法按预期工作。为了执行作业，Spark将RDD操作的处理分解为任务，每个任务都由执行程序执行。在执行之前，Spark计算任务的<strong>闭包</strong>。闭包是执行者在RDD上执行其计算时必须可见的那些变量和方法（在本例中为<code>foreach()</code>）。此闭包被序列化并发送给每个执行器。</p><p>The variables within the closure sent to each executor are now copies and thus, when <strong>counter</strong> is referenced within the <code>foreach</code> function, it’s no longer the <strong>counter</strong> on the driver node. There is still a <strong>counter</strong> in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of <strong>counter</strong> will still be zero since all operations on <strong>counter</strong> were referencing the value within the serialized closure.</p><p>发送给每个执行程序的闭包中的变量现在是副本，因此，在函数中引用<strong>计数器</strong>时<code>foreach</code>，它不再是驱动程序节点上的<strong>计数器</strong>。驱动程序节点的内存中仍然存在一个<strong>计数器</strong>，但是执行者将不再看到该<strong>计数器</strong>！执行者仅从序列化闭包中看到副本。因此，由于对<strong>计数器的</strong>所有操作都引用了序列化闭包内的值，所以<strong>counter</strong>的最终值仍将为零。（解释了为什么counter为0 ，因为executor中也有一个counter副本，在这个副本上累加的，打印的是driver端的counter，driver端的counter值为0）</p><p>In local mode, in some circumstances the <code>foreach</code> function will actually execute within the same JVM as the driver and will reference the same original <strong>counter</strong>, and may actually update it.</p><p>在本地模式下，在某些情况下，该<code>foreach</code>函数实际上将在与驱动程序相同的JVM中执行，并且将引用相同的原始<strong>计数器</strong>，并且实际上可能会对其进行更新。(怎样知道driver和executor在一个JVM中执行)</p><p>To ensure well-defined behavior in these sorts of scenarios one should use an <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#accumulators" target="_blank" rel="noopener"><code>Accumulator</code></a>. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.</p><p>In general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.</p><p>为确保在此类情况下行为明确，应使用<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#accumulators" target="_blank" rel="noopener"><code>Accumulator</code></a>。Spark中的累加器专门用于提供一种机制，用于在集群中的各个工作节点之间拆分执行时安全地更新变量。本指南的“累加器”部分将详细讨论这些内容。</p><p>通常，闭包-诸如循环或局部定义的方法之类的构造，不应用于突变某些全局状态。Spark不定义或保证从闭包外部引用的对象的突变行为。某些执行此操作的代码可能会在本地模式下工作，但这只是偶然的情况，此类代码在分布式模式下将无法按预期运行。如果需要一些全局聚合，请使用累加器。</p><h4 id="5-6-3-Printing-elements-of-an-RDD"><a href="#5-6-3-Printing-elements-of-an-RDD" class="headerlink" title="5.6.3 Printing elements of an RDD"></a>5.6.3 Printing elements of an RDD</h4><p>Another common idiom is attempting to print out the elements of an RDD using <code>rdd.foreach(println)</code> or <code>rdd.map(println)</code>. On a single machine, this will generate the expected output and print all the RDD’s elements. However, in <code>cluster</code> mode, the output to <code>stdout</code> being called by the executors is now writing to the executor’s <code>stdout</code> instead, not the one on the driver, so <code>stdout</code> on the driver won’t show these! To print all elements on the driver, one can use the <code>collect()</code> method to first bring the RDD to the driver node thus: <code>rdd.collect().foreach(println)</code>. This can cause the driver to run out of memory, though, because <code>collect()</code> fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the <code>take()</code>: <code>rdd.take(100).foreach(println)</code>.</p><p>另一个常见用法是尝试使用<code>rdd.foreach(println)</code>或打印RDD的元素<code>rdd.map(println)</code>。在单台机器上，这将生成预期的输出并打印所有RDD的元素。但是，在<code>cluster</code>模式下，<code>stdout</code>执行程序要调用的输出现在正在写入执行程序的输出，<code>stdout</code>而不是驱动程序上的那个，因此<code>stdout</code>驱动程序将不会显示这些！要打印在驱动器的所有元素，可以使用的<code>collect()</code>方法，首先使RDD到驱动器节点从而：<code>rdd.collect().foreach(println)</code>。但是，这可能会导致驱动程序用尽内存，因为<code>collect()</code>会将整个RDD提取到一台计算机上。如果只需要打印RDD的一些元素，则更安全的方法是使用<code>take()</code>：<code>rdd.take(100).foreach(println)</code>。</p><ol><li>本地模式：可以用<code>rdd.foreach(println)</code>或者<code>rdd.map(println)</code></li><li>集群模式：<code>rdd.collect().foreach(println)</code>，可能会内存溢出，取出部分数据用<code>rdd.take(100).foreach(println)</code></li></ol><h3 id="5-7-Working-with-Key-Value-Pairs"><a href="#5-7-Working-with-Key-Value-Pairs" class="headerlink" title="5.7 Working with Key-Value Pairs"></a>5.7 Working with Key-Value Pairs</h3><p>While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements by a key.</p><p>In Scala, these operations are automatically available on RDDs containing <a href="http://www.scala-lang.org/api/2.11.8/index.html#scala.Tuple2" target="_blank" rel="noopener">Tuple2</a> objects (the built-in tuples in the language, created by simply writing <code>(a, b)</code>). The key-value pair operations are available in the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">PairRDDFunctions</a> class, which automatically wraps around an RDD of tuples.</p><p>For example, the following code uses the <code>reduceByKey</code> operation on key-value pairs to count how many times each line of text occurs in a file:</p><p>尽管大多数Spark操作可在包含任何类型对象的RDD上运行，但一些特殊操作仅可用于键-值对的RDD。最常见的是分布式“混洗”操作，例如通过键对元素进行分组或聚合。</p><p>在Scala中，这些操作在包含<a href="http://www.scala-lang.org/api/2.11.8/index.html#scala.Tuple2" target="_blank" rel="noopener">Tuple2</a>对象（该语言的内置元组，只需编写即可创建<code>(a, b)</code>）的<a href="http://www.scala-lang.org/api/2.11.8/index.html#scala.Tuple2" target="_blank" rel="noopener">RDD</a>上自动可用 。<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">PairRDDFunctions</a>类中提供键值对操作， 该类会自动包装RDD元组。</p><p>例如，以下代码<code>reduceByKey</code>对键值对使用运算来计算文件中每一行文本出现的次数：</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> lines = sc.textFile(<span class="hljs-string">"data.txt"</span>)<span class="hljs-keyword">val</span> pairs = lines.map(s =&gt; (s, <span class="hljs-number">1</span>))<span class="hljs-keyword">val</span> counts = pairs.reduceByKey((a, b) =&gt; a + b)</code></pre><p>We could also use <code>counts.sortByKey()</code>, for example, to sort the pairs alphabetically, and finally <code>counts.collect()</code> to bring them back to the driver program as an array of objects.</p><p><strong>Note:</strong> when using custom objects as the key in key-value pair operations, you must be sure that a custom <code>equals()</code> method is accompanied with a matching <code>hashCode()</code> method. For full details, see the contract outlined in the <a href="http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()" target="_blank" rel="noopener">Object.hashCode() documentation</a>.</p><p><code>counts.sortByKey()</code>例如，我们还可以使用按字母顺序对，最后 <code>counts.collect()</code>将它们作为对象数组带回到驱动程序中。</p><p><strong>注意：</strong>在键-值对操作中使用自定义对象作为键时，必须确保自定义<code>equals()</code>方法与匹配<code>hashCode()</code>方法一起使用。有关完整的详细信息，请参见<a href="http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()" target="_blank" rel="noopener">Object.hashCode（）文档中</a>概述的合同。</p><h3 id="5-8-Transformations"><a href="#5-8-Transformations" class="headerlink" title="5.8 Transformations"></a>5.8 Transformations</h3><p>The following table lists some of the common transformations supported by Spark. Refer to the RDD API doc (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/api/java/JavaRDD.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.html#pyspark.RDD" target="_blank" rel="noopener">Python</a>, <a href="http://spark.apache.org/docs/2.1.2/api/R/index.html" target="_blank" rel="noopener">R</a>) and pair RDD functions doc (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html" target="_blank" rel="noopener">Java</a>) for details.</p><table><thead><tr><th align="left">Transformation</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>map</strong>(<em>func</em>)</td><td align="left">Return a new distributed dataset formed by passing each element of the source through a function <em>func</em>.</td></tr><tr><td align="left"><strong>filter</strong>(<em>func</em>)</td><td align="left">Return a new dataset formed by selecting those elements of the source on which <em>func</em> returns true.</td></tr><tr><td align="left"><strong>flatMap</strong>(<em>func</em>)</td><td align="left">Similar to map, but each input item can be mapped to 0 or more output items (so <em>func</em> should return a Seq rather than a single item).</td></tr><tr><td align="left"><strong>mapPartitions</strong>(<em>func</em>)</td><td align="left">Similar to map, but runs separately on each partition (block) of the RDD, so <em>func</em> must be of type Iterator<T> =&gt; Iterator<U> when running on an RDD of type T.</td></tr><tr><td align="left"><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td><td align="left">Similar to mapPartitions, but also provides <em>func</em> with an integer value representing the index of the partition, so <em>func</em> must be of type (Int, Iterator<T>) =&gt; Iterator<U> when running on an RDD of type T.</td></tr><tr><td align="left"><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td><td align="left">Sample a fraction <em>fraction</em> of the data, with or without replacement, using a given random number generator seed.</td></tr><tr><td align="left"><strong>union</strong>(<em>otherDataset</em>)</td><td align="left">Return a new dataset that contains the union of the elements in the source dataset and the argument.</td></tr><tr><td align="left"><strong>intersection</strong>(<em>otherDataset</em>)</td><td align="left">Return a new RDD that contains the intersection of elements in the source dataset and the argument.</td></tr><tr><td align="left"><strong>distinct</strong>([<em>numTasks</em>]))</td><td align="left">Return a new dataset that contains the distinct elements of the source dataset.</td></tr><tr><td align="left"><strong>groupByKey</strong>([<em>numTasks</em>])</td><td align="left">When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. <strong>Note:</strong> If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better performance. <strong>Note:</strong> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td></tr><tr><td align="left"><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td><td align="left">When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <em>func</em>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument.</td></tr><tr><td align="left"><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td><td align="left">When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral “zero” value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument.</td></tr><tr><td align="left"><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td><td align="left">When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td></tr><tr><td align="left"><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td><td align="left">When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.</td></tr><tr><td align="left"><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td><td align="left">When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called <code>groupWith</code>.</td></tr><tr><td align="left"><strong>cartesian</strong>(<em>otherDataset</em>)</td><td align="left">When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).</td></tr><tr><td align="left"><strong>pipe</strong>(<em>command</em>, <em>[envVars]</em>)</td><td align="left">Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process’s stdin and lines output to its stdout are returned as an RDD of strings.</td></tr><tr><td align="left"><strong>coalesce</strong>(<em>numPartitions</em>)</td><td align="left">Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.</td></tr><tr><td align="left"><strong>repartition</strong>(<em>numPartitions</em>)</td><td align="left">Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.</td></tr><tr><td align="left"><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td><td align="left">Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within each partition because it can push the sorting down into the shuffle machinery.</td></tr></tbody></table><h3 id="5-9-Actions"><a href="#5-9-Actions" class="headerlink" title="5.9 Actions"></a>5.9 Actions</h3><p>The following table lists some of the common actions supported by Spark. Refer to the RDD API doc (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/api/java/JavaRDD.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.html#pyspark.RDD" target="_blank" rel="noopener">Python</a>, <a href="http://spark.apache.org/docs/2.1.2/api/R/index.html" target="_blank" rel="noopener">R</a>)</p><p>and pair RDD functions doc (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html" target="_blank" rel="noopener">Java</a>) for details.</p><table><thead><tr><th align="left">Action</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>reduce</strong>(<em>func</em>)</td><td align="left">Aggregate the elements of the dataset using a function <em>func</em> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.</td></tr><tr><td align="left"><strong>collect</strong>()</td><td align="left">Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.</td></tr><tr><td align="left"><strong>count</strong>()</td><td align="left">Return the number of elements in the dataset.</td></tr><tr><td align="left"><strong>first</strong>()</td><td align="left">Return the first element of the dataset (similar to take(1)).</td></tr><tr><td align="left"><strong>take</strong>(<em>n</em>)</td><td align="left">Return an array with the first <em>n</em> elements of the dataset.</td></tr><tr><td align="left"><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td><td align="left">Return an array with a random sample of <em>num</em> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td></tr><tr><td align="left"><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td><td align="left">Return the first <em>n</em> elements of the RDD using either their natural order or a custom comparator.</td></tr><tr><td align="left"><strong>saveAsTextFile</strong>(<em>path</em>)</td><td align="left">Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.</td></tr><tr><td align="left"><strong>saveAsSequenceFile</strong>(<em>path</em>) (Java and Scala)</td><td align="left">Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop’s Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).</td></tr><tr><td align="left"><strong>saveAsObjectFile</strong>(<em>path</em>) (Java and Scala)</td><td align="left">Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using <code>SparkContext.objectFile()</code>.</td></tr><tr><td align="left"><strong>countByKey</strong>()</td><td align="left">Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.</td></tr><tr><td align="left"><strong>foreach</strong>(<em>func</em>)</td><td align="left">Run a function <em>func</em> on each element of the dataset. This is usually done for side effects such as updating an <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#accumulators" target="_blank" rel="noopener">Accumulator</a> or interacting with external storage systems. <strong>Note</strong>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#understanding-closures-a-nameclosureslinka" target="_blank" rel="noopener">Understanding closures </a>for more details.</td></tr></tbody></table><h3 id="5-10-Shuffle-operations"><a href="#5-10-Shuffle-operations" class="headerlink" title="5.10 Shuffle operations"></a>5.10 Shuffle operations</h3><p>Certain operations within Spark trigger an event known as the shuffle. The shuffle is Spark’s mechanism for re-distributing data so that it’s grouped differently across partitions. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation.</p><p>Spark中的某些操作会触发一个称为shuffle的事件。shuffle是Spark的一种用于重新分配数据的机制，因此可以跨分区对数据进行不同的分组。这通常涉及跨执行程序和机器复制数据，从而使改组成为复杂且昂贵的操作。</p><h4 id="5-10-1-Background"><a href="#5-10-1-Background" class="headerlink" title="5.10.1 Background"></a>5.10.1 Background</h4><p>To understand what happens during the shuffle we can consider the example of the <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#ReduceByLink" target="_blank" rel="noopener"><code>reduceByKey</code></a> operation. The <code>reduceByKey</code> operation generates a new RDD where all values for a single key are combined into a tuple - the key and the result of executing a reduce function against all values associated with that key. The challenge is that not all values for a single key necessarily reside on the same partition, or even the same machine, but they must be co-located to compute the result.</p><p>In Spark, data is generally not distributed across partitions to be in the necessary place for a specific operation. During computations, a single task will operate on a single partition - thus, to organize all the data for a single <code>reduceByKey</code> reduce task to execute, Spark needs to perform an all-to-all operation. It must read from all partitions to find all the values for all keys, and then bring together values across partitions to compute the final result for each key - this is called the <strong>shuffle</strong>.</p><p>Although the set of elements in each partition of newly shuffled data will be deterministic, and so is the ordering of partitions themselves, the ordering of these elements is not. If one desires predictably ordered data following shuffle then it’s possible to use:</p><ul><li><code>mapPartitions</code> to sort each partition using, for example, <code>.sorted</code></li><li><code>repartitionAndSortWithinPartitions</code> to efficiently sort partitions while simultaneously repartitioning</li><li><code>sortBy</code> to make a globally ordered RDD</li></ul><p>Operations which can cause a shuffle include <strong>repartition</strong> operations like <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#RepartitionLink" target="_blank" rel="noopener"><code>repartition</code></a> and <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#CoalesceLink" target="_blank" rel="noopener"><code>coalesce</code></a>, <strong>‘ByKey</strong> operations (except for counting) like <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#GroupByLink" target="_blank" rel="noopener"><code>groupByKey</code></a> and <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#ReduceByLink" target="_blank" rel="noopener"><code>reduceByKey</code></a>, and <strong>join</strong> operations like <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#CogroupLink" target="_blank" rel="noopener"><code>cogroup</code></a> and <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#JoinLink" target="_blank" rel="noopener"><code>join</code></a>.</p><p>要了解随机播放期间发生的情况，我们可以考虑<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#ReduceByLink" target="_blank" rel="noopener"><code>reduceByKey</code></a>操作示例 。该<code>reduceByKey</code>操作将生成一个新的RDD，其中将单个键的所有值组合为一个元组-键以及针对与该键关联的所有值执行reduce函数的结果。挑战在于，并非单个键的所有值都必须位于同一分区，甚至同一台机器上，但是必须将它们放在同一位置才能计算结果。</p><p>在Spark中，数据通常不会跨分区分布在特定操作的必要位置。在计算期间，单个任务将在单个分区上运行-因此，要组织所有数据<code>reduceByKey</code>以执行单个reduce任务，Spark需要执行所有操作。它必须从所有分区读取以找到所有键的所有值，然后将各个分区的值汇总在一起以计算每个键的最终结果-这称为<strong>shuffle</strong>。</p><p>尽管新改组后的数据的每个分区中的元素集都是确定性的，分区本身的顺序也是如此，但这些元素的顺序不是确定性的。如果人们希望在改组后可以预期地排序数据，则可以使用：</p><ul><li><code>mapPartitions</code> 使用例如，对每个分区进行排序 <code>.sorted</code></li><li><code>repartitionAndSortWithinPartitions</code> 在对分区进行有效排序的同时进行重新分区</li><li><code>sortBy</code> 制作全球订购的RDD</li></ul><p>这可能会导致一个洗牌的操作包括<strong>重新分区</strong>一样操作 <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#RepartitionLink" target="_blank" rel="noopener"><code>repartition</code></a>和<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#CoalesceLink" target="_blank" rel="noopener"><code>coalesce</code></a>，<strong>ByKey”</strong>操作，比如（除计数）<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#GroupByLink" target="_blank" rel="noopener"><code>groupByKey</code></a>和<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#ReduceByLink" target="_blank" rel="noopener"><code>reduceByKey</code></a>，并 <strong>加入</strong>操作，如<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#CogroupLink" target="_blank" rel="noopener"><code>cogroup</code></a>和<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#JoinLink" target="_blank" rel="noopener"><code>join</code></a>。</p><h4 id="5-10-2-Performance-Impact"><a href="#5-10-2-Performance-Impact" class="headerlink" title="5.10.2 Performance Impact"></a>5.10.2 Performance Impact</h4><p>The <strong>Shuffle</strong> is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - <em>map</em> tasks to organize the data, and a set of <em>reduce</em> tasks to aggregate it. This nomenclature comes from MapReduce and does not directly relate to Spark’s <code>map</code> and <code>reduce</code> operations.</p><p>Internally, results from individual map tasks are kept in memory until they can’t fit. Then, these are sorted based on the target partition and written to a single file. On the reduce side, tasks read the relevant sorted blocks.</p><p>Certain shuffle operations can consume significant amounts of heap memory since they employ in-memory data structures to organize records before or after transferring them. Specifically, <code>reduceByKey</code> and <code>aggregateByKey</code> create these structures on the map side, and <code>&#39;ByKey</code> operations generate these on the reduce side. When data does not fit in memory Spark will spill these tables to disk, incurring the additional overhead of disk I/O and increased garbage collection.</p><p>Shuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. This is done so the shuffle files don’t need to be re-created if the lineage is re-computed. Garbage collection may happen only after a long period of time, if the application retains references to these RDDs or if GC does not kick in frequently. This means that long-running Spark jobs may consume a large amount of disk space. The temporary storage directory is specified by the <code>spark.local.dir</code> configuration parameter when configuring the Spark context.</p><p>Shuffle behavior can be tuned by adjusting a variety of configuration parameters. See the ‘Shuffle Behavior’ section within the <a href="http://spark.apache.org/docs/2.1.2/configuration.html" target="_blank" rel="noopener">Spark Configuration Guide</a>.</p><p>所述<strong>随机播放</strong>是昂贵的操作，因为它涉及的磁盘I / O，数据序列，和网络I / O。为了组织随机数据，Spark生成任务集- <em>映射</em>任务以组织数据，以及一组<em>reduce</em>任务来聚合数据。此术语来自MapReduce，与Spark <code>map</code>和<code>reduce</code>操作没有直接关系。</p><p>在内部，单个地图任务的结果会保留在内存中，直到无法容纳为止。然后，根据目标分区对它们进行排序并写入单个文件。在简化方面，任务读取相关的已排序块。</p><p>某些混洗操作会消耗大量的堆内存，因为它们在转移它们之前或之后采用内存中的数据结构来组织记录。具体而言， <code>reduceByKey</code>并<code>aggregateByKey</code>创建在地图上侧这样的结构，和<code>&#39;ByKey</code>操作产生这些上减少侧。当数据不适合内存时，Spark会将这些表溢出到磁盘上，从而产生磁盘I / O的额外开销并增加垃圾回收。</p><p>随机播放还会在磁盘上生成大量中间文件。从Spark 1.3开始，将保留这些文件，直到不再使用相应的RDD并进行垃圾回收为止。这样做是为了在重新计算沿袭时无需重新创建随机播放文件。如果应用程序保留了对这些RDD的引用，或者如果GC不经常启动，则垃圾收集可能仅在很长一段时间之后才会发生。这意味着长时间运行的Spark作业可能会占用大量磁盘空间。<code>spark.local.dir</code>在配置Spark上下文时，临时存储目录由配置参数指定 。</p><p>可以通过调整各种配置参数来调整随机播放行为。请参阅《<a href="http://spark.apache.org/docs/2.1.2/configuration.html" target="_blank" rel="noopener">Spark配置指南</a>》中的“随机播放行为”部分。</p><h2 id="5-11-RDD-Persistence"><a href="#5-11-RDD-Persistence" class="headerlink" title="5.11 RDD Persistence"></a>5.11 RDD Persistence</h2><p>One of the most important capabilities in Spark is <em>persisting</em> (or <em>caching</em>) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.</p><p>You can mark an RDD to be persisted using the <code>persist()</code> or <code>cache()</code> methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.</p><p>In addition, each persisted RDD can be stored using a different <em>storage level</em>, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a <code>StorageLevel</code> object (<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.storage.StorageLevel" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/storage/StorageLevel.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.html#pyspark.StorageLevel" target="_blank" rel="noopener">Python</a>) to <code>persist()</code>. The <code>cache()</code> method is a shorthand for using the default storage level, which is <code>StorageLevel.MEMORY_ONLY</code> (store deserialized objects in memory). The full set of storage levels is:</p><p>Spark中最重要的功能之一就是跨操作将数据集<em>持久化</em>（或<em>缓存</em>）在内存中。当您保留RDD时，每个节点都会将其计算的所有分区存储在内存中，并在该数据集（或从该数据集派生的数据集）上的其他操作中重用它们。这样可以使以后的操作更快（通常快10倍以上）。缓存是用于迭代算法和快速交互使用的关键工具。</p><p>您可以使用RDD上的<code>persist()</code>或<code>cache()</code>方法将其标记为要保留。第一次在操作中对其进行计算时，它将被保存在节点上的内存中。Spark的缓存是容错的-如果RDD的任何分区丢失，它将使用最初创建它的转换自动重新计算。</p><p>此外，每个持久化的RDD可以使用不同的<em>存储级别</em>进行存储，例如，允许您将数据集持久化在磁盘上，持久化在内存中，但作为序列化的Java对象（以节省空间）在节点之间复制。通过将一个<code>StorageLevel</code>对象（<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.storage.StorageLevel" target="_blank" rel="noopener">Scala</a>， <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/storage/StorageLevel.html" target="_blank" rel="noopener">Java</a>， <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.html#pyspark.StorageLevel" target="_blank" rel="noopener">Python</a>）传递给来设置这些级别 <code>persist()</code>。该<code>cache()</code>方法是使用默认存储级别<code>StorageLevel.MEMORY_ONLY</code>（将反序列化的对象存储在内存中）的简写。完整的存储级别集是：</p><table><thead><tr><th align="left">Storage Level</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left">MEMORY_ONLY</td><td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</td></tr><tr><td align="left">MEMORY_AND_DISK</td><td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed.</td></tr><tr><td align="left">MEMORY_ONLY_SER (Java and Scala)</td><td align="left">Store RDD as <em>serialized</em> Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a <a href="http://spark.apache.org/docs/2.1.2/tuning.html" target="_blank" rel="noopener">fast serializer</a>, but more CPU-intensive to read.</td></tr><tr><td align="left">MEMORY_AND_DISK_SER (Java and Scala)</td><td align="left">Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of recomputing them on the fly each time they’re needed.</td></tr><tr><td align="left">DISK_ONLY</td><td align="left">Store the RDD partitions only on disk.</td></tr><tr><td align="left">MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td><td align="left">Same as the levels above, but replicate each partition on two cluster nodes.</td></tr><tr><td align="left">OFF_HEAP (experimental)</td><td align="left">Similar to MEMORY_ONLY_SER, but store the data in <a href="http://spark.apache.org/docs/2.1.2/configuration.html#memory-management" target="_blank" rel="noopener">off-heap memory</a>. This requires off-heap memory to be enabled.</td></tr></tbody></table><p><strong>Note:</strong> <em>In Python, stored objects will always be serialized with the <a href="https://docs.python.org/2/library/pickle.html" target="_blank" rel="noopener">Pickle</a> library, so it does not matter whether you choose a serialized level. The available storage levels in Python include <code>MEMORY_ONLY</code>, <code>MEMORY_ONLY_2</code>, <code>MEMORY_AND_DISK</code>, <code>MEMORY_AND_DISK_2</code>, <code>DISK_ONLY</code>, and <code>DISK_ONLY_2</code>.</em></p><p>Spark also automatically persists some intermediate data in shuffle operations (e.g. <code>reduceByKey</code>), even without users calling <code>persist</code>. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call <code>persist</code> on the resulting RDD if they plan to reuse it.</p><h3 id="5-12-Which-Storage-Level-to-Choose"><a href="#5-12-Which-Storage-Level-to-Choose" class="headerlink" title="5.12 Which Storage Level to Choose?"></a>5.12 Which Storage Level to Choose?</h3><p>Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. We recommend going through the following process to select one:</p><ul><li>If your RDDs fit comfortably with the default storage level (<code>MEMORY_ONLY</code>), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.</li><li>If not, try using <code>MEMORY_ONLY_SER</code> and <a href="http://spark.apache.org/docs/2.1.2/tuning.html" target="_blank" rel="noopener">selecting a fast serialization library</a> to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)</li><li>Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.</li><li>Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). <em>All</em> the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.</li></ul><p>Spark的存储级别旨在在内存使用率和CPU效率之间提供不同的权衡。我们建议通过以下过程选择一个：</p><ul><li>如果您的RDD与默认的存储级别（<code>MEMORY_ONLY</code>）相称，请保持这种状态。这是CPU效率最高的选项，允许对RDD的操作尽可能快地运行。</li><li>如果不是，请尝试使用<code>MEMORY_ONLY_SER</code>并<a href="http://spark.apache.org/docs/2.1.2/tuning.html" target="_blank" rel="noopener">选择一个快速的序列化库，</a>以使对象的空间效率更高，但访问速度仍然相当快。（Java和Scala）</li><li>除非用于计算数据集的函数很昂贵，否则它们会过滤到磁盘上，否则它们会过滤大量数据。否则，重新计算分区可能与从磁盘读取分区一样快。</li><li>如果要快速恢复故障，请使用复制的存储级别（例如，如果使用Spark来处理来自Web应用程序的请求）。<em>所有</em>存储级别都通过重新计算丢失的数据来提供完全的容错能力，但是复制的存储级别使您可以继续在RDD上运行任务，而不必等待重新计算丢失的分区。</li></ul><h2 id="6-Shared-Variables"><a href="#6-Shared-Variables" class="headerlink" title="6. Shared Variables"></a>6. Shared Variables</h2><p>Normally, when a function passed to a Spark operation (such as <code>map</code> or <code>reduce</code>) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of <em>shared variables</em> for two common usage patterns: broadcast variables and accumulators.</p><p>通常，当传递给Spark操作的函数（例如<code>map</code>或<code>reduce</code>）在远程集群节点上执行时，它将在该函数中使用的所有变量的单独副本上工作。这些变量将复制到每台计算机，并且远程计算机上变量的更新不会传播回驱动程序。在各个任务之间支持通用的读写共享变量将效率很低。但是，Spark确实为两种常用用法模式提供了两种有限类型的<em>共享变量</em>：广播变量和累加器。</p><h3 id="6-1-Broadcast-Variables"><a href="#6-1-Broadcast-Variables" class="headerlink" title="6.1 Broadcast Variables"></a>6.1 Broadcast Variables</h3><p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.</p><p>Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.</p><p>Broadcast variables are created from a variable <code>v</code> by calling <code>SparkContext.broadcast(v)</code>. The broadcast variable is a wrapper around <code>v</code>, and its value can be accessed by calling the <code>value</code> method. The code below shows this:</p><p>广播变量使程序员可以在每台计算机上保留一个只读变量，而不是将其副本与任务一起发送。例如，可以使用它们以有效的方式为每个节点提供大型输入数据集的副本。Spark还尝试使用有效的广播算法分配广播变量，以降低通信成本。</p><p>火花动作是通过一组阶段执行的，这些阶段由分布式“随机”操作分开。Spark自动广播每个阶段任务所需的通用数据。在运行每个任务之前，以这种方式广播的数据以序列化形式缓存并反序列化。这意味着仅当跨多个阶段的任务需要相同的数据或以反序列化形式缓存数据非常重要时，显式创建广播变量才有用。</p><p><code>v</code>通过调用从变量创建广播变量<code>SparkContext.broadcast(v)</code>。broadcast变量是的包装<code>v</code>，可以通过调用<code>value</code> 方法访问其值。下面的代码显示了这一点：</p><pre><code class="hljs scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;  <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"broadcast variables"</span>).setMaster(<span class="hljs-string">"local"</span>)  <span class="hljs-keyword">val</span> sparkContext = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)  <span class="hljs-keyword">val</span> broadcast = sparkContext.broadcast(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>))  broadcast.value.foreach(println)&#125;</code></pre><p>After the broadcast variable is created, it should be used instead of the value <code>v</code> in any functions run on the cluster so that <code>v</code> is not shipped to the nodes more than once. In addition, the object <code>v</code> should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).</p><p>创建广播变量之后，应使用它而不是<code>v</code>集群中运行的任何函数中的值，这样才<code>v</code>不会多次将其传送给节点。此外，在<code>v</code>广播对象后不应修改该对象 ，以确保所有节点都获得相同的广播变量值（例如，如果变量稍后被运送到新节点）。</p><h2 id="6-2-Accumulators"><a href="#6-2-Accumulators" class="headerlink" title="6.2 Accumulators"></a>6.2 Accumulators</h2><p>Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.</p><p>As a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance <code>counter</code>) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table.</p><p>累加器是仅通过关联和交换操作“添加”的变量，因此可以有效地并行支持。它们可用于实现计数器（如在MapReduce中）或总和。Spark本机支持数字类型的累加器，程序员可以添加对新类型的支持。</p><p>作为用户，您可以创建命名或未命名的累加器。如下图所示，一个已命名的累加器（在这种情况下<code>counter</code>）将在Web UI中显示修改该累加器的阶段。Spark在“任务”表中显示由任务修改的每个累加器的值。</p><p>A numeric accumulator can be created by calling <code>SparkContext.longAccumulator()</code> or <code>SparkContext.doubleAccumulator()</code> to accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using the <code>add</code> method. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its <code>value</code> method.</p><p>The code below shows an accumulator being used to add up the elements of an array:</p><p>可以通过分别调用<code>SparkContext.longAccumulator()</code>或<code>SparkContext.doubleAccumulator()</code> 累加Long或Double类型的值来创建数字累加器。然后，可以使用<code>add</code>方法将在集群上运行的任务添加到集群中。但是，他们无法读取其值。只有驱动程序可以使用其<code>value</code>方法读取累加器的值。</p><p>下面的代码显示了一个累加器，用于累加一个数组的元素：</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Accumulators</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"Accumulators"</span>).setMaster(<span class="hljs-string">"local[*]"</span>)    <span class="hljs-keyword">val</span> sparkContext = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)    <span class="hljs-keyword">val</span> accumulator = sparkContext.longAccumulator(<span class="hljs-string">"my accumulator"</span>)    <span class="hljs-keyword">val</span> myRdd = sparkContext.parallelize(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>))    <span class="hljs-keyword">val</span> sum = myRdd.foreach(x =&gt; accumulator.add(x))    println(accumulator.value)  &#125;&#125;</code></pre><p>While this code used the built-in support for accumulators of type Long, programmers can also create their own types by subclassing <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.util.AccumulatorV2" target="_blank" rel="noopener">AccumulatorV2</a>. The AccumulatorV2 abstract class has several methods which one has to override: <code>reset</code> for resetting the accumulator to zero, <code>add</code> for adding another value into the accumulator, <code>merge</code> for merging another same-type accumulator into this one. Other methods that must be overridden are contained in the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.util.AccumulatorV2" target="_blank" rel="noopener">API documentation</a>. For example, supposing we had a <code>MyVector</code> class representing mathematical vectors, we could write:</p><p>虽然此代码使用了对Long类型的累加器的内置支持，但程序员也可以通过对<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.util.AccumulatorV2" target="_blank" rel="noopener">AccumulatorV2</a>进行子类化来创建自己的类型。AccumulatorV2抽象类具有几种必须被覆盖的方法：<code>reset</code>将累加器重置为零，<code>add</code>将另一个值添加到累加器，<code>merge</code>将另一个相同类型的累加器合并到该累加器中 。<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.util.AccumulatorV2" target="_blank" rel="noopener">API文档</a>中包含其他必须重写的方法。例如，假设我们有一个<code>MyVector</code>代表数学向量的类，我们可以这样写：</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorAccumulatorV2</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">AccumulatorV2</span>[<span class="hljs-type">MyVector</span>, <span class="hljs-type">MyVector</span>] </span>&#123;  <span class="hljs-keyword">private</span> <span class="hljs-keyword">val</span> myVector: <span class="hljs-type">MyVector</span> = <span class="hljs-type">MyVector</span>.createZeroVector  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset</span></span>(): <span class="hljs-type">Unit</span> = &#123;    myVector.reset()  &#125;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span></span>(v: <span class="hljs-type">MyVector</span>): <span class="hljs-type">Unit</span> = &#123;    myVector.add(v)  &#125;  ...&#125;<span class="hljs-comment">// Then, create an Accumulator of this type:</span><span class="hljs-keyword">val</span> myVectorAcc = <span class="hljs-keyword">new</span> <span class="hljs-type">VectorAccumulatorV2</span><span class="hljs-comment">// Then, register it into spark context:</span>sc.register(myVectorAcc, <span class="hljs-string">"MyVectorAcc1"</span>)</code></pre><p>Note that, when programmers define their own type of AccumulatorV2, the resulting type can be different than that of the elements added.</p><p>For accumulator updates performed inside <strong>actions only</strong>, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.</p><p>Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like <code>map()</code>. The below code fragment demonstrates this property:</p><p>请注意，当程序员定义自己的AccumulatorV2类型时，结果类型可能与所添加元素的类型不同。</p><p>对于<strong>仅</strong>在<strong>操作</strong>内部<strong>执行的</strong>累加器更新，Spark保证每个任务对累加器的更新将仅应用一次，即重新启动的任务将不会更新该值。在转换中，用户应注意，如果重新执行任务或作业阶段，则可能不止一次应用每个任务的更新。</p><p>累加器不会更改Spark的惰性评估模型。如果在RDD上的操作中对其进行更新，则仅当将RDD计算为操作的一部分时才更新其值。因此，当在类似的惰性转换中进行累加器更新时，不能保证执行更新<code>map()</code>。下面的代码片段演示了此属性：</p><pre><code class="hljs scala"><span class="hljs-keyword">val</span> accum = sc.longAccumulatordata.map &#123; x =&gt; accum.add(x); x &#125;<span class="hljs-comment">// Here, accum is still 0 because no actions have caused the map operation to be computed.</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive编程指南-笔记(二)</title>
    <link href="/2020/07/11/Hive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E2%80%94%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2020/07/11/Hive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E2%80%94%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hive编程指南——笔记（一）</title>
    <link href="/2020/06/23/Hive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E2%80%94%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2020/06/23/Hive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E2%80%94%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="3-数据类型和文件格式"><a href="#3-数据类型和文件格式" class="headerlink" title="3. 数据类型和文件格式"></a>3. 数据类型和文件格式</h2><h3 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h3><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><ul><li>hive中的所有数据类型是对java中接口的实现，具体细节和java中的数据类型一样</li><li>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</li></ul><h2 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h2><p>表6-2</p><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct(‘John’，’Doe’)</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map(‘first’，’JOIN’，’last’，’Doe’)</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array(‘John’，’Doe’)</td></tr></tbody></table><ul><li>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</li><li>例子</li></ul><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">test</span>(<span class="hljs-keyword">name</span> <span class="hljs-keyword">string</span>,friends <span class="hljs-built_in">array</span>&lt;<span class="hljs-keyword">string</span>&gt;,children <span class="hljs-keyword">map</span>&lt;<span class="hljs-keyword">string</span>, <span class="hljs-built_in">int</span>&gt;,address <span class="hljs-keyword">struct</span>&lt;street:<span class="hljs-keyword">string</span>, city:<span class="hljs-keyword">string</span>&gt;)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span>collection items <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'_'</span><span class="hljs-keyword">map</span> <span class="hljs-keyword">keys</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">':'</span><span class="hljs-keyword">lines</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\n'</span>;</code></pre><p>row format delimited fields terminated by ‘,’ – 列分隔符</p><p>collection items terminated by ‘_’     –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p>map keys terminated by ‘:’                – MAP中的key与value的分隔符</p><p>lines terminated by ‘\n’;                  – 行分隔符</p><p>存储的数据：</p><pre><code class="hljs json">&#123;    <span class="hljs-attr">"name"</span>: <span class="hljs-string">"songsong"</span>,    <span class="hljs-attr">"friends"</span>: [<span class="hljs-string">"bingbing"</span> , <span class="hljs-string">"lili"</span>] ,       <span class="hljs-comment">//列表Array, </span>    <span class="hljs-attr">"children"</span>: &#123;                      <span class="hljs-comment">//键值Map,</span>        <span class="hljs-attr">"xiao song"</span>: <span class="hljs-number">18</span> ,        <span class="hljs-attr">"xiaoxiao song"</span>: <span class="hljs-number">19</span>    &#125;    <span class="hljs-string">"address"</span>: &#123;                      <span class="hljs-comment">//结构Struct,</span>        <span class="hljs-attr">"street"</span>: <span class="hljs-string">"hui long guan"</span> ,        <span class="hljs-attr">"city"</span>: <span class="hljs-string">"beijing"</span>     &#125;&#125;</code></pre><h3 id="3-3-文本文件数据编码"><a href="#3-3-文本文件数据编码" class="headerlink" title="3.3 文本文件数据编码"></a>3.3 文本文件数据编码</h3><table><thead><tr><th>分隔符</th><th>描述</th></tr></thead><tbody><tr><td>\n</td><td>对于文本文件来说，每行都是一条记录，因此换行符可以分割记录</td></tr><tr><td>^A（Ctrl+A）</td><td>用于分隔字段（列）。在CREATE TABLE语句中可以使用八进制编码\001表示</td></tr><tr><td>^B</td><td>用于分隔ARRARY或者STRUCT中的元素，或用于MAP中键-值对之间的分隔。在CREATE TABLE 语句中可以使用八进制编码\002表示</td></tr><tr><td>^C</td><td>用于MAP中键和值之间的分隔。在CREATE TABLE 语句中可以使用八进制编码\003表示</td></tr></tbody></table><pre><code class="hljs cos">CREATE TABLE employees (　name　　　　　　　STRING,　salary　　　　　　FLOAT,　subordinates 　　ARRAY&lt;STRING&gt;,　deductions　　　　MAP&lt;STRING, FLOAT&gt;,　address　　　　　STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;)<span class="hljs-comment">;</span>&#123;　<span class="hljs-string">"name"</span>: <span class="hljs-string">"John Doe"</span>,　<span class="hljs-string">"salary"</span>: <span class="hljs-number">100000.0</span>,　<span class="hljs-string">"subordinates"</span>: [<span class="hljs-string">"Mary Smith"</span>, <span class="hljs-string">"Todd Jones"</span>],　<span class="hljs-string">"deductions"</span>: &#123;　　<span class="hljs-string">"Federal Taxes"</span>: 　.<span class="hljs-number">2</span>,　　<span class="hljs-string">"State Taxes"</span>: 　　.<span class="hljs-number">05</span>,　　<span class="hljs-string">"Insurance"</span>: 　　　.<span class="hljs-number">1</span>　&#125;,　<span class="hljs-string">"address"</span>: &#123;　　<span class="hljs-string">"street"</span>: <span class="hljs-string">"1 Michigan Ave."</span>,　　<span class="hljs-string">"city"</span>: <span class="hljs-string">"Chicago"</span>,　　<span class="hljs-string">"state"</span>: <span class="hljs-string">"IL"</span>,　　<span class="hljs-string">"zip"</span>: <span class="hljs-number">60600</span>　&#125;&#125;John Doe<span class="hljs-symbol">^A100000</span><span class="hljs-number">.0</span><span class="hljs-symbol">^AMary</span> Smith<span class="hljs-symbol">^BTodd</span> Jones<span class="hljs-symbol">^AFederal</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.2</span><span class="hljs-symbol">^BStateTaxes</span><span class="hljs-symbol">^C</span><span class="hljs-number">.05</span>^ BInsurance<span class="hljs-symbol">^C</span><span class="hljs-number">.1</span><span class="hljs-symbol">^A1</span> Michigan Ave.<span class="hljs-symbol">^BChicago</span><span class="hljs-symbol">^BIL</span><span class="hljs-symbol">^B60600</span>Mary Smith<span class="hljs-symbol">^A80000</span><span class="hljs-number">.0</span><span class="hljs-symbol">^ABill</span> King<span class="hljs-symbol">^AFederal</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.2</span><span class="hljs-symbol">^BState</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.05</span><span class="hljs-symbol">^BInsurance</span>^ C<span class="hljs-number">.1</span><span class="hljs-symbol">^A100</span> Ontario St.<span class="hljs-symbol">^BChicago</span><span class="hljs-symbol">^BIL</span><span class="hljs-symbol">^B60601</span>Todd Jones<span class="hljs-symbol">^A70000</span><span class="hljs-number">.0</span><span class="hljs-symbol">^AFederal</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.15</span><span class="hljs-symbol">^BState</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.03</span><span class="hljs-symbol">^BInsurance</span><span class="hljs-symbol">^C</span><span class="hljs-number">.1</span><span class="hljs-symbol">^A200</span>   Chicago Ave.<span class="hljs-symbol">^BOak</span> Park<span class="hljs-symbol">^BIL</span><span class="hljs-symbol">^B60700</span>Bill King<span class="hljs-symbol">^A60000</span><span class="hljs-number">.0</span><span class="hljs-symbol">^AFederal</span> Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.15</span><span class="hljs-symbol">^BState</span>Taxes<span class="hljs-symbol">^C</span><span class="hljs-number">.03</span><span class="hljs-symbol">^BInsurance</span><span class="hljs-symbol">^C</span><span class="hljs-number">.1</span><span class="hljs-symbol">^A300</span> Obscure Dr.<span class="hljs-symbol">^BObscuria</span><span class="hljs-symbol">^BIL</span><span class="hljs-symbol">^B60100</span></code></pre><h3 id="3-4-读时模式"><a href="#3-4-读时模式" class="headerlink" title="3.4 读时模式"></a>3.4 读时模式</h3><p>不懂</p><h2 id="4-HiveQL：数据定义"><a href="#4-HiveQL：数据定义" class="headerlink" title="4. HiveQL：数据定义"></a>4. HiveQL：数据定义</h2><h3 id="4-1-Hive中的数据库"><a href="#4-1-Hive中的数据库" class="headerlink" title="4.1 Hive中的数据库"></a>4.1 Hive中的数据库</h3><ol><li>数据库类似命名空间或者目录</li><li>创建 create database [if not exists] database_name LOCATION … COMMENT…  WITH DBPROPERTIES()<ol><li>location 指定位置</li><li>comment 添加注释</li><li>WITH DBPROPERTIES 添加属性</li></ol></li><li>查看数据库 show database [like …]</li><li>查看数据的详细信息 describe database_name</li><li>使用数据库use database_name</li><li>查看当前正在使用的数据库 SELECT current_database()</li><li>删除数据库 drop database if exists database_name [CASCADE]</li></ol><h3 id="4-2-修改数据库"><a href="#4-2-修改数据库" class="headerlink" title="4.2 修改数据库"></a>4.2 修改数据库</h3><ol><li>修改数据库，只能修改DBPROPERTIES，数据库名字和存储位置都不能修改，alter database  database_name SET DBPROPERTIES (‘edited-by’ = ‘Joe Dba’);</li><li>没有办法可以删除或者“重置”数据库属性</li></ol><h3 id="4-3-创建表"><a href="#4-3-创建表" class="headerlink" title="4.3 创建表"></a>4.3 创建表</h3><ol><li>创建表create table if not exists table_name()</li><li>if not exists 只判断有没有同名的表存在，不去比较属性是不是不一样，如果是想修改表的属性，需要先删除原来的表，再重建新表</li><li>Hive会自动增加两个表属性：一个是last_modified_by，其保存着最后修改这个表的用户的用户名；另一个是last_modified_time，其保存着最后一次修改<br>的新纪元时间秒。</li><li>查看表结构 desc table_name，只输出包含有列描述信息的表结构信息</li><li>查看详细的表结构信息，DESCRIBE EXTENDED table_name，展示结构很乱</li><li>查看详细的表结构信息，DESCRIBE formatted table_name，展示结构很清晰</li><li>SHOW TBLPROPERTIES table_name命令，用于列举出某个表的TBLPROPERTIES属性<br>信息。</li><li>拷贝一个已存在的表结构，不包含数据，CREATE TABLE IF NOT EXISTS mydb.employees2<br>LIKE mydb.employees;</li><li>查看所有的表：show tables [like …]；支持简单正则</li><li>指定查看摸个数据库下的表：show tables in database_name;</li></ol><h4 id="4-3-1-管理表"><a href="#4-3-1-管理表" class="headerlink" title="4.3.1 管理表"></a>4.3.1 管理表</h4><ol><li>hive控制着数据的生命周期，删除表的时候，表中的数据也会被删除</li></ol><h4 id="4-3-2-外部表"><a href="#4-3-2-外部表" class="headerlink" title="4.3.2 外部表"></a>4.3.2 外部表</h4><ol><li>CREATE EXTERNAL TABLE IF NOT EXISTS table_name() ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’  LOCATION ‘/data/stocks’;</li><li>hive删除外部表时候，表中的数据不会被删除</li><li>查看是外部表还是内部表：DESCRIBE EXTENDED table_name中tableType:</li></ol><h3 id="4-4-分区表、管理表"><a href="#4-4-分区表、管理表" class="headerlink" title="4.4 分区表、管理表"></a>4.4 分区表、管理表</h3><ol><li>分区表，更快的查询</li><li>存储，按照分区字段分目录存储</li><li>分区字段使用起来和普通字段是一样的</li><li>如果表中的数据太多，select代价很大，可以开启hive 的严格模式，select必须要加上where条件，set hive.mapred.mode=strict; set hive.mapred.mode=nonstrict;</li></ol><blockquote><p>如果表中的数据以及分区个数都非常大的话，执行这样一个包含有所有<br>分区的查询可能会触发一个巨大的MapReduce任务。一个高度建议的安全措施就是<br>将Hive设置为“strict(严格)”模式，这样如果对分区表进行查询而WHERE子句没<br>有加分区过滤的话，将会禁止提交这个任务。用户也可以按照下面的语句将属性值<br>设置为“nostrict(非严格)”</p></blockquote><ol start="5"><li>查看分区show partitions table_name partition(’‘);</li></ol><h4 id="4-4-1-外部分区表"><a href="#4-4-1-外部分区表" class="headerlink" title="4.4.1 外部分区表"></a>4.4.1 外部分区表</h4><ol><li>创建分区，ALTER TABLE … ADD PARTITION</li><li>往分区内移入数据，hdfs拷贝数据到对应的目录下</li><li>查看到分区数据所在的路径： DESCRIBE EXTENDED log_messages PARTITION (year=2012, month=1, day=2);</li></ol><h4 id="4-4-2-自定义表的存储格式"><a href="#4-4-2-自定义表的存储格式" class="headerlink" title="4.4.2 自定义表的存储格式"></a>4.4.2 自定义表的存储格式</h4><ol><li>hive默认存储格式：text</li><li>创建表的时候指定：STORED AS TEXTFILE;</li><li>记录的解析是由序列化器/反序列化器（或者缩写为SerDe）来控制的。</li><li>ROW FORMAT SERDE …指定SerDe</li><li>STORED AS INPUTFORMAT … OUTPUTFORMAT …子句分别指定了用于输入格式和输出格式的Java类。</li><li><strong>Hive使用一个inputformat对象将输入流分割成记录，然后使用一个outputformat对象来将记录格式化为输出流（例如查询的输出结果），再使用一个SerDe在读数据时将记录解析成列，在写数据时将列编码成记录。</strong>—有疑问</li></ol><h3 id="4-5-删除表"><a href="#4-5-删除表" class="headerlink" title="4.5 删除表"></a>4.5 删除表</h3><ol><li>drop table if exists table_name</li><li>如果开启回收站功能，会将删除的数据移到用户根目录下的.Trash目录下，即hdfs中的/user/$USER/.Trash目录</li></ol><h3 id="4-6-修改表"><a href="#4-6-修改表" class="headerlink" title="4.6 修改表"></a>4.6 修改表</h3><p>不怎么常用，一般会选择重新建表</p><h4 id="4-6-2-增加，修改，删除分区"><a href="#4-6-2-增加，修改，删除分区" class="headerlink" title="4.6.2 增加，修改，删除分区"></a>4.6.2 增加，修改，删除分区</h4><ol><li>增加分区：ALTER TABLE log_messages ADD IF NOT EXISTS PARTITION (year = 2011, month = 1, day = 1) LOCATION ‘/logs/2011/01/01’</li><li>修改分区路径：ALTER TABLE log_messages PARTITION(year = 2011, month = 12, day = 2) SET LOCATION ‘s3n://ourbucket/logs/2011/01/02’;</li><li>删除分区：ALTER TABLE log_messages DROP IF EXISTS PARTITION(year = 2011, month = 12, day = 2);</li></ol>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>读书笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL中字符串相关的部分函数（一）</title>
    <link href="/2020/06/23/Mysql%E4%B8%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E5%85%B3%E7%9A%84%E9%83%A8%E5%88%86%E5%87%BD%E6%95%B0/"/>
    <url>/2020/06/23/Mysql%E4%B8%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E5%85%B3%E7%9A%84%E9%83%A8%E5%88%86%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-trim"><a href="#1-trim" class="headerlink" title="1. trim"></a>1. trim</h2><p><strong>TRIM([{BOTH | LEADING | TRAILING} [remstr] FROM] str)</strong> </p><p>作用：去掉指定的字符</p><ol><li><p>不指定指定字符，默认去掉空格</p><ul><li><p>默认去掉左右空格： select trim(‘  a   ‘);   –’a’</p></li><li><p>去掉左边空格：select ltrim(‘   a  ‘);  –’a  ‘</p></li><li><p>去掉右边空格：select rtrim(‘  a  ‘);  –’  a’</p></li></ul></li><li><p>指定字符</p><ul><li>去掉两边字符：select trim(both ‘|’  from ‘|abc|’); –’abc’</li><li>去掉左边字符：select trim(LEADING ‘|’  from ‘|abc|’); –’abc|’</li><li>去掉右边字符：select trim(TRAILING ‘|’  from ‘|abc|’); –’|abc’</li></ul></li></ol><h2 id="2-instr"><a href="#2-instr" class="headerlink" title="2. instr"></a>2. instr</h2><p><strong>instr(str,substr)</strong> </p><p>作用：返回substr第一次在str中出现的位置</p><p>select instr(‘aaa_0001’ ,  ‘_’); –4</p><h2 id="3-substr"><a href="#3-substr" class="headerlink" title="3. substr"></a>3. substr</h2><p><strong>substr(str,pos[,len])</strong></p><p>作用：从字符串中的指定位置pos开始取一个字符串返回</p><p>select substr(‘aaa_0001’ ,  4); –’_0001’</p><h2 id="4-substring"><a href="#4-substring" class="headerlink" title="4. substring"></a>4. substring</h2><p><strong>substring(str,n,len)</strong></p><p>作用：获取子串，从n开始截取str中长度为len的子串</p><p>select substring(‘aaa_0001’, 1, 3); –’aaa’</p>]]></content>
    
    
    <categories>
      
      <category>MySQL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hive常用函数</title>
    <link href="/2020/06/18/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
    <url>/2020/06/18/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-concat：拼接字符串"><a href="#1-concat：拼接字符串" class="headerlink" title="1. concat：拼接字符串"></a>1. concat：拼接字符串</h2><p> <strong>concat(string|binary A, string|binary B…)</strong></p><p> <strong>作用</strong>：将字符串按顺序拼接成一个字符串</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat</span>(<span class="hljs-string">'a'</span>,<span class="hljs-string">'_'</span>,<span class="hljs-string">'b'</span>);  <span class="hljs-comment">-- a_b</span></code></pre><p> <strong>注意</strong>：如果有任何一个参数为null，返回结果为null </p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat</span>(<span class="hljs-string">'a'</span>,<span class="hljs-string">'_'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-literal">null</span>);  <span class="hljs-comment">-- NULL</span></code></pre><h2 id="2-concat-ws：指定拼接字符串拼接"><a href="#2-concat-ws：指定拼接字符串拼接" class="headerlink" title="2. concat_ws：指定拼接字符串拼接"></a>2. concat_ws：指定拼接字符串拼接</h2><p><strong>concat_ws(string SEP, string A, string B…)</strong></p><p>concat_ws是concat的特殊形式，可以自定义分隔符SEP</p><p>select concat_ws(‘_’,’a’,’b’);    – a_b</p><p><strong>注意</strong>：</p><ol><li>分隔符可以任何参数，字符串，特殊符号都可以</li><li>分隔符为null时，结果为null</li><li>concat_ws会忽略其他除分隔符外其他为null的参数，不会忽略空字符串</li></ol><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat_ws</span>(<span class="hljs-string">'W'</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>);<span class="hljs-comment">-- aWb</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat_ws</span>(<span class="hljs-literal">null</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>);<span class="hljs-comment">-- NULL</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat_ws</span>(<span class="hljs-string">'#'</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">' '</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">''</span>,<span class="hljs-string">'c'</span>);<span class="hljs-comment">-- a# #b##c</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">concat_ws</span>(<span class="hljs-string">'|'</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-literal">null</span>,<span class="hljs-string">'c'</span>,<span class="hljs-literal">null</span>,<span class="hljs-string">'d'</span>);<span class="hljs-comment">-- a|b|c|d</span></code></pre><h2 id="3-nvl：空值处理"><a href="#3-nvl：空值处理" class="headerlink" title="3. nvl：空值处理"></a>3. nvl：空值处理</h2><p><strong>nvl(T value, T default_value)</strong></p><p>如果value为null，返回default_value,否则返回value，开发中十分常用</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> nvl(<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>);<span class="hljs-comment">-- a</span><span class="hljs-keyword">select</span> nvl(<span class="hljs-literal">null</span>, <span class="hljs-string">'b'</span>);<span class="hljs-comment">-- b</span></code></pre><p><strong>补充</strong>：去除为null和空’’的值，select nvl(id,’’) != ‘’;</p><h2 id="4-datediff：计算两个日期的差值"><a href="#4-datediff：计算两个日期的差值" class="headerlink" title="4.  datediff：计算两个日期的差值"></a>4.  datediff：计算两个日期的差值</h2><p><strong>datediff(string enddate, string startdate)</strong></p><p>返回值类型：int</p><p>日期比较函数，返回结束日期减去开始日期的天数</p><pre><code class="hljs sql">hive&gt; select datediff('2020-02-02','2020-02-01');  --返回1</code></pre><p>注：传入时间的格式必须是YYYY-MM-DD形式</p><pre><code class="hljs sql">hive&gt; select datediff('20200202','20200201');  --返回null</code></pre><h2 id="5-cast：显式转换"><a href="#5-cast：显式转换" class="headerlink" title="5. cast：显式转换"></a>5. cast：显式转换</h2><p><strong>类型转换：</strong></p><ol><li>任何类型都可以自动转成一种范围更大的类型，TINYINT,SMALLINT,INT,BIGINT,FLOAT和STRING都可以隐式地转换成DOUBLE；</li><li>BOOLEAN类型不能转换为其他任何数据类型。</li><li>从小范围往大范围转，不能自动完成，需要加cast关键字做强制类型转换。</li><li>转换失败返回null。</li></ol><p><strong>常用：</strong></p><p>1.获取当前日期：</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">cast</span> (<span class="hljs-keyword">current_date</span> <span class="hljs-keyword">as</span> <span class="hljs-keyword">string</span>)； <span class="hljs-comment">-- 2020-06-12</span></code></pre><p>2.string转数字类型，参与运算</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span>  <span class="hljs-keyword">max</span>(<span class="hljs-keyword">cast</span>(code <span class="hljs-keyword">as</span> <span class="hljs-built_in">int</span>)); <span class="hljs-comment">-- code 是string类型的一串数字 例：110911893303 ，2000098777，</span><span class="hljs-comment">--为取到code的最大值，如果不强转成int，按string比较，会取到的最大值是2000098777</span></code></pre><h2 id="6-case-when-then-else-end"><a href="#6-case-when-then-else-end" class="headerlink" title="6. case when then else end"></a>6. case when then else end</h2><p><b>CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END<b/></p><p>解释：When a = true, returns b; when c = true, returns d; else returns e.</p><p>类似java中的switch case语句和scala中的模式匹配</p><p><strong>示例：</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> age <span class="hljs-keyword">from</span> student;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142231143.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age &lt; <span class="hljs-number">18</span> <span class="hljs-keyword">then</span> <span class="hljs-string">"少年"</span> <span class="hljs-keyword">when</span> age &lt; <span class="hljs-number">60</span> <span class="hljs-keyword">then</span> <span class="hljs-string">"成年人"</span>  <span class="hljs-keyword">else</span> <span class="hljs-string">"老年人"</span> <span class="hljs-keyword">end</span> <span class="hljs-keyword">from</span> student;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142313352.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3gxMjM0NV8=,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><h2 id="7-sum-case-when"><a href="#7-sum-case-when" class="headerlink" title="7. sum case when"></a>7. sum case when</h2><p><strong>对是否满足条件进行计数</strong></p><p>统计各个年龄段人的数量：</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age&lt; <span class="hljs-number">18</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span> teenager, <span class="hljs-keyword">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age&lt; <span class="hljs-number">60</span> <span class="hljs-keyword">and</span> age&gt;=<span class="hljs-number">18</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span> adult, <span class="hljs-keyword">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> age&gt;= <span class="hljs-number">60</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span> elderly <span class="hljs-keyword">from</span> student;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142451709.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><h2 id="8-row-number-over"><a href="#8-row-number-over" class="headerlink" title="8. row_number() over()"></a>8. row_number() over()</h2><p><strong>row_number() over()可以给每个行数据根据分组排序形成一个序号</strong></p><p>示例：</p><p>取最近的一条订单号</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> test_row_number_over;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142740868.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">no</span>,paydate,rn <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-keyword">no</span>,paydate,row_number() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">distribute</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">no</span> <span class="hljs-keyword">sort</span> <span class="hljs-keyword">by</span> paydate <span class="hljs-keyword">desc</span> ) <span class="hljs-keyword">as</span> rn <span class="hljs-keyword">from</span> test_row_number_over) t ;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142826546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3gxMjM0NV8=,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">no</span>,paydate <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-keyword">no</span>,paydate,row_number() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">distribute</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">no</span> <span class="hljs-keyword">sort</span> <span class="hljs-keyword">by</span> paydate <span class="hljs-keyword">desc</span> ) <span class="hljs-keyword">as</span> rn <span class="hljs-keyword">from</span> test_row_number_over) t <span class="hljs-keyword">where</span> t.rn=<span class="hljs-number">1</span>;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612142933642.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><h2 id="9-greatest：多列中取到最大值"><a href="#9-greatest：多列中取到最大值" class="headerlink" title="9. greatest：多列中取到最大值"></a>9. greatest：多列中取到最大值</h2><pre><code class="hljs sql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> test_greatest;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612143100732.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">greatest</span>(a,b,c) <span class="hljs-keyword">from</span> test_greatest; <span class="hljs-comment">-- 3 7</span></code></pre><p><strong>注：</strong> 如果含有string类型的列，返回null</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">greatest</span>(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-string">"a"</span>); <span class="hljs-comment">--null</span></code></pre><h2 id="10-collect-list：列转行"><a href="#10-collect-list：列转行" class="headerlink" title="10.  collect_list：列转行"></a>10.  collect_list：列转行</h2><p>Hive中collect相关的函数有collect_list和collect_set。</p><p>它们都是将分组中的某列转为一个数组返回，不同的是collect_list不去重而collect_set去重。</p><p>示例：</p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> test_collect_list;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612143327465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3gxMjM0NV8=,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>1. collect_list 不去重</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">name</span>,collect_list(video) <span class="hljs-keyword">from</span> test_collect_list <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span>;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612143348422.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>2. collect_set 去重</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">name</span>,collect_set(video) <span class="hljs-keyword">from</span> test_collect_list <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span>;</code></pre><p><img src="https://img-blog.csdnimg.cn/202006121434126.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>3. concat_ws修改拼接格式</strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">name</span>,<span class="hljs-keyword">concat_ws</span>(<span class="hljs-string">','</span>,collect_list(video)) <span class="hljs-keyword">from</span> test_collect_list <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">name</span>;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200612143438521.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Impala入门</title>
    <link href="/2020/06/16/impala%E5%85%A5%E9%97%A8/"/>
    <url>/2020/06/16/impala%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Impala简介"><a href="#1-Impala简介" class="headerlink" title="1. Impala简介"></a>1. Impala简介</h2><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h3><ol><li>Impala提供对存储在HDFS、Hbase或Amazon Simple Storage Service（S3）数据的高性能、低延迟的交互式SQL查询功能。</li><li>基于Hive，使用Hive的元数据，类似Hive的SQL，JDBC和用户操作界面。Impala和Hive的定位不同，Impala查询速度快，可以做为大数据查询的补充，Hive更适合做需要长时间运行的跑批操作。</li><li>Impala由Cloudera公司推出，2017年11份从Apache Incubator毕业，是CDH平台首选的PB级大数据实时查询分析引擎。</li></ol><h3 id="1-2-优缺点"><a href="#1-2-优缺点" class="headerlink" title="1.2 优缺点"></a>1.2 优缺点</h3><h4 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h4><ol><li>使用SQL查询，使用上手快。</li><li>兼容Hive，可以访问hive的metastore，对hive数据直接做数据分析。</li><li>基于内存运算，不需要把中间结果写入磁盘，省掉了大量的I/O开销。</li><li>无需转换为Mapreduce，直接访问存储在HDFS，HBase中的数据进行作业调度，速度快。</li><li>使用了支持Data locality的I/O调度机制，尽可能地将数据和计算分配在同一台机器上进行，减少了网络开销。</li><li>支持各种文件格式，如TEXTFILE 、SEQUENCEFILE 、RCFile、Parquet。</li></ol><h4 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h4><ol><li>对内存的依赖大，且完全依赖于hive。</li><li>实践中，分区超过1万，性能严重下降。</li><li>只能读取文本文件，而不能直接读取自定义二进制文件。</li><li>每当新的记录/文件被添加到HDFS中的数据目录时，该表需要被刷新。</li></ol><h3 id="1-3-Impala架构"><a href="#1-3-Impala架构" class="headerlink" title="1.3 Impala架构"></a>1.3 Impala架构</h3><p>Impala自身包含三个模块：Impalad、Statestore和Catalog，除此之外它还依赖Hive Metastore和HDFS。</p><h4 id="1-3-1-核心组件"><a href="#1-3-1-核心组件" class="headerlink" title="1.3.1 核心组件"></a>1.3.1 核心组件</h4><ol><li><p>Impalad：</p><ul><li>接收client的请求、Query执行并返回给中心协调节点</li><li>子节点上的守护进程，负责向statestore保持通信，汇报工作。</li><li>具体细节：<ul><li>query planner：翻译sql，生成计划</li><li>query coordinator：协调器，分配任务给query executor</li><li>query executor：执行查询任务</li></ul></li></ul></li><li><p>Catalog：</p><ul><li><p>分发表的元数据信息到各个impalad中；</p></li><li><p>接收来自statestore的所有请求。</p></li></ul></li><li><p>Statestore：</p><ul><li><p>负责收集分布在集群中各个impalad进程的资源信息、各节点健康状况，同步节点信息；</p></li><li><p>负责query的协调调度。</p></li></ul></li></ol><h4 id="1-3-2-执行流程"><a href="#1-3-2-执行流程" class="headerlink" title="1.3.2 执行流程"></a>1.3.2 执行流程</h4><p><img src="C:%5CUsers%5C16336%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200616172905838.png" srcset="/img/loading.gif" alt="image-20200616172905838"></p><ol><li>由Client发送一个执行SQL到任意一台Impalad的Query Planner</li><li>由Query Planner 把SQL发向Query Coordinator</li><li>由Query Coordinator 来调度分配任务到Impalad的所有节点</li><li>各个Impalad节点的Query Executor 进行执行SQL工作</li><li>执行SQL结束以后，将结果返回给Query Coordinator</li><li>再由Query Coordinator 将结果返回给Client</li></ol><h2 id="2-操作命令"><a href="#2-操作命令" class="headerlink" title="2. 操作命令"></a>2. 操作命令</h2><h3 id="2-1-外部shell命令"><a href="#2-1-外部shell命令" class="headerlink" title="2.1 外部shell命令"></a>2.1 外部shell命令</h3><table><thead><tr><th align="left">选项</th><th>描述</th></tr></thead><tbody><tr><td align="left">-h,  –help</td><td>显示帮助信息</td></tr><tr><td align="left">-v  or –version</td><td>显示版本信息</td></tr><tr><td align="left">-i  hostname, –impalad=hostname</td><td>指定连接运行  impalad 守护进程的主机。默认端口是 21000。</td></tr><tr><td align="left">-q query, –query=query</td><td>从命令行中传递一个shell  命令。执行完这一语句后 shell 会立即退出。</td></tr><tr><td align="left">-f query_file, –query_file= query_file</td><td>传递一个文件中的 SQL  查询。文件内容必须以分号分隔</td></tr><tr><td align="left">-o filename or –output_file filename</td><td>保存所有查询结果到指定的文件。通常用于保存在命令行使用 -q 选项执行单个查询时的查询结果。</td></tr><tr><td align="left">-c</td><td>查询执行失败时继续执行</td></tr><tr><td align="left">-d default_db or  –database=default_db</td><td>指定启动后使用的数据库，与建立连接后使用use语句选择数据库作用相同，如果没有指定，那么使用default数据库</td></tr><tr><td align="left">-r or –refresh_after_connect</td><td>建立连接后刷新  Impala 元数据</td></tr><tr><td align="left">-p, –show_profiles</td><td>对 shell 中执行的每一个查询，显示其查询执行计划</td></tr><tr><td align="left">-B（–delimited）</td><td>去格式化输出</td></tr><tr><td align="left">–output_delimiter=character</td><td>指定分隔符</td></tr><tr><td align="left">–print_header</td><td>打印列名</td></tr></tbody></table><h3 id="2-2-内部shell命令"><a href="#2-2-内部shell命令" class="headerlink" title="2.2 内部shell命令"></a>2.2 内部shell命令</h3><table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>help</td><td>显示帮助信息</td></tr><tr><td>explain  <sql></td><td>显示执行计划</td></tr><tr><td>profile</td><td>(查询完成后执行） 查询最近一次查询的底层信息</td></tr><tr><td>shell <shell></td><td>不退出impala-shell执行shell命令</td></tr><tr><td>version</td><td>显示版本信息（同于impala-shell  -v）</td></tr><tr><td>connect</td><td>连接impalad主机，默认端口21000（同于impala-shell -i）</td></tr><tr><td>refresh <tablename></td><td>增量刷新元数据库</td></tr><tr><td>invalidate metadata</td><td>全量刷新元数据库（慎用）（同于 impala-shell -r）</td></tr><tr><td>history</td><td>历史命令</td></tr></tbody></table><h2 id="3-数据类型"><a href="#3-数据类型" class="headerlink" title="3. 数据类型"></a>3. 数据类型</h2><table><thead><tr><th>Hive数据类型</th><th>Impala数据类型</th><th>长度</th></tr></thead><tbody><tr><td>TINYINT</td><td>TINYINT</td><td>1byte有符号整数</td></tr><tr><td>SMALINT</td><td>SMALINT</td><td>2byte有符号整数</td></tr><tr><td>INT</td><td>INT</td><td>4byte有符号整数</td></tr><tr><td>BIGINT</td><td>BIGINT</td><td>8byte有符号整数</td></tr><tr><td>BOOLEAN</td><td>BOOLEAN</td><td>布尔类型，true或者false</td></tr><tr><td>FLOAT</td><td>FLOAT</td><td>单精度浮点数</td></tr><tr><td>DOUBLE</td><td>DOUBLE</td><td>双精度浮点数</td></tr><tr><td>STRING</td><td>STRING</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td></tr><tr><td>TIMESTAMP</td><td>TIMESTAMP</td><td>时间类型</td></tr><tr><td>BINARY</td><td>不支持</td><td>字节数组</td></tr></tbody></table><p>注意：Impala虽然支持array，map，struct复杂数据类型，但是支持并不完全，一般处理方法，将复杂类型转化为基本类型，通过hive创建表。</p><h2 id="4-DDL操作"><a href="#4-DDL操作" class="headerlink" title="4. DDL操作"></a>4. DDL操作</h2><h2 id="5-DML操作"><a href="#5-DML操作" class="headerlink" title="5. DML操作"></a>5. DML操作</h2>]]></content>
    
    
    <categories>
      
      <category>Impala</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Impala</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive Create Table</title>
    <link href="/2020/06/12/Hive_CreateDropTruncate_Table/"/>
    <url>/2020/06/12/Hive_CreateDropTruncate_Table/</url>
    
    <content type="html"><![CDATA[<h3 id="Create-Table"><a href="#Create-Table" class="headerlink" title="Create Table"></a>Create Table</h3><pre><code class="hljs mysql">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])]  [COMMENT table_comment]  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)     [STORED AS DIRECTORIES]  [   [ROW FORMAT row_format]    [STORED AS file_format]     | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)  ]  [LOCATION hdfs_path]  [TBLPROPERTIES (property_name&#x3D;property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</code></pre><ol><li><p>CREATE TABLE 创建一个指定名字的表，如果一个同名的表或者视图已经存在，则会抛出异常，加上IF NOT EXISTS时，如果表已经存在，Hive就会忽略掉后面的执行语句，而且不会有任何提示。</p><ul><li><p>IF NOT EXISTS只比较表名，不比较表的模式。</p><blockquote><p>如果用户使用了IF NOT EXISTS，当用户所指定的表的模式和已经存在的这个表的模式不同的话，Hive不会为此做出提示。如果用户的意图是使这个表具有重新指定的那个新的模式的话，那么就需要先删除这个表，也就是丢弃之前的数据，然后再重建这张表。用户可以考虑使用一个或多个ALTER TABLE语句来修改已经存在的表的结构。</p></blockquote></li></ul></li><li><p>表名和列名不区分大小写，但SerDe和属性名区分大小写。</p><ol><li>在Hive 0.12和更早版本中，表和列名称中仅允许使用字母数字和下划线。在Hive 0.13之后，列名可以使用任何Unicode字符，但是点(.)和冒号(:)在查询时候会报错，所以在Hive 1.2.0中不允许使用它们</li><li>反引号（``）中指定的任何列名均按字面意义处理。在反引号字符串中，使用双反引号（```）表示反引号字符。反引号还可以将保留关键字用于表和列标识符。</li><li>如果要限制列名只能为字母数字和下划线字符，可以将将配置属性设置<code>hive.support.quoted.identifiers</code>为<code>none</code>。在此配置中，带反引号的名称被解释为正则表达式。</li></ol></li><li><p>不加EXTERNAL创建的表是管理表，hive管理自己的数据，要确认一个表是管理表还是外部表，请在DESCRIBE EXTENDED table_name的输出中查找tableType 。</p></li><li><p>TBLPROPERTIES可以用键值对形式，给表添加额外的一些说明。</p><p>常见用法：</p><ul><li><p>指定压缩格式：TBLPROPERTIES（“ orc.compress” =“ ZLIB”）或（“ orc.compress” =“ SNAPPY”）或（“ orc.compress” =“ NONE”</p><blockquote><p>Hive会自动增加两个表属性：一个是last_modified_by，其保存着最后修改这个表的用户的用户名；另一个是last_modified_time，其保存着最后一次修改的新纪元时间秒。</p></blockquote></li><li><p>查看TBLPROPERTIES属性：show TBLPROPERTIES table_name；</p></li></ul></li><li><p>查看表的详细信息</p><ul><li>desc  [extended|formatted]  table_name;</li><li>formatted打印的表信息比extended的更加详细，而且阅读性更强。</li></ul></li><li><p>查看某一列的详细信息</p><ul><li>desc  [extended|formatted]  table_name.column_name;</li></ul></li></ol><h2 id="Managed-and-External-Tables"><a href="#Managed-and-External-Tables" class="headerlink" title="Managed and External Tables"></a>Managed and External Tables</h2><p>hive的表从本质上分两种类型：</p><ul><li>管理表</li><li>外部表</li></ul><h3 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h3><ol><li>默认创建的表，或者是Managed修饰的表是管理表，元数据，文件，统计信息等由Hive自己管理，即Hive控制着数据的生命周期，当我们删除一个管理表时候，表中的数据也会被删除。</li><li>管理表的数据存储在属性<code>hive.metastore.warehouse.dir</code>指定的路径下，默认情况下存储在类似/user/hive/warehouse/databasename.db/tablename/的路径下。在创建表的时候，如果指定location的位置，就可以覆盖默认位置。</li></ol><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><ol><li>外部表描述外部文件的元数据信息，外部表的文件可以由Hive之外的工具管理，外部表可以访问存储在例如Azure存储卷（ASV）或远程HDFS位置的源中的数据。如果更改了外部表的结构或分区，则可以使用MSCK REPAIR TABLE table_name语句刷新元数据信息。</li><li>删除外部表不会删除表中的数据，只会删除掉描述表的元数据信息。</li></ol><h3 id="区别与应用"><a href="#区别与应用" class="headerlink" title="区别与应用"></a>区别与应用</h3><ol><li>管理表拥有对表中数据的管理权，而外部表没有，删除管理表，元数据信息和表中的数据都会被删除，删除外部表只删除元数据信息，表中的数据不会被删除。</li><li>需要Hive需要管理表的生命周期或生成临时表时候，使用管理表，管理表不适合和其他工具共享数据。</li><li>当表中的文件已经存在时候，使用外部表，即使表被删除，文件也会保留。</li></ol><h3 id="管理表和外部表转换"><a href="#管理表和外部表转换" class="headerlink" title="管理表和外部表转换"></a>管理表和外部表转换</h3><ol><li><p>内部表转外部表</p><p>alter table table_name set tblproperties(‘EXTERNAL’=’TRUE’);</p></li><li><p>外部表转内部表</p><p>alter table table_name set tblproperties(‘EXTERNAL’=’FALSE’);</p></li><li><p>查看是外部表还是管理表</p><pre><code class="hljs gams">hive&gt; desc formatted table_name;<span class="hljs-keyword">Table</span> Type:       MANAGED_TABLE    //管理表<span class="hljs-keyword">Table</span> Type:       EXTERNAL_TABLE   //外部表</code></pre></li></ol><p><strong>注：</strong>(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写</p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/06/12/hello-world/"/>
    <url>/2020/06/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="hljs bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="hljs bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="hljs bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
