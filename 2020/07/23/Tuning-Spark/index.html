<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <title>Spark系列（五）—— Tuning Spark - ruixinyue</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>ruixinyue</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
                Spark系列（五）—— Tuning Spark
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2020-07-23 14:01">
      July 23, 2020 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      7.8k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      113
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h1 id="Tuning-Spark"><a href="#Tuning-Spark" class="headerlink" title="Tuning Spark"></a>Tuning Spark</h1><p>Because of the in-memory nature of most Spark computations, Spark programs can be bottlenecked by any resource in the cluster: CPU, network bandwidth, or memory. Most often, if the data fits in memory, the bottleneck is network bandwidth, but sometimes, you also need to do some tuning, such as <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">storing RDDs in serialized form</a>, to decrease memory usage. This guide will cover two main topics: data serialization, which is crucial for good network performance and can also reduce memory use, and memory tuning. We also sketch several smaller topics.</p>
<h2 id="1-Data-Serialization"><a href="#1-Data-Serialization" class="headerlink" title="1. Data Serialization"></a>1. Data Serialization</h2><p>Serialization plays an important role in the performance of any distributed application. Formats that are slow to serialize objects into, or consume a large number of bytes, will greatly slow down the computation. Often, this will be the first thing you should tune to optimize a Spark application. Spark aims to strike a balance between convenience (allowing you to work with any Java type in your operations) and performance. It provides two serialization libraries:</p>
<p>序列化在任何分布式应用程序的性能中都起着重要作用。将对象序列化为慢速格式或占用大量字节的格式将大大减慢计算速度。通常，这是您应该优化Spark应用程序的第一件事。Spark旨在在便利性（允许您在操作中使用任何Java类型）和性能之间取得平衡。它提供了两个序列化库：</p>
<ul>
<li><a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html" target="_blank" rel="noopener">Java serialization</a>: By default, Spark serializes objects using Java’s <code>ObjectOutputStream</code> framework, and can work with any class you create that implements <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html" target="_blank" rel="noopener"><code>java.io.Serializable</code></a>. You can also control the performance of your serialization more closely by extending <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Externalizable.html" target="_blank" rel="noopener"><code>java.io.Externalizable</code></a>. Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.</li>
<li><a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html" target="_blank" rel="noopener">Java序列化</a>：默认情况下，Spark使用Java的<code>ObjectOutputStream</code>框架对对象进行序列化，并且可以与您创建的实现的任何类一起使用 <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html" target="_blank" rel="noopener"><code>java.io.Serializable</code></a>。您还可以通过扩展来更紧密地控制序列化的性能 <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Externalizable.html" target="_blank" rel="noopener"><code>java.io.Externalizable</code></a>。Java序列化很灵活，但是通常很慢，并且导致许多类的序列化格式很大。</li>
<li><a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">Kryo serialization</a>: Spark can also use the Kryo library (version 2) to serialize objects more quickly. Kryo is significantly faster and more compact than Java serialization (often as much as 10x), but does not support all <code>Serializable</code> types and requires you to <em>register</em> the classes you’ll use in the program in advance for best performance.</li>
<li><a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">Kryo序列化</a>：Spark还可以使用Kryo库（版本2）更快地序列化对象。与Java序列化（通常多达10倍）相比，Kryo显着更快，更紧凑，但是Kryo不支持所有 <code>Serializable</code>类型，并且需要您预先<em>注册</em>要在程序中使用的类才能获得最佳性能。</li>
</ul>
<p>You can switch to using Kryo by initializing your job with a <a href="http://spark.apache.org/docs/2.1.2/configuration.html#spark-properties" target="_blank" rel="noopener">SparkConf</a> and calling <code>conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</code>. This setting configures the serializer used for not only shuffling data between worker nodes but also when serializing RDDs to disk. The only reason Kryo is not the default is because of the custom registration requirement, but we recommend trying it in any network-intensive application. Since Spark 2.0.0, we internally use Kryo serializer when shuffling RDDs with simple types, arrays of simple types, or string type.</p>
<p>Spark automatically includes Kryo serializers for the many commonly-used core Scala classes covered in the AllScalaRegistrar from the <a href="https://github.com/twitter/chill" target="_blank" rel="noopener">Twitter chill</a> library.</p>
<p>您可以通过使用<a href="http://spark.apache.org/docs/2.1.2/configuration.html#spark-properties" target="_blank" rel="noopener">SparkConf</a>初始化作业 并调用来切换为使用Kryo <code>conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</code>。此设置配置了串行器，该串行器不仅用于在工作程序节点之间改组数据，而且还用于将RDD序列化到磁盘时。Kryo不是默认值的唯一原因是由于自定义注册要求，但是我们建议在任何网络密集型应用程序中尝试使用它。从Spark 2.0.0开始，在将RDD与简单类型，简单类型的数组或字符串类型进行混洗时，我们在内部使用Kryo序列化器。</p>
<p>Spark自动为<a href="https://github.com/twitter/chill" target="_blank" rel="noopener">Twitter chill</a>库的AllScalaRegistrar中涵盖的许多常用Scala核心类包括Kryo序列化器。</p>
<p>To register your own custom classes with Kryo, use the <code>registerKryoClasses</code> method.</p>
<p>要向Kryo注册您自己的自定义类，请使用<code>registerKryoClasses</code>方法。</p>
<pre><code class="hljs scala"><span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(...).setAppName(...)
conf.registerKryoClasses(<span class="hljs-type">Array</span>(classOf[<span class="hljs-type">MyClass1</span>], classOf[<span class="hljs-type">MyClass2</span>]))
<span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)</code></pre>

<p>The <a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">Kryo documentation</a> describes more advanced registration options, such as adding custom serialization code.</p>
<p>If your objects are large, you may also need to increase the <code>spark.kryoserializer.buffer</code> <a href="http://spark.apache.org/docs/2.1.2/configuration.html#compression-and-serialization" target="_blank" rel="noopener">config</a>. This value needs to be large enough to hold the <em>largest</em> object you will serialize.</p>
<p>Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.</p>
<p>所述<a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">KRYO文档</a>描述了更先进的注册选项，如添加自定义序列的代码。</p>
<p>如果对象很大，则可能还需要增加<code>spark.kryoserializer.buffer</code> <a href="http://spark.apache.org/docs/2.1.2/configuration.html#compression-and-serialization" target="_blank" rel="noopener">config</a>。该值必须足够大以容纳要序列化的<em>最大</em>对象。</p>
<table>
<thead>
<tr>
<th><code>spark.kryoserializer.buffer.max</code></th>
<th>64m</th>
<th>Maximum allowable size of Kryo serialization buffer. This must be larger than any object you attempt to serialize. Increase this if you get a “buffer limit exceeded” exception inside Kryo.</th>
</tr>
</thead>
<tbody><tr>
<td><code>spark.kryoserializer.buffer</code></td>
<td>64k</td>
<td>Initial size of Kryo’s serialization buffer. Note that there will be one buffer <em>per core</em> on each worker. This buffer will grow up to <code>spark.kryoserializer.buffer.max</code> if needed.</td>
</tr>
</tbody></table>
<p>最后，如果您不注册自定义类，Kryo仍然可以工作，但必须将完整的类名与每个对象一起存储，这很浪费。</p>
<h2 id="2-Memory-Tuning"><a href="#2-Memory-Tuning" class="headerlink" title="2. Memory Tuning"></a>2. Memory Tuning</h2><p>There are three considerations in tuning memory usage: the <em>amount</em> of memory used by your objects (you may want your entire dataset to fit in memory), the <em>cost</em> of accessing those objects, and the overhead of <em>garbage collection</em> (if you have high turnover in terms of objects).</p>
<p>By default, Java objects are fast to access, but can easily consume a factor of 2-5x more space than the “raw” data inside their fields. This is due to several reasons:</p>
<ul>
<li>Each distinct Java object has an “object header”, which is about 16 bytes and contains information such as a pointer to its class. For an object with very little data in it (say one <code>Int</code> field), this can be bigger than the data.</li>
<li>Java <code>String</code>s have about 40 bytes of overhead over the raw string data (since they store it in an array of <code>Char</code>s and keep extra data such as the length), and store each character as <em>two</em> bytes due to <code>String</code>’s internal usage of UTF-16 encoding. Thus a 10-character string can easily consume 60 bytes.</li>
<li>Common collection classes, such as <code>HashMap</code> and <code>LinkedList</code>, use linked data structures, where there is a “wrapper” object for each entry (e.g. <code>Map.Entry</code>). This object not only has a header, but also pointers (typically 8 bytes each) to the next object in the list.</li>
<li>Collections of primitive types often store them as “boxed” objects such as <code>java.lang.Integer</code>.</li>
</ul>
<p>This section will start with an overview of memory management in Spark, then discuss specific strategies the user can take to make more efficient use of memory in his/her application. In particular, we will describe how to determine the memory usage of your objects, and how to improve it – either by changing your data structures, or by storing data in a serialized format. We will then cover tuning Spark’s cache size and the Java garbage collector.</p>
<p>有三个方面的考虑在调整内存使用：该<em>量</em>的存储你的对象所使用的（你可能希望你的整个数据集，以适应在内存中），则<em>成本</em>访问这些对象，并且开销<em>垃圾收集</em>（如果你有高成交条款）。</p>
<p>默认情况下，Java对象的访问速度很快，但是与其字段内的“原始”数据相比，它们很容易消耗2-5倍的空间。这是由于以下几个原因：</p>
<ul>
<li>每个不同的Java对象都有一个“对象标头”，大约16个字节，并包含诸如指向其类的指针之类的信息。对于其中数据很少的对象（例如一个<code>Int</code>字段），该对象可能大于数据。</li>
<li>Java <code>String</code>相对于原始字符串数据有大约40个字节的开销（因为它们将其存储在<code>Char</code>s 数组中并保留诸如长度之类的额外数据），并且由于UTF-16的内部用法，因此每个字符都存储为<em>两个</em>字节<code>String</code>编码。因此，一个10个字符的字符串可以轻松消耗60个字节。</li>
<li>诸如<code>HashMap</code>和的通用集合类<code>LinkedList</code>使用链接的数据结构，其中每个条目（例如<code>Map.Entry</code>）都有一个“包装”对象。该对象不仅具有标题，而且具有指向列表中下一个对象的指针（通常每个指针8个字节）。</li>
<li>基本类型的集合通常将它们存储为“盒装”对象，例如<code>java.lang.Integer</code>。</li>
</ul>
<p>本节将首先概述Spark中的内存管理，然后讨论用户可以采取的特定策略，以更有效地使用其应用程序中的内存。特别是，我们将描述如何确定对象的内存使用以及如何通过更改数据结构或以串行化格式存储数据来改善对象的使用情况。然后，我们将介绍调整Spark的缓存大小和Java垃圾收集器。</p>
<h3 id="2-1-Memory-Management-Overview"><a href="#2-1-Memory-Management-Overview" class="headerlink" title="2.1 Memory Management Overview"></a>2.1 Memory Management Overview</h3><p>Memory usage in Spark largely falls under one of two categories: execution and storage. Execution memory refers to that used for computation in shuffles, joins, sorts and aggregations, while storage memory refers to that used for caching and propagating internal data across the cluster. In Spark, execution and storage share a unified region (M). When no execution memory is used, storage can acquire all the available memory and vice versa. Execution may evict storage if necessary, but only until total storage memory usage falls under a certain threshold (R). In other words, <code>R</code> describes a subregion within <code>M</code> where cached blocks are never evicted. Storage may not evict execution due to complexities in implementation.</p>
<p>Spark中的内存使用情况大体上属于以下两种类别之一：执行和存储。执行内存是指用于洗牌，联接，排序和聚合的计算的内存，而存储内存是指用于在集群中缓存和传播内部数据的内存。在Spark中，执行和存储共享一个统一的区域（M）。当不使用执行内存时，存储可以获取所有可用内存，反之亦然。如果有必要，执行可能会驱逐存储，但只有在存储总内存使用量下降到某个阈值（R）以下时，才可以执行该操作。换句话说，<code>R</code>描述了一个子区域，在该子区域<code>M</code>中永远不会移出缓存的块。由于实现的复杂性，存储可能无法退出执行。</p>
<p>This design ensures several desirable properties. First, applications that do not use caching can use the entire space for execution, obviating unnecessary disk spills. Second, applications that do use caching can reserve a minimum storage space (R) where their data blocks are immune to being evicted. Lastly, this approach provides reasonable out-of-the-box performance for a variety of workloads without requiring user expertise of how memory is divided internally.</p>
<p>这种设计确保了几种理想的性能。首先，不使用缓存的应用程序可以将整个空间用于执行，从而避免了不必要的磁盘溢出。其次，确实使用缓存的应用程序可以保留最小的存储空间（R），以免其数据块被逐出。最后，这种方法可为各种工作负载提供合理的即用即用性能，而无需用户了解如何在内部划分内存。</p>
<p>Although there are two relevant configurations, the typical user should not need to adjust them as the default values are applicable to most workloads:</p>
<ul>
<li><code>spark.memory.fraction</code> expresses the size of <code>M</code> as a fraction of the (JVM heap space - 300MB) (default 0.6). The rest of the space (40%) is reserved for user data structures, internal metadata in Spark, and safeguarding against OOM errors in the case of sparse and unusually large records.</li>
<li><code>spark.memory.storageFraction</code> expresses the size of <code>R</code> as a fraction of <code>M</code> (default 0.5). <code>R</code> is the storage space within <code>M</code> where cached blocks immune to being evicted by execution.</li>
</ul>
<p>尽管有两种相关的配置，但典型用户无需调整它们，因为默认值适用于大多数工作负载：</p>
<ul>
<li><code>spark.memory.fraction</code>表示的大小<code>M</code>为（JVM堆空间-300MB）的一部分（默认值为0.6）。其余的空间（40％）保留用于用户数据结构，Spark中的内部元数据，并在记录稀疏和异常大的情况下防止OOM错误。</li>
<li><code>spark.memory.storageFraction</code>将的大小表示<code>R</code>为的分数<code>M</code>（默认值为0.5）。 <code>R</code>是<code>M</code>其中的缓存块不受执行影响而退出的存储空间。</li>
</ul>
<p>The value of <code>spark.memory.fraction</code> should be set in order to fit this amount of heap space comfortably within the JVM’s old or “tenured” generation. See the discussion of advanced GC tuning below for details.</p>
<p><code>spark.memory.fraction</code>应该设置的值，以便在JVM的旧版本或“长期使用的”一代中舒适地适应此堆空间量。有关详细信息，请参见下面有关高级GC调整的讨论。</p>
<h3 id="2-2-Determining-Memory-Consumption"><a href="#2-2-Determining-Memory-Consumption" class="headerlink" title="2.2 Determining Memory Consumption"></a>2.2 Determining Memory Consumption</h3><p>The best way to size the amount of memory consumption a dataset will require is to create an RDD, put it into cache, and look at the “Storage” page in the web UI. The page will tell you how much memory the RDD is occupying.</p>
<p>To estimate the memory consumption of a particular object, use <code>SizeEstimator</code>’s <code>estimate</code> method This is useful for experimenting with different data layouts to trim memory usage, as well as determining the amount of space a broadcast variable will occupy on each executor heap.</p>
<p>确定数据集所需的内存消耗量的最佳方法是创建一个RDD，将其放入缓存中，然后查看Web UI中的“ Storage”页面。该页面将告诉您RDD占用了多少内存。</p>
<p>要估算特定对象的内存使用量，请使用<code>SizeEstimator</code>的<code>estimate</code>方法。此方法可用于试验不同的数据布局以减少内存使用量，并确定广播变量将在每个执行程序堆上占用的空间量。</p>
<h3 id="2-3-Tuning-Data-Structures"><a href="#2-3-Tuning-Data-Structures" class="headerlink" title="2.3 Tuning Data Structures"></a>2.3 Tuning Data Structures</h3><p>The first way to reduce memory consumption is to avoid the Java features that add overhead, such as pointer-based data structures and wrapper objects. There are several ways to do this:</p>
<ol>
<li>Design your data structures to prefer arrays of objects, and primitive types, instead of the standard Java or Scala collection classes (e.g. <code>HashMap</code>). The <a href="http://fastutil.di.unimi.it/" target="_blank" rel="noopener">fastutil</a> library provides convenient collection classes for primitive types that are compatible with the Java standard library.</li>
<li>Avoid nested structures with a lot of small objects and pointers when possible.</li>
<li>Consider using numeric IDs or enumeration objects instead of strings for keys.</li>
<li>If you have less than 32 GB of RAM, set the JVM flag <code>-XX:+UseCompressedOops</code> to make pointers be four bytes instead of eight. You can add these options in <a href="http://spark.apache.org/docs/2.1.2/configuration.html#environment-variables" target="_blank" rel="noopener"><code>spark-env.sh</code></a>.</li>
</ol>
<p>减少内存消耗的第一种方法是避免使用Java功能，这些功能会增加开销，例如基于指针的数据结构和包装对象。做这件事有很多种方法：</p>
<ol>
<li>设计数据结构，使其更喜欢对象数组和原始类型，而不是标准的Java或Scala集合类（例如<code>HashMap</code>）。该<a href="http://fastutil.di.unimi.it/" target="_blank" rel="noopener">fastutil</a> 库提供方便的集合类基本类型是与Java标准库兼容。</li>
<li>尽可能避免使用带有许多小对象和指针的嵌套结构。</li>
<li>考虑使用数字ID或枚举对象代替键的字符串。</li>
<li>如果您的RAM少于32 GB，则设置JVM标志<code>-XX:+UseCompressedOops</code>以使指针为四个字节而不是八个字节。您可以在中添加这些选项 <a href="http://spark.apache.org/docs/2.1.2/configuration.html#environment-variables" target="_blank" rel="noopener"><code>spark-env.sh</code></a>。</li>
</ol>
<h3 id="2-4-Serialized-RDD-Storage"><a href="#2-4-Serialized-RDD-Storage" class="headerlink" title="2.4 Serialized RDD Storage"></a>2.4 Serialized RDD Storage</h3><p>When your objects are still too large to efficiently store despite this tuning, a much simpler way to reduce memory usage is to store them in <em>serialized</em> form, using the serialized StorageLevels in the <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">RDD persistence API</a>, such as <code>MEMORY_ONLY_SER</code>. Spark will then store each RDD partition as one large byte array. The only downside of storing data in serialized form is slower access times, due to having to deserialize each object on the fly. We highly recommend <a href="http://spark.apache.org/docs/2.1.2/tuning.html#data-serialization" target="_blank" rel="noopener">using Kryo</a> if you want to cache data in serialized form, as it leads to much smaller sizes than Java serialization (and certainly than raw Java objects).</p>
<p>当您的对象仍然太大而无法进行优化存储时，减少内存使用的一种更简单的方法是使用<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">RDD持久性API中</a>的序列化StorageLevels （例如）以<em>序列化</em>形式存储它们。然后，Spark将每个RDD分区存储为一个大字节数组。由于必须动态地反序列化每个对象，因此以串行形式存储数据的唯一缺点是访问时间较慢。如果您要缓存序列化形式的数据，我们强烈建议<a href="http://spark.apache.org/docs/2.1.2/tuning.html#data-serialization" target="_blank" rel="noopener">使用Kryo</a>，因为它导致的大小比Java序列化（当然也比原始Java对象）小。</p>
<h3 id="2-5-Garbage-Collection-Tuning"><a href="#2-5-Garbage-Collection-Tuning" class="headerlink" title="2.5 Garbage Collection Tuning"></a>2.5 Garbage Collection Tuning</h3><p>JVM garbage collection can be a problem when you have large “churn” in terms of the RDDs stored by your program. (It is usually not a problem in programs that just read an RDD once and then run many operations on it.) When Java needs to evict old objects to make room for new ones, it will need to trace through all your Java objects and find the unused ones. The main point to remember here is that <em>the cost of garbage collection is proportional to the number of Java objects</em>, so using data structures with fewer objects (e.g. an array of <code>Int</code>s instead of a <code>LinkedList</code>) greatly lowers this cost. An even better method is to persist objects in serialized form, as described above: now there will be only <em>one</em> object (a byte array) per RDD partition. Before trying other techniques, the first thing to try if GC is a problem is to use <a href="http://spark.apache.org/docs/2.1.2/tuning.html#serialized-rdd-storage" target="_blank" rel="noopener">serialized caching</a>.</p>
<p>GC can also be a problem due to interference between your tasks’ working memory (the amount of space needed to run the task) and the RDDs cached on your nodes. We will discuss how to control the space allocated to the RDD cache to mitigate this.</p>
<p>当您在程序存储的RDD方面有较大的“搅动”时，JVM垃圾回收可能会成为问题。（在只读取一次RDD然后对其执行许多操作的程序中，这通常不是问题。）当Java需要逐出旧对象以为新对象腾出空间时，它将需要遍历所有Java对象并查找未使用的。这里要记住的要点是，<em>垃圾回收的成本与Java对象的数量成正比</em>，因此使用具有较少对象的数据结构（例如<code>Int</code>s而不是a 的数组<code>LinkedList</code>）可以大大降低此成本。更好的方法是如上所述以序列化形式持久化对象：现在将只有<em>一个</em>每个RDD分区的对象（字节数组）。在尝试其他技术之前，尝试尝试GC是否有问题的第一件事是使用<a href="http://spark.apache.org/docs/2.1.2/tuning.html#serialized-rdd-storage" target="_blank" rel="noopener">序列化缓存</a>。</p>
<p>由于任务的工作内存（运行任务所需的空间量）与节点上缓存的RDD之间的干扰，GC也会成为问题。我们将讨论如何控制分配给RDD缓存的空间以减轻这种情况。</p>
<p><strong>Measuring the Impact of GC</strong></p>
<p>The first step in GC tuning is to collect statistics on how frequently garbage collection occurs and the amount of time spent GC. This can be done by adding <code>-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps</code> to the Java options. (See the <a href="http://spark.apache.org/docs/2.1.2/configuration.html#Dynamically-Loading-Spark-Properties" target="_blank" rel="noopener">configuration guide</a> for info on passing Java options to Spark jobs.) Next time your Spark job is run, you will see messages printed in the worker’s logs each time a garbage collection occurs. Note these logs will be on your cluster’s worker nodes (in the <code>stdout</code> files in their work directories), <em>not</em> on your driver program.</p>
<p><strong>衡量GC的影响</strong></p>
<p>GC调整的第一步是收集有关垃圾收集发生频率和GC使用时间的统计信息。这可以通过添加<code>-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps</code>Java选项来完成。（有关将Java选项传递给Spark作业的信息，请参阅<a href="http://spark.apache.org/docs/2.1.2/configuration.html#Dynamically-Loading-Spark-Properties" target="_blank" rel="noopener">配置指南</a>。）下次运行Spark作业时，每次发生垃圾收集时，您都会在工作日志中看到打印的消息。请注意，这些日志将位于群集的工作节点上（<code>stdout</code>位于其工作目录中的文件中），<em>而不</em>位于驱动程序上。</p>
<p><strong>Advanced GC Tuning</strong></p>
<p>To further tune garbage collection, we first need to understand some basic information about memory management in the JVM:</p>
<ul>
<li>Java Heap space is divided in to two regions Young and Old. The Young generation is meant to hold short-lived objects while the Old generation is intended for objects with longer lifetimes.</li>
<li>The Young generation is further divided into three regions [Eden, Survivor1, Survivor2].</li>
<li>A simplified description of the garbage collection procedure: When Eden is full, a minor GC is run on Eden and objects that are alive from Eden and Survivor1 are copied to Survivor2. The Survivor regions are swapped. If an object is old enough or Survivor2 is full, it is moved to Old. Finally when Old is close to full, a full GC is invoked.</li>
</ul>
<p>The goal of GC tuning in Spark is to ensure that only long-lived RDDs are stored in the Old generation and that the Young generation is sufficiently sized to store short-lived objects. This will help avoid full GCs to collect temporary objects created during task execution. Some steps which may be useful are:</p>
<ul>
<li>Check if there are too many garbage collections by collecting GC stats. If a full GC is invoked multiple times for before a task completes, it means that there isn’t enough memory available for executing tasks.</li>
<li>If there are too many minor collections but not many major GCs, allocating more memory for Eden would help. You can set the size of the Eden to be an over-estimate of how much memory each task will need. If the size of Eden is determined to be <code>E</code>, then you can set the size of the Young generation using the option <code>-Xmn=4/3*E</code>. (The scaling up by 4/3 is to account for space used by survivor regions as well.)</li>
<li>In the GC stats that are printed, if the OldGen is close to being full, reduce the amount of memory used for caching by lowering <code>spark.memory.fraction</code>; it is better to cache fewer objects than to slow down task execution. Alternatively, consider decreasing the size of the Young generation. This means lowering <code>-Xmn</code> if you’ve set it as above. If not, try changing the value of the JVM’s <code>NewRatio</code> parameter. Many JVMs default this to 2, meaning that the Old generation occupies 2/3 of the heap. It should be large enough such that this fraction exceeds <code>spark.memory.fraction</code>.</li>
<li>Try the G1GC garbage collector with <code>-XX:+UseG1GC</code>. It can improve performance in some situations where garbage collection is a bottleneck. Note that with large executor heap sizes, it may be important to increase the <a href="https://blogs.oracle.com/g1gc/entry/g1_gc_tuning_a_case" target="_blank" rel="noopener">G1 region size</a> with <code>-XX:G1HeapRegionSize</code></li>
<li>As an example, if your task is reading data from HDFS, the amount of memory used by the task can be estimated using the size of the data block read from HDFS. Note that the size of a decompressed block is often 2 or 3 times the size of the block. So if we wish to have 3 or 4 tasks’ worth of working space, and the HDFS block size is 128 MB, we can estimate size of Eden to be <code>4*3*128MB</code>.</li>
<li>Monitor how the frequency and time taken by garbage collection changes with the new settings.</li>
</ul>
<p>Our experience suggests that the effect of GC tuning depends on your application and the amount of memory available. There are <a href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" target="_blank" rel="noopener">many more tuning options</a> described online, but at a high level, managing how frequently full GC takes place can help in reducing the overhead.</p>
<p>GC tuning flags for executors can be specified by setting <code>spark.executor.extraJavaOptions</code> in a job’s configuration.</p>
<p><strong>高级GC调整</strong></p>
<p>为了进一步调整垃圾回收，我们首先需要了解有关JVM中内存管理的一些基本信息：</p>
<ul>
<li>Java Heap空间分为Young和Old两个区域。年轻一代用于保存寿命短的对象，而老一代则用于寿命更长的对象。</li>
<li>年轻一代又分为三个区域[Eden，Survivor1，Survivor2]。</li>
<li>垃圾收集过程的简化描述：当Eden已满时，将在Eden上运行次要GC，并将来自Eden和Survivor1的活动对象复制到Survivor2。幸存者区域被交换。如果对象足够旧或Survivor2已满，则将其移到“旧”。最后，当Old接近满时，将调用完整的GC。</li>
</ul>
<p>在Spark中进行GC调整的目的是确保仅将长寿命的RDD存储在旧一代中，并确保年轻一代的大小足以存储短寿命的对象。这将有助于避免完整的GC收集任务执行期间创建的临时对象。可能有用的一些步骤是：</p>
<ul>
<li>通过收集GC统计信息检查是否有太多垃圾回收。如果在任务完成之前多次调用一个完整的GC，则意味着没有足够的内存来执行任务。</li>
<li>如果次要集合太多，但主要GC却没有很多，那么为Eden分配更多的内存将有所帮助。您可以将Eden的大小设置为高估每个任务所需的内存量。如果确定Eden的大小为<code>E</code>，则可以使用选项设置Young一代的大小<code>-Xmn=4/3*E</code>。（按4/3比例放大也是为了考虑幸存者区域使用的空间。）</li>
<li>在打印的GC统计信息中，如果OldGen即将满，请通过降低来减少用于缓存的内存量<code>spark.memory.fraction</code>；最好缓存较少的对象，而不是减慢任务的执行。或者，考虑减小Young代的大小。<code>-Xmn</code>如果您如上所述进行设置，则意味着降低。如果不是，请尝试更改JVM <code>NewRatio</code>参数的值。许多JVM将此默认值设置为2，这意味着旧代占据了堆的2/3。它应该足够大以使该分数超过<code>spark.memory.fraction</code>。</li>
<li>使用尝试G1GC垃圾收集器<code>-XX:+UseG1GC</code>。在垃圾收集成为瓶颈的某些情况下，它可以提高性能。需要注意的是大执行人堆大小，可能重要的是增加了<a href="https://blogs.oracle.com/g1gc/entry/g1_gc_tuning_a_case" target="_blank" rel="noopener">G1区域大小</a> 与<code>-XX:G1HeapRegionSize</code></li>
<li>例如，如果您的任务是从HDFS读取数据，则可以使用从HDFS读取的数据块的大小来估算任务使用的内存量。注意，解压缩块的大小通常是块大小的2或3倍。因此，如果我们希望拥有3或4个任务的工作空间，并且HDFS块大小为128 MB，则我们可以将Eden的大小估计为<code>4*3*128MB</code>。</li>
<li>使用新设置监视垃圾回收所花费的频率和时间如何变化。</li>
</ul>
<p>我们的经验表明，GC调整的效果取决于您的应用程序和可用内存量。有<a href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" target="_blank" rel="noopener">更多的微调选项</a>描述联机，但在较高的水平，管理GC如何充分频繁发生可以帮助减少开销。</p>
<p>可以通过<code>spark.executor.extraJavaOptions</code>在作业的配置中设置来指定执行程序的GC调整标志。</p>
<h1 id="Other-Considerations"><a href="#Other-Considerations" class="headerlink" title="Other Considerations"></a>Other Considerations</h1><h2 id="Level-of-Parallelism"><a href="#Level-of-Parallelism" class="headerlink" title="Level of Parallelism"></a>Level of Parallelism</h2><p>Clusters will not be fully utilized unless you set the level of parallelism for each operation high enough. Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to <code>SparkContext.textFile</code>, etc), and for distributed “reduce” operations, such as <code>groupByKey</code> and <code>reduceByKey</code>, it uses the largest parent RDD’s number of partitions. You can pass the level of parallelism as a second argument (see the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener"><code>spark.PairRDDFunctions</code></a> documentation), or set the config property <code>spark.default.parallelism</code> to change the default. In general, we recommend 2-3 tasks per CPU core in your cluster.</p>
<h2 id="并行度"><a href="#并行度" class="headerlink" title="并行度"></a>并行度</h2><p>除非您为每个操作设置足够高的并行度，否则群集将无法充分利用。Spark根据文件的大小自动设置要在每个文件上运行的“映射”任务的数量（尽管您可以通过可选的参数来控制它<code>SparkContext.textFile</code>，等等），并且对于分布式“减少”操作（例如<code>groupByKey</code>和）<code>reduceByKey</code>，它使用最大的父文件RDD的分区数。您可以将并行性级别作为第二个参数传递（请参阅<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener"><code>spark.PairRDDFunctions</code></a>文档），或将config属性设置<code>spark.default.parallelism</code>为更改默认值。通常，我们建议您群集中的每个CPU内核执行2-3个任务。</p>
<h2 id="Memory-Usage-of-Reduce-Tasks"><a href="#Memory-Usage-of-Reduce-Tasks" class="headerlink" title="Memory Usage of Reduce Tasks"></a>Memory Usage of Reduce Tasks</h2><p>Sometimes, you will get an OutOfMemoryError not because your RDDs don’t fit in memory, but because the working set of one of your tasks, such as one of the reduce tasks in <code>groupByKey</code>, was too large. Spark’s shuffle operations (<code>sortByKey</code>, <code>groupByKey</code>, <code>reduceByKey</code>, <code>join</code>, etc) build a hash table within each task to perform the grouping, which can often be large. The simplest fix here is to <em>increase the level of parallelism</em>, so that each task’s input set is smaller. Spark can efficiently support tasks as short as 200 ms, because it reuses one executor JVM across many tasks and it has a low task launching cost, so you can safely increase the level of parallelism to more than the number of cores in your clusters.</p>
<p>有时，您会收到OutOfMemoryError的原因不是因为您的RDD不能容纳在内存中，而是因为您的一项任务（例如中的reduce任务之一）的工作集<code>groupByKey</code>太大。斯巴克的整理操作（<code>sortByKey</code>，<code>groupByKey</code>，<code>reduceByKey</code>，<code>join</code>，等）建立每个任务中的哈希表来进行分组，而这往往是大的。这里最简单的解决方法是 <em>提高并行度</em>，以使每个任务的输入集更小。Spark可以高效地支持短至200 ms的任务，因为它可以在多个任务中重用一个执行器JVM，并且任务启动成本较低，因此您可以安全地将并行度提高到集群中内核的数量以上。</p>
<h2 id="Broadcasting-Large-Variables"><a href="#Broadcasting-Large-Variables" class="headerlink" title="Broadcasting Large Variables"></a>Broadcasting Large Variables</h2><p>Using the <a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#broadcast-variables" target="_blank" rel="noopener">broadcast functionality</a> available in <code>SparkContext</code> can greatly reduce the size of each serialized task, and the cost of launching a job over a cluster. If your tasks use any large object from the driver program inside of them (e.g. a static lookup table), consider turning it into a broadcast variable. Spark prints the serialized size of each task on the master, so you can look at that to decide whether your tasks are too large; in general tasks larger than about 20 KB are probably worth optimizing.</p>
<p>利用<a href="http://spark.apache.org/docs/2.1.2/programming-guide.html#broadcast-variables" target="_blank" rel="noopener">广播功能</a> 提供<code>SparkContext</code>可大大降低每个序列化任务的大小，并在集群启动作业的成本。如果您的任务使用驱动程序中的任何大对象（例如，静态查找表），请考虑将其转换为广播变量。Spark在主服务器上打印每个任务的序列化大小，因此您可以查看它来确定任务是否太大；通常，大约20 KB以上的任务可能值得优化。</p>
<h2 id="Data-Locality"><a href="#Data-Locality" class="headerlink" title="Data Locality"></a>Data Locality</h2><p>Data locality can have a major impact on the performance of Spark jobs. If data and the code that operates on it are together then computation tends to be fast. But if code and data are separated, one must move to the other. Typically it is faster to ship serialized code from place to place than a chunk of data because code size is much smaller than data. Spark builds its scheduling around this general principle of data locality.</p>
<p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data’s current location. In order from closest to farthest:</p>
<ul>
<li><code>PROCESS_LOCAL</code> data is in the same JVM as the running code. This is the best locality possible</li>
<li><code>NODE_LOCAL</code> data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than <code>PROCESS_LOCAL</code> because the data has to travel between processes</li>
<li><code>NO_PREF</code> data is accessed equally quickly from anywhere and has no locality preference</li>
<li><code>RACK_LOCAL</code> data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li>
<li><code>ANY</code> data is elsewhere on the network and not in the same rack</li>
</ul>
<p>Spark prefers to schedule all tasks at the best locality level, but this is not always possible. In situations where there is no unprocessed data on any idle executor, Spark switches to lower locality levels. There are two options: a) wait until a busy CPU frees up to start a task on data on the same server, or b) immediately start a new task in a farther away place that requires moving data there.</p>
<p>What Spark typically does is wait a bit in the hopes that a busy CPU frees up. Once that timeout expires, it starts moving the data from far away to the free CPU. The wait timeout for fallback between each level can be configured individually or all together in one parameter; see the <code>spark.locality</code> parameters on the <a href="http://spark.apache.org/docs/2.1.2/configuration.html#scheduling" target="_blank" rel="noopener">configuration page</a> for details. You should increase these settings if your tasks are long and see poor locality, but the default usually works well.</p>
<p>数据局部性可能会对Spark作业的性能产生重大影响。如果数据和对其进行操作的代码在一起，则计算速度往往会很快。但是，如果代码和数据是分开的，那么一个必须移到另一个。通常，从一个地方到另一个地方传送序列化代码要比块数据更快，因为代码大小比数据小得多。Spark围绕此数据本地性一般原则构建调度。</p>
<p>数据局部性是数据与处理它的代码之间的接近程度。根据数据的当前位置，可分为多个级别。从最远到最远的顺序：</p>
<ul>
<li><code>PROCESS_LOCAL</code>数据与正在运行的代码位于同一JVM中。这是最好的位置</li>
<li><code>NODE_LOCAL</code>数据在同一节点上。示例可能在同一节点上的HDFS中，或者在同一节点上的另一执行程序中。这比<code>PROCESS_LOCAL</code>由于数据必须在进程之间传输而要慢一些</li>
<li><code>NO_PREF</code> 可以从任何地方快速访问数据，并且不受位置限制</li>
<li><code>RACK_LOCAL</code>数据在同一服务器机架上。数据位于同一机架上的另一台服务器上，因此通常需要通过网络通过单个交换机进行发送</li>
<li><code>ANY</code> 数据在网络上的其他位置，而不是在同一机架中</li>
</ul>
<p>Spark倾向于在最佳位置级别安排所有任务，但这并不总是可能的。在任何空闲执行器上没有未处理的数据的情况下，Spark会切换到较低的本地级别。有两种选择：a）等待忙碌的CPU释放以在同一服务器上的数据上启动任务，或b）立即在需要将数据移动到更远的地方启动新任务。</p>
<p>Spark通常要做的是稍等一下，以期释放繁忙的CPU。一旦超时到期，它将开始将数据从较远的地方移至空闲的CPU。每个级别之间的回退等待超时可以单独配置，也可以一起配置在一个参数中。有关详细信息，请参见<a href="http://spark.apache.org/docs/2.1.2/configuration.html#scheduling" target="_blank" rel="noopener">配置页面</a><code>spark.locality</code>上的 参数。如果您的任务很长并且位置不佳，则应该增加这些设置，但是默认设置通常效果很好。</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>This has been a short guide to point out the main concerns you should know about when tuning a Spark application – most importantly, data serialization and memory tuning. For most programs, switching to Kryo serialization and persisting data in serialized form will solve most common performance issues. Feel free to ask on the <a href="https://spark.apache.org/community.html" target="_blank" rel="noopener">Spark mailing list</a> about other tuning best practices.</p>
<p>这是一个简短的指南，指出了在调整Spark应用程序时应了解的主要问题-最重要的是数据序列化和内存调整。对于大多数程序，切换到Kryo序列化并以序列化形式保留数据将解决大多数常见的性能问题。随时在 <a href="https://spark.apache.org/community.html" target="_blank" rel="noopener">Spark邮件列表中</a>询问其他调优最佳实践。</p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Spark/">Spark</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Spark/">Spark</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2020/07/23/Spark%E5%AE%9E%E7%8E%B0WordCount%E7%9A%844%E7%A7%8D%E6%96%B9%E5%BC%8F/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Spark实现WordCount的4种方式</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/07/21/Spark-SQL-DataFrames-and-Datasets-Guide/">
                        <span class="hidden-mobile">Spark系列（四）-- Spark SQL, DataFrames and Datasets Guide</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>





  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>




















</body>
</html>
