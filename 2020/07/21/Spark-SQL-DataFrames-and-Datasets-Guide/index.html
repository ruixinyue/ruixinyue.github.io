<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <title>Spark系列（四）-- Spark SQL, DataFrames and Datasets Guide - ruixinyue</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>ruixinyue</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
                Spark系列（四）-- Spark SQL, DataFrames and Datasets Guide
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2020-07-21 17:19">
      July 21, 2020 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      12.2k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      202
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h1 id="Spark-SQL-DataFrames-and-Datasets-Guide"><a href="#Spark-SQL-DataFrames-and-Datasets-Guide" class="headerlink" title="Spark SQL, DataFrames and Datasets Guide"></a>Spark SQL, DataFrames and Datasets Guide</h1><h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><p>Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API<strong>. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation.</strong> This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.</p>
<p>All of the examples on this page use sample data included in the Spark distribution and can be run in the <code>spark-shell</code>, <code>pyspark</code> shell, or <code>sparkR</code> shell.</p>
<p>Spark SQL是用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了有关数据结构和正在执行的计算的更多信息。在内部，Spark SQL使用这些额外的信息来执行额外的优化。与Spark SQL交互的方法有多种，包括SQL和Dataset API。计算结果时，将使用相同的执行引擎，而与要用来表达计算的API /语言无关。这种统一意味着开发人员可以轻松地在不同的API之间来回切换，从而提供最自然的方式来表达给定的转换。</p>
<p>此页面上的所有示例均使用Spark发行版中包含的示例数据，并且可以在<code>spark-shell</code>，<code>pyspark</code>shell或<code>sparkR</code>shell中运行。</p>
<h3 id="1-1-SQL"><a href="#1-1-SQL" class="headerlink" title="1.1 SQL"></a>1.1 SQL</h3><p>One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#hive-tables" target="_blank" rel="noopener">Hive Tables</a> section. When running SQL from within another programming language the results will be returned as a <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#datasets-and-dataframes" target="_blank" rel="noopener">Dataset/DataFrame</a>. You can also interact with the SQL interface using the <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#running-the-spark-sql-cli" target="_blank" rel="noopener">command-line</a> or over <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" target="_blank" rel="noopener">JDBC/ODBC</a>.</p>
<p>Spark SQL的一种用途是执行SQL查询。Spark SQL还可以用于从现有的Hive安装中读取数据。有关如何配置此功能的更多信息，请参考<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#hive-tables" target="_blank" rel="noopener">Hive Tables</a>部分。当从另一种编程语言中运行SQL时，结果将作为<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#datasets-and-dataframes" target="_blank" rel="noopener">Dataset / DataFrame</a>返回。您还可以使用<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#running-the-spark-sql-cli" target="_blank" rel="noopener">命令行</a> 或通过<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" target="_blank" rel="noopener">JDBC / ODBC</a>与SQL接口进行交互。</p>
<h3 id="1-2-Datasets-and-DataFrames"><a href="#1-2-Datasets-and-DataFrames" class="headerlink" title="1.2 Datasets and DataFrames"></a>1.2 Datasets and DataFrames</h3><p><strong>A Dataset is a distributed collection of data.</strong> Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#creating-datasets" target="_blank" rel="noopener">constructed</a> from JVM objects and then manipulated using functional transformations (<code>map</code>, <code>flatMap</code>, <code>filter</code>, etc.). The Dataset API is available in <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a> and <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally <code>row.columnName</code>). The case for R is similar.</p>
<p><strong>A DataFrame is a <em>Dataset</em> organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">sources</a> such as: structured data files, tables in Hive, external databases, or existing RDDs.</strong> The DataFrame API is available in Scala, Java, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>, and <a href="http://spark.apache.org/docs/2.1.2/api/R/index.html" target="_blank" rel="noopener">R</a>. In Scala and Java, a DataFrame is represented by a Dataset of <code>Row</code>s. In <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">the Scala API</a>, <code>DataFrame</code> is simply a type alias of <code>Dataset[Row]</code>. While, in <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java API</a>, users need to use <code>Dataset&lt;Row&gt;</code> to represent a <code>DataFrame</code>.</p>
<p>Throughout this document, we will often refer to Scala/Java Datasets of <code>Row</code>s as DataFrames.</p>
<p>数据集是数据的分布式集合。数据集是Spark 1.6中添加的新接口，它具有RDD的优点（强类型输入，使用强大的Lambda函数的能力）以及Spark SQL的优化执行引擎的优点。数据集可以<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#creating-datasets" target="_blank" rel="noopener">构成</a>从JVM对象，然后使用功能性的转换（操作<code>map</code>，<code>flatMap</code>，<code>filter</code>等等）。Dataset API在<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a>和 <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java中</a>可用。Python不支持Dataset API。但是由于Python的动态特性，Dataset API的许多优点已经可用（即，您可以自然地通过名称访问行的字段 <code>row.columnName</code>）。R的情况类似。</p>
<p>DataFrame是组织为命名列的<em>数据集</em>。从概念上讲，它等效于关系数据库中的表或R / Python中的数据框，但是在后台进行了更丰富的优化。可以从多种<a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">来源</a>构造DataFrame，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。DataFrame API在Scala，Java，<a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>和<a href="http://spark.apache.org/docs/2.1.2/api/R/index.html" target="_blank" rel="noopener">R中</a>可用。在Scala和Java中，DataFrame由的数据集表示<code>Row</code>。在<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala API中</a>，<code>DataFrame</code>仅是类型别名<code>Dataset[Row]</code>。而在<a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java API中</a>，用户需要使用<code>Dataset&lt;Row&gt;</code>来代表<code>DataFrame</code>。</p>
<p>在整个文档中，我们通常将的Scala / Java数据集<code>Row</code>称为DataFrames。</p>
<h2 id="2-Getting-Started"><a href="#2-Getting-Started" class="headerlink" title="2. Getting Started"></a>2. Getting Started</h2><h3 id="2-1-Starting-Point-SparkSession"><a href="#2-1-Starting-Point-SparkSession" class="headerlink" title="2.1 Starting Point: SparkSession"></a>2.1 Starting Point: SparkSession</h3><p>The entry point into all functionality in Spark is the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.SparkSession" target="_blank" rel="noopener"><code>SparkSession</code></a> class. To create a basic <code>SparkSession</code>, just use <code>SparkSession.builder()</code>:</p>
<pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span>

<span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>
  .builder()
  .appName(<span class="hljs-string">"Spark SQL basic example"</span>)
  .config(<span class="hljs-string">"spark.some.config.option"</span>, <span class="hljs-string">"some-value"</span>)
  .getOrCreate()

<span class="hljs-comment">// For implicit conversions like converting RDDs to DataFrames</span>
<span class="hljs-keyword">import</span> spark.implicits._</code></pre>

<p>Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo.</p>
<p><code>SparkSession</code> in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables. To use these features, you do not need to have an existing Hive setup.</p>
<p><code>SparkSession</code>Spark 2.0中的内置支持Hive功能，包括使用HiveQL编写查询，访问Hive UDF以及从Hive表读取数据的功能。要使用这些功能，您不需要现有的Hive设置。</p>
<h3 id="2-2-Creating-DataFrames"><a href="#2-2-Creating-DataFrames" class="headerlink" title="2.2 Creating DataFrames"></a>2.2 Creating DataFrames</h3><p>With a <code>SparkSession</code>, applications can create DataFrames from an <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">existing <code>RDD</code></a>, from a Hive table, or from <a href="http://spark.apache.org/docs/2.1.2/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">Spark data sources</a>.</p>
<p>As an example, the following creates a DataFrame based on the content of a JSON file:</p>
<pre><code class="hljs scala"><span class="hljs-keyword">val</span> df = spark.read.json(<span class="hljs-string">"examples/src/main/resources/people.json"</span>)

<span class="hljs-comment">// Displays the content of the DataFrame to stdout</span>
df.show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span></code></pre>

<p>Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo.</p>
<h3 id="2-3-Untyped-Dataset-Operations-aka-DataFrame-Operations"><a href="#2-3-Untyped-Dataset-Operations-aka-DataFrame-Operations" class="headerlink" title="2.3 Untyped Dataset Operations (aka DataFrame Operations)"></a>2.3 Untyped Dataset Operations (aka DataFrame Operations)</h3><p>DataFrames provide a domain-specific language for structured data manipulation in <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a> and <a href="http://spark.apache.org/docs/2.1.2/api/R/SparkDataFrame.html" target="_blank" rel="noopener">R</a>.</p>
<p>As mentioned above, in Spark 2.0, DataFrames are just Dataset of <code>Row</code>s in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.</p>
<p>Here we include some basic examples of structured data processing using Datasets:</p>
<p>DataFrame为<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a>，<a href="http://spark.apache.org/docs/2.1.2/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>，<a href="http://spark.apache.org/docs/2.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>和<a href="http://spark.apache.org/docs/2.1.2/api/R/SparkDataFrame.html" target="_blank" rel="noopener">R中的</a>结构化数据操作提供了一种特定于域的语言。</p>
<p>如上所述，在Spark 2.0中，DataFrames只是<code>Row</code>Scala和Java API中的的数据集。与强类型的Scala / Java数据集附带的“类型转换”相反，这些操作也称为“非类型转换”。</p>
<p>这里我们包括一些使用数据集进行结构化数据处理的基本示例：</p>
<pre><code class="hljs scala"><span class="hljs-comment">// This import is needed to use the $-notation</span>
<span class="hljs-keyword">import</span> spark.implicits._
<span class="hljs-comment">// Print the schema in a tree format</span>
df.printSchema()
<span class="hljs-comment">// root</span>
<span class="hljs-comment">// |-- age: long (nullable = true)</span>
<span class="hljs-comment">// |-- name: string (nullable = true)</span>

<span class="hljs-comment">// Select only the "name" column</span>
df.select(<span class="hljs-string">"name"</span>).show()
<span class="hljs-comment">// +-------+</span>
<span class="hljs-comment">// |   name|</span>
<span class="hljs-comment">// +-------+</span>
<span class="hljs-comment">// |Michael|</span>
<span class="hljs-comment">// |   Andy|</span>
<span class="hljs-comment">// | Justin|</span>
<span class="hljs-comment">// +-------+</span>

<span class="hljs-comment">// Select everybody, but increment the age by 1</span>
df.select($<span class="hljs-string">"name"</span>, $<span class="hljs-string">"age"</span> + <span class="hljs-number">1</span>).show()
<span class="hljs-comment">// +-------+---------+</span>
<span class="hljs-comment">// |   name|(age + 1)|</span>
<span class="hljs-comment">// +-------+---------+</span>
<span class="hljs-comment">// |Michael|     null|</span>
<span class="hljs-comment">// |   Andy|       31|</span>
<span class="hljs-comment">// | Justin|       20|</span>
<span class="hljs-comment">// +-------+---------+</span>

<span class="hljs-comment">// Select people older than 21</span>
df.filter($<span class="hljs-string">"age"</span> &gt; <span class="hljs-number">21</span>).show()
<span class="hljs-comment">// +---+----+</span>
<span class="hljs-comment">// |age|name|</span>
<span class="hljs-comment">// +---+----+</span>
<span class="hljs-comment">// | 30|Andy|</span>
<span class="hljs-comment">// +---+----+</span>

<span class="hljs-comment">// Count people by age</span>
df.groupBy(<span class="hljs-string">"age"</span>).count().show()
<span class="hljs-comment">// +----+-----+</span>
<span class="hljs-comment">// | age|count|</span>
<span class="hljs-comment">// +----+-----+</span>
<span class="hljs-comment">// |  19|    1|</span>
<span class="hljs-comment">// |null|    1|</span>
<span class="hljs-comment">// |  30|    1|</span>
<span class="hljs-comment">// +----+-----+</span></code></pre>

<p>Find full example code at “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” in the Spark repo.</p>
<p>For a complete list of the types of operations that can be performed on a Dataset refer to the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">API Documentation</a>.</p>
<p>In addition to simple column references and expressions, Datasets also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank" rel="noopener">DataFrame Function Reference</a>.</p>
<pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">QuickExample</span> </span>&#123;
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;
<span class="hljs-comment">//    val sparkSession = SparkSession</span>
<span class="hljs-comment">//                          .builder()</span>
<span class="hljs-comment">//                          .appName("QuickExample")</span>
<span class="hljs-comment">//                          .config("spark.some.config.option", "some-value")</span>
<span class="hljs-comment">//                          .getOrCreate()</span>
    <span class="hljs-comment">//报错 org.apache.spark.SparkException: A master URL must be set in your configuration</span>


    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"QuickExample"</span>).setMaster(<span class="hljs-string">"local[*]"</span>)
    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>
                          .builder()
                          .appName(<span class="hljs-string">"QuickExample"</span>)
                          .config(sparkConf)
                          .getOrCreate()
    <span class="hljs-keyword">import</span> sparkSession.implicits._
    <span class="hljs-keyword">val</span> dataFrame = sparkSession.read.json(<span class="hljs-string">"src\\main\\resources\\people.json"</span>)

    dataFrame.show()
<span class="hljs-comment">//    +----+-------+</span>
<span class="hljs-comment">//    | age|   name|</span>
<span class="hljs-comment">//    +----+-------+</span>
<span class="hljs-comment">//    |null|Michael|</span>
<span class="hljs-comment">//    |  30|   Andy|</span>
<span class="hljs-comment">//    |  19| Justin|</span>
<span class="hljs-comment">//    +----+-------+</span>

    dataFrame.printSchema()
<span class="hljs-comment">//    root</span>
<span class="hljs-comment">//    |-- age: long (nullable = true)</span>
<span class="hljs-comment">//    |-- name: string (nullable = true)</span>

    dataFrame.select(<span class="hljs-string">"name"</span>).show()
<span class="hljs-comment">//    +-------+</span>
<span class="hljs-comment">//    |   name|</span>
<span class="hljs-comment">//    +-------+</span>
<span class="hljs-comment">//    |Michael|</span>
<span class="hljs-comment">//    |   Andy|</span>
<span class="hljs-comment">//    | Justin|</span>
<span class="hljs-comment">//    +-------+</span>

    <span class="hljs-comment">//dataFrame.select("name", "age"+1).show() 会报错</span>

    dataFrame.select($<span class="hljs-string">"name"</span>,$<span class="hljs-string">"age"</span>+<span class="hljs-number">1</span>).show()
<span class="hljs-comment">//    +-------+---------+</span>
<span class="hljs-comment">//    |   name|(age + 1)|</span>
<span class="hljs-comment">//    +-------+---------+</span>
<span class="hljs-comment">//    |Michael|     null|</span>
<span class="hljs-comment">//    |   Andy|       31|</span>
<span class="hljs-comment">//    | Justin|       20|</span>
<span class="hljs-comment">//    +-------+---------+</span>

    dataFrame.filter($<span class="hljs-string">"age"</span> &gt; <span class="hljs-number">21</span>).show()
<span class="hljs-comment">//    +---+----+</span>
<span class="hljs-comment">//    |age|name|</span>
<span class="hljs-comment">//    +---+----+</span>
<span class="hljs-comment">//    | 30|Andy|</span>
<span class="hljs-comment">//    +---+----+</span>

    dataFrame.groupBy(<span class="hljs-string">"age"</span>).count().show()
<span class="hljs-comment">//    +----+-----+</span>
<span class="hljs-comment">//    | age|count|</span>
<span class="hljs-comment">//    +----+-----+</span>
<span class="hljs-comment">//    |  19|    1|</span>
<span class="hljs-comment">//    |null|    1|</span>
<span class="hljs-comment">//    |  30|    1|</span>
<span class="hljs-comment">//    +----+-----+</span>
  &#125;
&#125;</code></pre>



<h3 id="2-4-Running-SQL-Queries-Programmatically"><a href="#2-4-Running-SQL-Queries-Programmatically" class="headerlink" title="2.4 Running SQL Queries Programmatically"></a>2.4 Running SQL Queries Programmatically</h3><p>The <code>sql</code> function on a <code>SparkSession</code> enables applications to run SQL queries programmatically and returns the result as a <code>DataFrame</code>.</p>
<p>上的<code>sql</code>函数<code>SparkSession</code>使应用程序能够以编程方式运行SQL查询，并以形式返回结果<code>DataFrame</code>。</p>
<pre><code class="hljs scala"><span class="hljs-comment">// Register the DataFrame as a SQL temporary view</span>
df.createOrReplaceTempView(<span class="hljs-string">"people"</span>)

<span class="hljs-keyword">val</span> sqlDF = spark.sql(<span class="hljs-string">"SELECT * FROM people"</span>)
sqlDF.show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span></code></pre>

<h3 id="2-5-Global-Temporary-View"><a href="#2-5-Global-Temporary-View" class="headerlink" title="2.5 Global Temporary View"></a>2.5 Global Temporary View</h3><p>Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database <code>global_temp</code>, and we must use the qualified name to refer it, e.g. <code>SELECT * FROM global_temp.view1</code>.</p>
<p>Spark SQL中的临时视图是会话作用域的，如果创建它的会话终止，它将消失。如果要在所有会话之间共享一个临时视图并保持活动状态，直到Spark应用程序终止，则可以创建全局临时视图。全局临时视图与系统保留的数据库相关联<code>global_temp</code>，我们必须使用限定名称来引用它，例如<code>SELECT * FROM global_temp.view1</code>。</p>
<pre><code class="hljs scala"><span class="hljs-comment">// Register the DataFrame as a global temporary view</span>
df.createGlobalTempView(<span class="hljs-string">"people"</span>)

<span class="hljs-comment">// Global temporary view is tied to a system preserved database `global_temp`</span>
spark.sql(<span class="hljs-string">"SELECT * FROM global_temp.people"</span>).show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span>

<span class="hljs-comment">// Global temporary view is cross-session</span>
spark.newSession().sql(<span class="hljs-string">"SELECT * FROM global_temp.people"</span>).show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span></code></pre>

<pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-type">SparkConf</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span>

<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">SQLExample</span> </span>&#123;
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;
    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">"SQLExample"</span>).setMaster(<span class="hljs-string">"local[2]"</span>)
    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()
    <span class="hljs-keyword">val</span> dataFrame = sparkSession.read.json(<span class="hljs-string">"src\\main\\resources\\people.json"</span>)
    <span class="hljs-comment">// Register the DataFrame as a SQL temporary view</span>
    dataFrame.createOrReplaceTempView(<span class="hljs-string">"people"</span>)
    <span class="hljs-keyword">val</span> frame = sparkSession.sql(<span class="hljs-string">"select * from people"</span>)
    frame.show()


    <span class="hljs-comment">//新创建一个session，报错，找不到 Table or view not found: people;</span>
    <span class="hljs-comment">//sparkSession.newSession().sql("select * from people").show()</span>
    <span class="hljs-comment">//20/07/21 11:20:04 INFO SparkSqlParser: Parsing command: select * from people</span>
    <span class="hljs-comment">//Exception in thread "main" org.apache.spark.sql.AnalysisException: Table or view not found: people; line 1 pos 14</span>

    <span class="hljs-comment">//注册成全局视图，只有spark程序关闭时，才会失效</span>
    <span class="hljs-comment">//Global temporary view is tied to a system preserved database global_temp,</span>
    <span class="hljs-comment">// and we must use the qualified name to refer it,</span>
    <span class="hljs-comment">// e.g. SELECT * FROM global_temp.view1.</span>
    <span class="hljs-comment">//类似放在global_temp库下了，访问时候把库名加上，不然找不到表</span>
    dataFrame.createGlobalTempView(<span class="hljs-string">"glo_people"</span>)
    sparkSession.sql(<span class="hljs-string">"select * from global_temp.glo_people"</span>).show()
    sparkSession.newSession().sql(<span class="hljs-string">"select * from global_temp.glo_people"</span>).show()
  &#125;
&#125;</code></pre>



<h3 id="2-6-Creating-Datasets"><a href="#2-6-Creating-Datasets" class="headerlink" title="2.6 Creating Datasets"></a>2.6 Creating Datasets</h3><p>Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Encoder" target="_blank" rel="noopener">Encoder</a> to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.</p>
<p>数据集类似于RDD，但是，它们不使用Java序列化或Kryo，而是使用专用的<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.Encoder" target="_blank" rel="noopener">Encoder</a>对对象进行序列化以进行处理或通过网络传输。虽然编码器和标准序列化都负责将对象转换为字节，但是编码器是动态生成的代码，并使用一种格式，该格式允许Spark执行许多操作，如过滤，排序和哈希处理，而无需将字节反序列化回对象。</p>
<pre><code class="hljs scala"><span class="hljs-comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span>
<span class="hljs-comment">// you can use custom classes that implement the Product interface</span>
<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, age: <span class="hljs-type">Long</span></span>)</span>
<span class="hljs-class"></span>
<span class="hljs-class"><span class="hljs-title">//</span> <span class="hljs-title">Encoders</span> <span class="hljs-title">are</span> <span class="hljs-title">created</span> <span class="hljs-title">for</span> <span class="hljs-title">case</span> <span class="hljs-title">classes</span></span>
<span class="hljs-class"><span class="hljs-title">val</span> <span class="hljs-title">caseClassDS</span> </span>= <span class="hljs-type">Seq</span>(<span class="hljs-type">Person</span>(<span class="hljs-string">"Andy"</span>, <span class="hljs-number">32</span>)).toDS()
caseClassDS.show()
<span class="hljs-comment">// +----+---+</span>
<span class="hljs-comment">// |name|age|</span>
<span class="hljs-comment">// +----+---+</span>
<span class="hljs-comment">// |Andy| 32|</span>
<span class="hljs-comment">// +----+---+</span>

<span class="hljs-comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span>
<span class="hljs-keyword">val</span> primitiveDS = <span class="hljs-type">Seq</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>).toDS()
primitiveDS.map(_ + <span class="hljs-number">1</span>).collect() <span class="hljs-comment">// Returns: Array(2, 3, 4)</span>

<span class="hljs-comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span>
<span class="hljs-keyword">val</span> path = <span class="hljs-string">"examples/src/main/resources/people.json"</span>
<span class="hljs-keyword">val</span> peopleDS = spark.read.json(path).as[<span class="hljs-type">Person</span>]
peopleDS.show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span></code></pre>

<h3 id="2-7-Interoperating-with-RDDs"><a href="#2-7-Interoperating-with-RDDs" class="headerlink" title="2.7 Interoperating with RDDs"></a>2.7 Interoperating with RDDs</h3><p>Spark SQL supports two different methods for converting existing RDDs into Datasets. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.</p>
<p>The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime.</p>
<p>Spark SQL支持两种将现有RDD转换为数据集的方法。第一种方法使用反射来推断包含特定对象类型的RDD的架构。这种基于反射的方法可以使代码更简洁，当您在编写Spark应用程序时已经了解架构时，可以很好地工作。</p>
<p>创建数据集的第二种方法是通过编程界面，该界面允许您构造模式，然后将其应用于现有的RDD。尽管此方法较为冗长，但可以在运行时才知道列及其类型的情况下构造数据集。</p>
<h4 id="2-7-1-Inferring-the-Schema-Using-Reflection"><a href="#2-7-1-Inferring-the-Schema-Using-Reflection" class="headerlink" title="2.7.1 Inferring the Schema Using Reflection"></a>2.7.1 Inferring the Schema Using Reflection</h4><p>The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as <code>Seq</code>s or <code>Array</code>s. This RDD can be implicitly converted to a DataFrame and then be registered as a table. Tables can be used in subsequent SQL statements.</p>
<p>Spark SQL的Scala接口支持将包含样例类的RDD自动转换为DataFrame。样例类定义表的结构。样例类的参数名称使用反射读取，并成为列的名称。Case类也可以嵌套或包含<code>Seq</code>s或<code>Array</code>s之类的复杂类型。可以将该RDD隐式转换为DataFrame，然后将其注册为表。表可以在后续的SQL语句中使用。</p>
<pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.catalyst.encoders.<span class="hljs-type">ExpressionEncoder</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoder</span>

<span class="hljs-comment">// For implicit conversions from RDDs to DataFrames</span>
<span class="hljs-keyword">import</span> spark.implicits._

<span class="hljs-comment">// Create an RDD of Person objects from a text file, convert it to a Dataframe</span>
<span class="hljs-keyword">val</span> peopleDF = spark.sparkContext
  .textFile(<span class="hljs-string">"examples/src/main/resources/people.txt"</span>)
  .map(_.split(<span class="hljs-string">","</span>))
  .map(attributes =&gt; <span class="hljs-type">Person</span>(attributes(<span class="hljs-number">0</span>), attributes(<span class="hljs-number">1</span>).trim.toInt))
  .toDF()
<span class="hljs-comment">// Register the DataFrame as a temporary view</span>
peopleDF.createOrReplaceTempView(<span class="hljs-string">"people"</span>)

<span class="hljs-comment">// SQL statements can be run by using the sql methods provided by Spark</span>
<span class="hljs-keyword">val</span> teenagersDF = spark.sql(<span class="hljs-string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)

<span class="hljs-comment">// The columns of a row in the result can be accessed by field index</span>
teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager(<span class="hljs-number">0</span>)).show()
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |       value|</span>
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |Name: Justin|</span>
<span class="hljs-comment">// +------------+</span>

<span class="hljs-comment">// or by field name</span>
teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager.getAs[<span class="hljs-type">String</span>](<span class="hljs-string">"name"</span>)).show()
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |       value|</span>
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |Name: Justin|</span>
<span class="hljs-comment">// +------------+</span>

<span class="hljs-comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span>
<span class="hljs-keyword">implicit</span> <span class="hljs-keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="hljs-type">Encoders</span>.kryo[<span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">Any</span>]]
<span class="hljs-comment">// Primitive types and case classes can be also defined as</span>
<span class="hljs-comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span>

<span class="hljs-comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>
teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="hljs-type">Any</span>](<span class="hljs-type">List</span>(<span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>))).collect()
<span class="hljs-comment">// Array(Map("name" -&gt; "Justin", "age" -&gt; 19))</span></code></pre>

<pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-type">SparkConf</span>
<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;<span class="hljs-type">DataFrame</span>, <span class="hljs-type">Dataset</span>, <span class="hljs-type">SparkSession</span>&#125;

<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">RDD2DS</span> </span>&#123;
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;
    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">"local[*]"</span>)
    <span class="hljs-keyword">val</span> sparkSession = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()
    <span class="hljs-keyword">val</span> dataSet: <span class="hljs-type">Dataset</span>[<span class="hljs-type">String</span>] = sparkSession.read.textFile(<span class="hljs-string">"src\\main\\resources\\people.txt"</span>)
    <span class="hljs-keyword">val</span> dataRdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sparkSession.sparkContext.textFile(<span class="hljs-string">"src\\main\\resources\\people.txt"</span>)
    dataRdd.foreach(println)
<span class="hljs-comment">//    Justin, 19</span>
<span class="hljs-comment">//    Michael, 29</span>
<span class="hljs-comment">//    Andy, 30</span>
    dataSet.show()
    <span class="hljs-comment">// 自动转成dataset,类型不对，是string应该</span>
<span class="hljs-comment">//    +-----------+</span>
<span class="hljs-comment">//    |      value|</span>
<span class="hljs-comment">//    +-----------+</span>
<span class="hljs-comment">//    |Michael, 29|</span>
<span class="hljs-comment">//    |   Andy, 30|</span>
<span class="hljs-comment">//    | Justin, 19|</span>
<span class="hljs-comment">//    +-----------+</span>

    <span class="hljs-comment">//将这个RDD转成dateset</span>
    <span class="hljs-keyword">val</span> dataRddPerson: <span class="hljs-type">RDD</span>[<span class="hljs-type">Person</span>] = dataRdd.map(_.split(<span class="hljs-string">","</span>))
          .map(attributes =&gt; <span class="hljs-type">Person</span>(attributes(<span class="hljs-number">0</span>), attributes(<span class="hljs-number">1</span>).trim.toInt))
    dataRddPerson.foreach(println)
<span class="hljs-comment">//    Person(Justin,19)</span>
<span class="hljs-comment">//    Person(Michael,29)</span>
<span class="hljs-comment">//    Person(Andy,30)</span>
    <span class="hljs-keyword">import</span> sparkSession.implicits._
    <span class="hljs-keyword">val</span> dataSetPerson: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Person</span>] = dataRddPerson.toDS()
    dataSetPerson.show()
<span class="hljs-comment">//    +-------+---+</span>
<span class="hljs-comment">//    |   name|age|</span>
<span class="hljs-comment">//    +-------+---+</span>
<span class="hljs-comment">//    |Michael| 29|</span>
<span class="hljs-comment">//    |   Andy| 30|</span>
<span class="hljs-comment">//    | Justin| 19|</span>
<span class="hljs-comment">//    +-------+---+</span>

    dataSetPerson.createOrReplaceTempView(<span class="hljs-string">"people"</span>)
    <span class="hljs-keyword">val</span> teenagersDF: <span class="hljs-type">DataFrame</span> = sparkSession.sql(<span class="hljs-string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)
    teenagersDF.show()
<span class="hljs-comment">//    +------+---+</span>
<span class="hljs-comment">//    |  name|age|</span>
<span class="hljs-comment">//    +------+---+</span>
<span class="hljs-comment">//    |Justin| 19|</span>
<span class="hljs-comment">//    +------+---+</span>

    <span class="hljs-comment">// The columns of a row in the result can be accessed by field index</span>
    teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager(<span class="hljs-number">0</span>)).show()
<span class="hljs-comment">//    +------------+</span>
<span class="hljs-comment">//    |       value|</span>
<span class="hljs-comment">//    +------------+</span>
<span class="hljs-comment">//    |Name: Justin|</span>
<span class="hljs-comment">//    +------------+</span>
    teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager(<span class="hljs-number">0</span>) + <span class="hljs-string">"  age: "</span>+ teenager(<span class="hljs-number">1</span>)).show()
<span class="hljs-comment">//    +--------------------+</span>
<span class="hljs-comment">//    |               value|</span>
<span class="hljs-comment">//    +--------------------+</span>
<span class="hljs-comment">//    |Name: Justin  Age...|</span>
<span class="hljs-comment">//    +--------------------+</span>
    <span class="hljs-comment">//为什么age变成...</span>

    <span class="hljs-comment">// or by field name</span>
    teenagersDF.map(teenager =&gt; <span class="hljs-string">"Name: "</span> + teenager.getAs[<span class="hljs-type">String</span>](<span class="hljs-string">"name"</span>)).show()
<span class="hljs-comment">//    +------------+</span>
<span class="hljs-comment">//    |       value|</span>
<span class="hljs-comment">//    +------------+</span>
<span class="hljs-comment">//    |Name: Justin|</span>
<span class="hljs-comment">//    +------------+</span>
    teenagersDF.map(teenagers =&gt; teenagers.getAs[<span class="hljs-type">Long</span>](<span class="hljs-string">"age"</span>)).show()
<span class="hljs-comment">//    +-----+</span>
<span class="hljs-comment">//    |value|</span>
<span class="hljs-comment">//    +-----+</span>
<span class="hljs-comment">//    |   19|</span>
<span class="hljs-comment">//    +-----+</span>

    <span class="hljs-comment">// 不太理解</span>
    <span class="hljs-comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span>
    <span class="hljs-keyword">implicit</span> <span class="hljs-keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="hljs-type">Encoders</span>.kryo[<span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">Any</span>]]
    <span class="hljs-comment">// Primitive types and case classes can be also defined as</span>
    <span class="hljs-comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span>

    <span class="hljs-comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>
    teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="hljs-type">Any</span>](<span class="hljs-type">List</span>(<span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>))).collect()
    <span class="hljs-comment">// Array(Map("name" -&gt; "Justin", "age" -&gt; 19))</span>
  &#125;
&#125;</code></pre>



<h4 id="2-7-2-Programmatically-Specifying-the-Schema"><a href="#2-7-2-Programmatically-Specifying-the-Schema" class="headerlink" title="2.7.2 Programmatically Specifying the Schema"></a>2.7.2 Programmatically Specifying the Schema</h4><p>When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a <code>DataFrame</code> can be created programmatically with three steps.</p>
<ol>
<li>Create an RDD of <code>Row</code>s from the original RDD;</li>
<li>Create the schema represented by a <code>StructType</code> matching the structure of <code>Row</code>s in the RDD created in Step 1.</li>
<li>Apply the schema to the RDD of <code>Row</code>s via <code>createDataFrame</code> method provided by <code>SparkSession</code>.</li>
</ol>
<p>如果无法提前定义样例类（例如，记录的结构编码为字符串，或者将解析文本数据集，并且针对不同的用户对字段进行不同的投影），<code>DataFrame</code>则可以通过三个步骤以编程方式创建 。</p>
<ol>
<li>从原始RDD 创建一个的RDD；</li>
<li>在第1步中创建的RDD中创建<code>StructType</code>与<code>Row</code>s 的结构匹配 表示的模式。</li>
<li><code>Row</code>通过<code>createDataFrame</code>提供的方法将架构应用于的RDD <code>SparkSession</code>。</li>
</ol>
<p>For example:</p>
<pre><code class="hljs scala">
</code></pre>

<h3 id="2-8-Aggregations"><a href="#2-8-Aggregations" class="headerlink" title="2.8 Aggregations"></a>2.8 Aggregations</h3><p>The <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank" rel="noopener">built-in DataFrames functions</a> provide common aggregations such as <code>count()</code>, <code>countDistinct()</code>, <code>avg()</code>, <code>max()</code>, <code>min()</code>, etc. While those functions are designed for DataFrames, Spark SQL also has type-safe versions for some of them in <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$" target="_blank" rel="noopener">Scala</a> and <a href="http://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/sql/expressions/javalang/typed.html" target="_blank" rel="noopener">Java</a> to work with strongly typed Datasets. Moreover, users are not limited to the predefined aggregate functions and can create their own.</p>
<p>该<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank" rel="noopener">内置功能DataFrames</a>提供共同聚合，例如<code>count()</code>，<code>countDistinct()</code>，<code>avg()</code>，<code>max()</code>，<code>min()</code>，等。虽然这些功能是专为DataFrames，星火SQL也有类型安全的版本为他们中的一些 <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$" target="_blank" rel="noopener">斯卡拉</a>和 <a href="http://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/sql/expressions/javalang/typed.html" target="_blank" rel="noopener">Java的</a>与强类型数据集的工作。此外，用户不限于预定义的聚合功能，还可以创建自己的功能。</p>
<h4 id="2-8-1-Untyped-User-Defined-Aggregate-Functions"><a href="#2-8-1-Untyped-User-Defined-Aggregate-Functions" class="headerlink" title="2.8.1 Untyped User-Defined Aggregate Functions"></a>2.8.1 Untyped User-Defined Aggregate Functions</h4><p>Users have to extend the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" target="_blank" rel="noopener">UserDefinedAggregateFunction</a> abstract class to implement a custom untyped aggregate function. For example, a user-defined average can look like:</p>
<p>用户必须扩展<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" target="_blank" rel="noopener">UserDefinedAggregateFunction</a> 抽象类以实现自定义无类型的聚合函数。例如，用户定义的平均值如下所示：</p>
<pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">MutableAggregationBuffer</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">UserDefinedAggregateFunction</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.types._
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Row</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span>

<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">MyAverage</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">UserDefinedAggregateFunction</span> </span>&#123;
  <span class="hljs-comment">// Data types of input arguments of this aggregate function</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inputSchema</span></span>: <span class="hljs-type">StructType</span> = <span class="hljs-type">StructType</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">"inputColumn"</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">Nil</span>)
  <span class="hljs-comment">// Data types of values in the aggregation buffer</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferSchema</span></span>: <span class="hljs-type">StructType</span> = &#123;
    <span class="hljs-type">StructType</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">"sum"</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">StructField</span>(<span class="hljs-string">"count"</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">Nil</span>)
  &#125;
  <span class="hljs-comment">// The data type of the returned value</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataType</span></span>: <span class="hljs-type">DataType</span> = <span class="hljs-type">DoubleType</span>
  <span class="hljs-comment">// Whether this function always returns the same output on the identical input</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deterministic</span></span>: <span class="hljs-type">Boolean</span> = <span class="hljs-literal">true</span>
  <span class="hljs-comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span>
  <span class="hljs-comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span>
  <span class="hljs-comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span>
  <span class="hljs-comment">// immutable.</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>): <span class="hljs-type">Unit</span> = &#123;
    buffer(<span class="hljs-number">0</span>) = <span class="hljs-number">0</span>L
    buffer(<span class="hljs-number">1</span>) = <span class="hljs-number">0</span>L
  &#125;
  <span class="hljs-comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>, input: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = &#123;
    <span class="hljs-keyword">if</span> (!input.isNullAt(<span class="hljs-number">0</span>)) &#123;
      buffer(<span class="hljs-number">0</span>) = buffer.getLong(<span class="hljs-number">0</span>) + input.getLong(<span class="hljs-number">0</span>)
      buffer(<span class="hljs-number">1</span>) = buffer.getLong(<span class="hljs-number">1</span>) + <span class="hljs-number">1</span>
    &#125;
  &#125;
  <span class="hljs-comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(buffer1: <span class="hljs-type">MutableAggregationBuffer</span>, buffer2: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = &#123;
    buffer1(<span class="hljs-number">0</span>) = buffer1.getLong(<span class="hljs-number">0</span>) + buffer2.getLong(<span class="hljs-number">0</span>)
    buffer1(<span class="hljs-number">1</span>) = buffer1.getLong(<span class="hljs-number">1</span>) + buffer2.getLong(<span class="hljs-number">1</span>)
  &#125;
  <span class="hljs-comment">// Calculates the final result</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span></span>(buffer: <span class="hljs-type">Row</span>): <span class="hljs-type">Double</span> = buffer.getLong(<span class="hljs-number">0</span>).toDouble / buffer.getLong(<span class="hljs-number">1</span>)
&#125;

<span class="hljs-comment">// Register the function to access it</span>
spark.udf.register(<span class="hljs-string">"myAverage"</span>, <span class="hljs-type">MyAverage</span>)

<span class="hljs-keyword">val</span> df = spark.read.json(<span class="hljs-string">"examples/src/main/resources/employees.json"</span>)
df.createOrReplaceTempView(<span class="hljs-string">"employees"</span>)
df.show()
<span class="hljs-comment">// +-------+------+</span>
<span class="hljs-comment">// |   name|salary|</span>
<span class="hljs-comment">// +-------+------+</span>
<span class="hljs-comment">// |Michael|  3000|</span>
<span class="hljs-comment">// |   Andy|  4500|</span>
<span class="hljs-comment">// | Justin|  3500|</span>
<span class="hljs-comment">// |  Berta|  4000|</span>
<span class="hljs-comment">// +-------+------+</span>

<span class="hljs-keyword">val</span> result = spark.sql(<span class="hljs-string">"SELECT myAverage(salary) as average_salary FROM employees"</span>)
result.show()
<span class="hljs-comment">// +--------------+</span>
<span class="hljs-comment">// |average_salary|</span>
<span class="hljs-comment">// +--------------+</span>
<span class="hljs-comment">// |        3750.0|</span>
<span class="hljs-comment">// +--------------+</span></code></pre>

<h4 id="2-8-2-Type-Safe-User-Defined-Aggregate-Functions"><a href="#2-8-2-Type-Safe-User-Defined-Aggregate-Functions" class="headerlink" title="2.8.2 Type-Safe User-Defined Aggregate Functions"></a>2.8.2 Type-Safe User-Defined Aggregate Functions</h4><p>User-defined aggregations for strongly typed Datasets revolve around the <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator" target="_blank" rel="noopener">Aggregator</a> abstract class. For example, a type-safe user-defined average can look like:</p>
<p>用户定义的强类型数据集的<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator" target="_blank" rel="noopener">聚合</a>围绕<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator" target="_blank" rel="noopener">Aggregator</a>抽象类展开。例如，类型安全的用户定义的平均值如下所示：</p>
<pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">Aggregator</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoder</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoders</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span>

<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Employee</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, salary: <span class="hljs-type">Long</span></span>)</span>
<span class="hljs-class"><span class="hljs-title">case</span> <span class="hljs-title">class</span> <span class="hljs-title">Average</span>(<span class="hljs-params">var sum: <span class="hljs-type">Long</span>, var count: <span class="hljs-type">Long</span></span>)</span>
<span class="hljs-class"></span>
<span class="hljs-class"><span class="hljs-title">object</span> <span class="hljs-title">MyAverage</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Aggregator</span>[<span class="hljs-type">Employee</span>, <span class="hljs-type">Average</span>, <span class="hljs-type">Double</span>] </span>&#123;
  <span class="hljs-comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zero</span></span>: <span class="hljs-type">Average</span> = <span class="hljs-type">Average</span>(<span class="hljs-number">0</span>L, <span class="hljs-number">0</span>L)
  <span class="hljs-comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span>
  <span class="hljs-comment">// and return it instead of constructing a new object</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reduce</span></span>(buffer: <span class="hljs-type">Average</span>, employee: <span class="hljs-type">Employee</span>): <span class="hljs-type">Average</span> = &#123;
    buffer.sum += employee.salary
    buffer.count += <span class="hljs-number">1</span>
    buffer
  &#125;
  <span class="hljs-comment">// Merge two intermediate values</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(b1: <span class="hljs-type">Average</span>, b2: <span class="hljs-type">Average</span>): <span class="hljs-type">Average</span> = &#123;
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  &#125;
  <span class="hljs-comment">// Transform the output of the reduction</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">finish</span></span>(reduction: <span class="hljs-type">Average</span>): <span class="hljs-type">Double</span> = reduction.sum.toDouble / reduction.count
  <span class="hljs-comment">// Specifies the Encoder for the intermediate value type</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Average</span>] = <span class="hljs-type">Encoders</span>.product
  <span class="hljs-comment">// Specifies the Encoder for the final output value type</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outputEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Double</span>] = <span class="hljs-type">Encoders</span>.scalaDouble
&#125;

<span class="hljs-keyword">val</span> ds = spark.read.json(<span class="hljs-string">"examples/src/main/resources/employees.json"</span>).as[<span class="hljs-type">Employee</span>]
ds.show()
<span class="hljs-comment">// +-------+------+</span>
<span class="hljs-comment">// |   name|salary|</span>
<span class="hljs-comment">// +-------+------+</span>
<span class="hljs-comment">// |Michael|  3000|</span>
<span class="hljs-comment">// |   Andy|  4500|</span>
<span class="hljs-comment">// | Justin|  3500|</span>
<span class="hljs-comment">// |  Berta|  4000|</span>
<span class="hljs-comment">// +-------+------+</span>

<span class="hljs-comment">// Convert the function to a `TypedColumn` and give it a name</span>
<span class="hljs-keyword">val</span> averageSalary = <span class="hljs-type">MyAverage</span>.toColumn.name(<span class="hljs-string">"average_salary"</span>)
<span class="hljs-keyword">val</span> result = ds.select(averageSalary)
result.show()
<span class="hljs-comment">// +--------------+</span>
<span class="hljs-comment">// |average_salary|</span>
<span class="hljs-comment">// +--------------+</span>
<span class="hljs-comment">// |        3750.0|</span>
<span class="hljs-comment">// +--------------+</span></code></pre>

<h2 id="3-Data-Sources"><a href="#3-Data-Sources" class="headerlink" title="3. Data Sources"></a>3. Data Sources</h2><p>Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data. This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources.</p>
<p>Spark SQL支持通过DataFrame接口对各种数据源进行操作。DataFrame可以使用关系转换进行操作，也可以用于创建临时视图。将DataFrame注册为临时视图使您可以对其数据运行SQL查询。本节介绍了使用Spark数据源加载和保存数据的一般方法，然后介绍了可用于内置数据源的特定选项。</p>
<h3 id="3-1-Generic-Load-Save-Functions"><a href="#3-1-Generic-Load-Save-Functions" class="headerlink" title="3.1 Generic Load/Save Functions"></a>3.1 Generic Load/Save Functions</h3><p>In the simplest form, the default data source (<code>parquet</code> unless otherwise configured by <code>spark.sql.sources.default</code>) will be used for all operations.</p>
<p>以最简单的形式，所有操作都将使用默认数据源（<code>parquet</code>除非另有配置 <code>spark.sql.sources.default</code>）</p>
<pre><code class="hljs scala"><span class="hljs-keyword">val</span> usersDF = spark.read.load(<span class="hljs-string">"examples/src/main/resources/users.parquet"</span>)
usersDF.select(<span class="hljs-string">"name"</span>, <span class="hljs-string">"favorite_color"</span>).write.save(<span class="hljs-string">"namesAndFavColors.parquet"</span>)</code></pre>

<h4 id="3-1-1-Manually-Specifying-Options"><a href="#3-1-1-Manually-Specifying-Options" class="headerlink" title="3.1.1 Manually Specifying Options"></a>3.1.1 Manually Specifying Options</h4><p>You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. Data sources are specified by their fully qualified name (i.e., <code>org.apache.spark.sql.parquet</code>), but for built-in sources you can also use their short names (<code>json</code>, <code>parquet</code>, <code>jdbc</code>, <code>orc</code>, <code>libsvm</code>, <code>csv</code>, <code>text</code>). DataFrames loaded from any data source type can be converted into other types using this syntax</p>
<p>您还可以手动指定将要使用的数据源以及要传递给数据源的任何其他选项。数据源通过其全名指定（即<code>org.apache.spark.sql.parquet</code>），但内置的来源，你也可以使用自己的短名称（<code>json</code>，<code>parquet</code>，<code>jdbc</code>，<code>orc</code>，<code>libsvm</code>，<code>csv</code>，<code>text</code>）。从任何数据源类型加载的DataFrame都可以使用此语法转换为其他类型。</p>
<pre><code class="hljs scala"><span class="hljs-keyword">val</span> peopleDF = spark.read.format(<span class="hljs-string">"json"</span>).load(<span class="hljs-string">"examples/src/main/resources/people.json"</span>)
peopleDF.select(<span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>).write.format(<span class="hljs-string">"parquet"</span>).save(<span class="hljs-string">"namesAndAges.parquet"</span>)</code></pre>

<h4 id="3-1-2-Run-SQL-on-files-directly"><a href="#3-1-2-Run-SQL-on-files-directly" class="headerlink" title="3.1.2 Run SQL on files directly"></a>3.1.2 Run SQL on files directly</h4><p>Instead of using read API to load a file into DataFrame and query it, you can also query that file directly with SQL.</p>
<pre><code class="hljs scala"><span class="hljs-keyword">val</span> sqlDF = spark.sql(<span class="hljs-string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>)</code></pre>

<h4 id="3-1-3-Save-Modes"><a href="#3-1-3-Save-Modes" class="headerlink" title="3.1.3 Save Modes"></a>3.1.3 Save Modes</h4><p>Save operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing an <code>Overwrite</code>, the data will be deleted before writing out the new data.</p>
<p>保存操作可以选择带<code>SaveMode</code>，指定如何处理现有数据（如果存在）。重要的是要认识到这些保存模式不利用任何锁定，也不是原子的。另外，执行时<code>Overwrite</code>，将在写出新数据之前删除数据。</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th align="left">Any Language</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists</code> (default)</td>
<td align="left"><code>&quot;error&quot;</code> (default)</td>
<td align="left">When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td align="left"><code>&quot;append&quot;</code></td>
<td align="left">When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td align="left"><code>&quot;overwrite&quot;</code></td>
<td align="left">Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td align="left"><code>&quot;ignore&quot;</code></td>
<td align="left">Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a <code>CREATE TABLE IF NOT EXISTS</code> in SQL.</td>
</tr>
</tbody></table>
<h4 id="3-1-4-Saving-to-Persistent-Tables"><a href="#3-1-4-Saving-to-Persistent-Tables" class="headerlink" title="3.1.4 Saving to Persistent Tables"></a>3.1.4 Saving to Persistent Tables</h4><p><code>DataFrames</code> can also be saved as persistent tables into Hive metastore using the <code>saveAsTable</code> command. Notice that an existing Hive deployment is not necessary to use this feature. Spark will create a default local Hive metastore (using Derby) for you. Unlike the <code>createOrReplaceTempView</code> command, <code>saveAsTable</code> will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the <code>table</code> method on a <code>SparkSession</code> with the name of the table.</p>
<p><code>DataFrames</code>也可以使用以下<code>saveAsTable</code> 命令作为持久表保存到Hive Metastore中。请注意，使用此功能不需要现有的Hive部署。Spark将为您创建一个默认的本地Hive Metastore（使用Derby）。与<code>createOrReplaceTempView</code>命令不同， <code>saveAsTable</code>它将具体化DataFrame的内容并在Hive元存储中创建一个指向数据的指针。即使您重新启动Spark程序，持久表仍将存在，只要您保持与同一metastore的连接即可。可以通过使用表名称<code>table</code>在上调用方法来创建持久表的DataFrame <code>SparkSession</code>。</p>
<p>By default <code>saveAsTable</code> will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.</p>
<p>Currently, <code>saveAsTable</code> does not expose an API supporting the creation of an “external table” from a <code>DataFrame</code>. However, this functionality can be achieved by providing a <code>path</code> option to the <code>DataFrameWriter</code> with <code>path</code> as the key and location of the external table as its value (a string) when saving the table with <code>saveAsTable</code>. When an External table is dropped only its metadata is removed.</p>
<p>默认情况下，<code>saveAsTable</code>将创建一个“托管表”，这意味着数据的位置将由metastore控制。删除表时，托管表还将自动删除其数据。</p>
<p>当前，<code>saveAsTable</code>不公开支持从中创建“外部表”的API <code>DataFrame</code>。但是，可以通过以下方式来实现此功能：在使用保存表时，通过提供<code>path</code>选项<code>DataFrameWriter</code>with <code>path</code>作为键，并将外部表的位置作为其值（字符串）<code>saveAsTable</code>。删除外部表时，仅删除其元数据。</p>
<p>Starting from Spark 2.1, persistent datasource tables have per-partition metadata stored in the Hive metastore. This brings several benefits:</p>
<ul>
<li>Since the metastore can return only necessary partitions for a query, discovering all the partitions on the first query to the table is no longer needed.</li>
<li>Hive DDLs such as <code>ALTER TABLE PARTITION ... SET LOCATION</code> are now available for tables created with the Datasource API.</li>
</ul>
<p>Note that partition information is not gathered by default when creating external datasource tables (those with a <code>path</code> option). To sync the partition information in the metastore, you can invoke <code>MSCK REPAIR TABLE</code>.</p>
<p>从Spark 2.1开始，持久性数据源表在Hive元存储中存储了按分区的元数据。这带来了几个好处：</p>
<ul>
<li>由于元存储只能返回查询的必要分区，因此不再需要在第一个查询中将所有分区发现到表中。</li>
<li>Hive DDL，例如<code>ALTER TABLE PARTITION ... SET LOCATION</code>现在可用于使用Datasource API创建的表。</li>
</ul>
<p>请注意，在创建外部数据源表（带有<code>path</code>选项的表）时，默认情况下不会收集分区信息。要同步元存储中的分区信息，可以调用<code>MSCK REPAIR TABLE</code>。</p>
<h3 id="3-2-Parquet-Files"><a href="#3-2-Parquet-Files" class="headerlink" title="3.2 Parquet Files"></a>3.2 Parquet Files</h3><p><a href="http://parquet.io/" target="_blank" rel="noopener">Parquet</a> is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.</p>
<p><a href="http://parquet.io/" target="_blank" rel="noopener">Parquet</a>是许多其他数据处理系统支持的柱状格式。Spark SQL提供对读取和写入Parquet文件的支持，这些文件会自动保留原始数据的架构。编写Parquet文件时，出于兼容性原因，所有列都将自动转换为可为空。</p>
<h4 id="3-2-1-Loading-Data-Programmatically"><a href="#3-2-1-Loading-Data-Programmatically" class="headerlink" title="3.2.1 Loading Data Programmatically"></a>3.2.1 Loading Data Programmatically</h4><p>Using the data from the above example:</p>
<pre><code class="hljs scala"><span class="hljs-comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span>
<span class="hljs-keyword">import</span> spark.implicits._

<span class="hljs-keyword">val</span> peopleDF = spark.read.json(<span class="hljs-string">"examples/src/main/resources/people.json"</span>)

<span class="hljs-comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span>
peopleDF.write.parquet(<span class="hljs-string">"people.parquet"</span>)

<span class="hljs-comment">// Read in the parquet file created above</span>
<span class="hljs-comment">// Parquet files are self-describing so the schema is preserved</span>
<span class="hljs-comment">// The result of loading a Parquet file is also a DataFrame</span>
<span class="hljs-keyword">val</span> parquetFileDF = spark.read.parquet(<span class="hljs-string">"people.parquet"</span>)

<span class="hljs-comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span>
parquetFileDF.createOrReplaceTempView(<span class="hljs-string">"parquetFile"</span>)
<span class="hljs-keyword">val</span> namesDF = spark.sql(<span class="hljs-string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>)
namesDF.map(attributes =&gt; <span class="hljs-string">"Name: "</span> + attributes(<span class="hljs-number">0</span>)).show()
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |       value|</span>
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |Name: Justin|</span>
<span class="hljs-comment">// +------------+</span></code></pre>

<h4 id="3-2-2-Partition-Discovery"><a href="#3-2-2-Partition-Discovery" class="headerlink" title="3.2.2 Partition Discovery"></a>3.2.2 Partition Discovery</h4><p>Table partitioning is a common optimization approach used in systems like Hive. In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory. The Parquet data source is now able to discover and infer partitioning information automatically. For example, we can store all our previously used population data into a partitioned table using the following directory structure, with two extra columns, <code>gender</code> and <code>country</code> as partitioning columns:</p>
<p>表分区是Hive等系统中常用的优化方法。在分区表中，数据通常存储在不同的目录中，分区列值编码在每个分区目录的路径中。现在，Parquet数据源能够自动发现和推断分区信息。例如，我们可以使用以下目录结构将之前使用的所有填充数据存储到一个分区表中，该目录结构具有两个额外的列<code>gender</code>并<code>country</code>作为分区列：</p>
<pre><code class="hljs routeros">path
└── <span class="hljs-keyword">to</span>
    └── table
        ├── <span class="hljs-attribute">gender</span>=male
        │   ├── <span class="hljs-built_in">..</span>.
        │   │
        │   ├── <span class="hljs-attribute">country</span>=US
        │   │   └── data.parquet
        │   ├── <span class="hljs-attribute">country</span>=CN
        │   │   └── data.parquet
        │   └── <span class="hljs-built_in">..</span>.
        └── <span class="hljs-attribute">gender</span>=female
            ├── <span class="hljs-built_in">..</span>.
            │
            ├── <span class="hljs-attribute">country</span>=US
            │   └── data.parquet
            ├── <span class="hljs-attribute">country</span>=CN
            │   └── data.parquet
            └── <span class="hljs-built_in">..</span>.</code></pre>

<p>By passing <code>path/to/table</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, Spark SQL will automatically extract the partitioning information from the paths. Now the schema of the returned DataFrame becomes:</p>
<p>通过传递<code>path/to/table</code>给<code>SparkSession.read.parquet</code>或<code>SparkSession.read.load</code>，Spark SQL将自动从路径中提取分区信息。现在，返回的DataFrame的架构变为：</p>
<pre><code class="hljs yaml"><span class="hljs-string">root</span>
<span class="hljs-string">|--</span> <span class="hljs-attr">name:</span> <span class="hljs-string">string</span> <span class="hljs-string">(nullable</span> <span class="hljs-string">=</span> <span class="hljs-literal">true</span><span class="hljs-string">)</span>
<span class="hljs-string">|--</span> <span class="hljs-attr">age:</span> <span class="hljs-string">long</span> <span class="hljs-string">(nullable</span> <span class="hljs-string">=</span> <span class="hljs-literal">true</span><span class="hljs-string">)</span>
<span class="hljs-string">|--</span> <span class="hljs-attr">gender:</span> <span class="hljs-string">string</span> <span class="hljs-string">(nullable</span> <span class="hljs-string">=</span> <span class="hljs-literal">true</span><span class="hljs-string">)</span>
<span class="hljs-string">|--</span> <span class="hljs-attr">country:</span> <span class="hljs-string">string</span> <span class="hljs-string">(nullable</span> <span class="hljs-string">=</span> <span class="hljs-literal">true</span><span class="hljs-string">)</span></code></pre>

<p>Notice that the data types of the partitioning columns are automatically inferred. Currently, numeric data types and string type are supported. Sometimes users may not want to automatically infer the data types of the partitioning columns. For these use cases, the automatic type inference can be configured by <code>spark.sql.sources.partitionColumnTypeInference.enabled</code>, which is default to <code>true</code>. When type inference is disabled, string type will be used for the partitioning columns.</p>
<p>Starting from Spark 1.6.0, partition discovery only finds partitions under the given paths by default. For the above example, if users pass <code>path/to/table/gender=male</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, <code>gender</code> will not be considered as a partitioning column. If users need to specify the base path that partition discovery should start with, they can set <code>basePath</code> in the data source options. For example, when <code>path/to/table/gender=male</code> is the path of the data and users set <code>basePath</code> to <code>path/to/table/</code>, <code>gender</code> will be a partitioning column.</p>
<p>请注意，分区列的数据类型是自动推断的。当前，支持数字数据类型和字符串类型。有时用户可能不希望自动推断分区列的数据类型。对于这些用例，可以使用来配置自动类型推断<code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认为 <code>true</code>。禁用类型推断时，字符串类型将用于分区列。</p>
<p>从Spark 1.6.0开始，默认情况下，分区发现仅在给定路径下查找分区。对于上面的示例，如果用户传递<code>path/to/table/gender=male</code>给 <code>SparkSession.read.parquet</code>或<code>SparkSession.read.load</code>，<code>gender</code>则不会被视为分区列。如果用户需要指定分区发现应开始的基本路径，则可以<code>basePath</code>在数据源选项中进行设置。例如，当<code>path/to/table/gender=male</code>数据路径是且用户设置<code>basePath</code>为时<code>path/to/table/</code>，<code>gender</code>将是一个分区列。</p>
<h4 id="3-2-3-Schema-Merging"><a href="#3-2-3-Schema-Merging" class="headerlink" title="3.2.3 Schema Merging"></a>3.2.3 Schema Merging</h4><p>Like ProtocolBuffer, Avro, and Thrift, Parquet also supports schema evolution. Users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible schemas. The Parquet data source is now able to automatically detect this case and merge schemas of all these files.</p>
<p>Since schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0. You may enable it by</p>
<ol>
<li>setting data source option <code>mergeSchema</code> to <code>true</code> when reading Parquet files (as shown in the examples below), or</li>
<li>setting the global SQL option <code>spark.sql.parquet.mergeSchema</code> to <code>true</code>.</li>
</ol>
<p>像ProtocolBuffer，Avro和Thrift一样，Parquet也支持架构演变。用户可以从简单的架构开始，然后根据需要逐渐向架构中添加更多列。这样，用户可能最终得到具有不同但相互兼容的架构的多个Parquet文件。现在，Parquet数据源能够自动检测到这种情况并合并所有这些文件的模式。</p>
<p>由于模式合并是一项相对昂贵的操作，并且在大多数情况下不是必需的，因此默认情况下，我们从1.5.0开始将其关闭。您可以通过以下方式启用它</p>
<ol>
<li>将数据源选项设置<code>mergeSchema</code>为<code>true</code>在读取Parquet文件时（如下例所示），或者</li>
<li>将全局SQL选项设置<code>spark.sql.parquet.mergeSchema</code>为<code>true</code>。</li>
</ol>
<pre><code class="hljs scala"><span class="hljs-comment">// This is used to implicitly convert an RDD to a DataFrame.</span>
<span class="hljs-keyword">import</span> spark.implicits._

<span class="hljs-comment">// Create a simple DataFrame, store into a partition directory</span>
<span class="hljs-keyword">val</span> squaresDF = spark.sparkContext.makeRDD(<span class="hljs-number">1</span> to <span class="hljs-number">5</span>).map(i =&gt; (i, i * i)).toDF(<span class="hljs-string">"value"</span>, <span class="hljs-string">"square"</span>)
squaresDF.write.parquet(<span class="hljs-string">"data/test_table/key=1"</span>)

<span class="hljs-comment">// Create another DataFrame in a new partition directory,</span>
<span class="hljs-comment">// adding a new column and dropping an existing column</span>
<span class="hljs-keyword">val</span> cubesDF = spark.sparkContext.makeRDD(<span class="hljs-number">6</span> to <span class="hljs-number">10</span>).map(i =&gt; (i, i * i * i)).toDF(<span class="hljs-string">"value"</span>, <span class="hljs-string">"cube"</span>)
cubesDF.write.parquet(<span class="hljs-string">"data/test_table/key=2"</span>)

<span class="hljs-comment">// Read the partitioned table</span>
<span class="hljs-keyword">val</span> mergedDF = spark.read.option(<span class="hljs-string">"mergeSchema"</span>, <span class="hljs-string">"true"</span>).parquet(<span class="hljs-string">"data/test_table"</span>)
mergedDF.printSchema()

<span class="hljs-comment">// The final schema consists of all 3 columns in the Parquet files together</span>
<span class="hljs-comment">// with the partitioning column appeared in the partition directory paths</span>
<span class="hljs-comment">// root</span>
<span class="hljs-comment">//  |-- value: int (nullable = true)</span>
<span class="hljs-comment">//  |-- square: int (nullable = true)</span>
<span class="hljs-comment">//  |-- cube: int (nullable = true)</span>
<span class="hljs-comment">//  |-- key: int (nullable = true)</span></code></pre>

<h4 id="3-2-4-Hive-metastore-Parquet-table-conversion"><a href="#3-2-4-Hive-metastore-Parquet-table-conversion" class="headerlink" title="3.2.4 Hive metastore Parquet table conversion"></a>3.2.4 Hive metastore Parquet table conversion</h4><p>When reading from and writing to Hive metastore Parquet tables, Spark SQL will try to use its own Parquet support instead of Hive SerDe for better performance. This behavior is controlled by the <code>spark.sql.hive.convertMetastoreParquet</code> configuration, and is turned on by default.</p>
<p>在读取和写入Hive metastore Parquet表时，Spark SQL将尝试使用其自己的Parquet支持而不是Hive SerDe以获得更好的性能。此行为由<code>spark.sql.hive.convertMetastoreParquet</code>配置控制 ，并且默认情况下处于启用状态。</p>
<h5 id="3-2-4-1-Hive-Parquet-Schema-Reconciliation"><a href="#3-2-4-1-Hive-Parquet-Schema-Reconciliation" class="headerlink" title="3.2.4.1 Hive/Parquet Schema Reconciliation"></a>3.2.4.1 Hive/Parquet Schema Reconciliation</h5><p>There are two key differences between Hive and Parquet from the perspective of table schema processing.</p>
<ol>
<li>Hive is case insensitive, while Parquet is not</li>
<li>Hive considers all columns nullable, while nullability in Parquet is significant</li>
</ol>
<p>Due to this reason, we must reconcile Hive metastore schema with Parquet schema when converting a Hive metastore Parquet table to a Spark SQL Parquet table. The reconciliation rules are:</p>
<ol>
<li>Fields that have the same name in both schema must have the same data type regardless of nullability. The reconciled field should have the data type of the Parquet side, so that nullability is respected.</li>
<li>The reconciled schema contains exactly those fields defined in Hive metastore schema.<ul>
<li>Any fields that only appear in the Parquet schema are dropped in the reconciled schema.</li>
<li>Any fields that only appear in the Hive metastore schema are added as nullable field in the reconciled schema.</li>
</ul>
</li>
</ol>
<p>从表模式处理的角度来看，Hive和Parquet之间有两个关键区别。</p>
<ol>
<li>Hive不区分大小写，而Parquet区分</li>
<li>Hive认为所有列都可为空，而Parquet中的可为空性很重要</li>
</ol>
<p>由于这个原因，在将Hive Metastore Parquet表转换为Spark SQL Parquet表时，我们必须使Hive Metastore模式与Parquet模式一致。对帐规则为：</p>
<ol>
<li>在两个模式中具有相同名称的字段必须具有相同的数据类型，而不考虑可为空性。协调字段应具有Parquet端的数据类型，以便遵守可空性。</li>
<li>协调后的架构完全包含在Hive Metastore架构中定义的那些字段。<ul>
<li>仅出现在Parquet模式中的所有字段都将被放入对帐模式中。</li>
<li>仅在Hive Metastore模式中出现的所有字段都将添加为已对帐模式中的可为空字段。</li>
</ul>
</li>
</ol>
<h5 id="3-2-4-2-Metadata-Refreshing"><a href="#3-2-4-2-Metadata-Refreshing" class="headerlink" title="3.2.4.2 Metadata Refreshing"></a>3.2.4.2 Metadata Refreshing</h5><p>Spark SQL caches Parquet metadata for better performance. When Hive metastore Parquet table conversion is enabled, metadata of those converted tables are also cached. If these tables are updated by Hive or other external tools, you need to refresh them manually to ensure consistent metadata.</p>
<p>Spark SQL缓存Parquet元数据以获得更好的性能。启用Hive Metastore Parquet表转换后，这些转换表的元数据也会被缓存。如果这些表是通过Hive或其他外部工具更新的，则需要手动刷新它们以确保元数据一致。</p>
<pre><code class="hljs scala"><span class="hljs-comment">// spark is an existing SparkSession</span>
spark.catalog.refreshTable(<span class="hljs-string">"my_table"</span>)</code></pre>

<h4 id="3-2-5-Configuration"><a href="#3-2-5-Configuration" class="headerlink" title="3.2.5 Configuration"></a>3.2.5 Configuration</h4><p>Configuration of Parquet can be done using the <code>setConf</code> method on <code>SparkSession</code> or by running <code>SET key=value</code> commands using SQL.</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.sql.parquet.binaryAsString</code></td>
<td align="left">false</td>
<td align="left">Some other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.parquet.int96AsTimestamp</code></td>
<td align="left">true</td>
<td align="left">Some Parquet-producing systems, in particular Impala and Hive, store Timestamp into INT96. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.parquet.cacheMetadata</code></td>
<td align="left">true</td>
<td align="left">Turns on caching of Parquet schema metadata. Can speed up querying of static data.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.parquet.compression.codec</code></td>
<td align="left">snappy</td>
<td align="left">Sets the compression codec use when writing Parquet files. Acceptable values include: uncompressed, snappy, gzip, lzo.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.parquet.filterPushdown</code></td>
<td align="left">true</td>
<td align="left">Enables Parquet filter push-down optimization when set to true.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.hive.convertMetastoreParquet</code></td>
<td align="left">true</td>
<td align="left">When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in support.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.parquet.mergeSchema</code></td>
<td align="left">false</td>
<td align="left">When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.optimizer.metadataOnly</code></td>
<td align="left">true</td>
<td align="left">When true, enable the metadata-only query optimization that use the table’s metadata to produce the partition columns instead of table scans. It applies when all the columns scanned are partition columns and the query has an aggregate operator that satisfies distinct semantics.</td>
</tr>
</tbody></table>
<h3 id="3-3-JSON-Datasets"><a href="#3-3-JSON-Datasets" class="headerlink" title="3.3 JSON Datasets"></a>3.3 JSON Datasets</h3><p>Spark SQL can automatically infer the schema of a JSON dataset and load it as a <code>Dataset[Row]</code>. This conversion can be done using <code>SparkSession.read.json()</code> on either an RDD of String, or a JSON file.</p>
<p>Note that the file that is offered as <em>a json file</em> is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. For more information, please see <a href="http://jsonlines.org/" target="_blank" rel="noopener">JSON Lines text format, also called newline-delimited JSON</a>. As a consequence, a regular multi-line JSON file will most often fail.</p>
<p>Spark SQL可以自动推断JSON数据集的架构并将其作为加载<code>Dataset[Row]</code>。可以使用<code>SparkSession.read.json()</code>字符串的RDD或JSON文件来完成此转换。</p>
<p>请注意，<em>以json文件</em>形式提供<em>的文件</em>不是典型的JSON文件。每行必须包含一个单独的，自包含的有效JSON对象。有关更多信息，请参见 <a href="http://jsonlines.org/" target="_blank" rel="noopener">JSON Lines文本格式，也称为newline分隔的JSON</a>。因此，常规的多行JSON文件通常会失败。</p>
<pre><code class="hljs scala"><span class="hljs-comment">// A JSON dataset is pointed to by path.</span>
<span class="hljs-comment">// The path can be either a single text file or a directory storing text files</span>
<span class="hljs-keyword">val</span> path = <span class="hljs-string">"examples/src/main/resources/people.json"</span>
<span class="hljs-keyword">val</span> peopleDF = spark.read.json(path)

<span class="hljs-comment">// The inferred schema can be visualized using the printSchema() method</span>
peopleDF.printSchema()
<span class="hljs-comment">// root</span>
<span class="hljs-comment">//  |-- age: long (nullable = true)</span>
<span class="hljs-comment">//  |-- name: string (nullable = true)</span>

<span class="hljs-comment">// Creates a temporary view using the DataFrame</span>
peopleDF.createOrReplaceTempView(<span class="hljs-string">"people"</span>)

<span class="hljs-comment">// SQL statements can be run by using the sql methods provided by spark</span>
<span class="hljs-keyword">val</span> teenagerNamesDF = spark.sql(<span class="hljs-string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>)
teenagerNamesDF.show()
<span class="hljs-comment">// +------+</span>
<span class="hljs-comment">// |  name|</span>
<span class="hljs-comment">// +------+</span>
<span class="hljs-comment">// |Justin|</span>
<span class="hljs-comment">// +------+</span>

<span class="hljs-comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="hljs-comment">// an RDD[String] storing one JSON object per string</span>
<span class="hljs-keyword">val</span> otherPeopleRDD = spark.sparkContext.makeRDD(
  <span class="hljs-string">""</span><span class="hljs-string">"&#123;"</span><span class="hljs-string">name":"</span><span class="hljs-type">Yin</span><span class="hljs-string">","</span><span class="hljs-string">address":&#123;"</span><span class="hljs-string">city":"</span><span class="hljs-type">Columbus</span><span class="hljs-string">","</span><span class="hljs-string">state":"</span><span class="hljs-type">Ohio</span><span class="hljs-string">"&#125;&#125;"</span><span class="hljs-string">""</span> :: <span class="hljs-type">Nil</span>)
<span class="hljs-keyword">val</span> otherPeople = spark.read.json(otherPeopleRDD)
otherPeople.show()
<span class="hljs-comment">// +---------------+----+</span>
<span class="hljs-comment">// |        address|name|</span>
<span class="hljs-comment">// +---------------+----+</span>
<span class="hljs-comment">// |[Columbus,Ohio]| Yin|</span>
<span class="hljs-comment">// +---------------+----+</span></code></pre>

<h3 id="3-4-Hive-Tables"><a href="#3-4-Hive-Tables" class="headerlink" title="3.4 Hive Tables"></a>3.4 Hive Tables</h3><p>Spark SQL also supports reading and writing data stored in <a href="http://hive.apache.org/" target="_blank" rel="noopener">Apache Hive</a>. However, since Hive has a large number of dependencies, these dependencies are not included in the default Spark distribution. If Hive dependencies can be found on the classpath, Spark will load them automatically. Note that these Hive dependencies must also be present on all of the worker nodes, as they will need access to the Hive serialization and deserialization libraries (SerDes) in order to access data stored in Hive.</p>
<p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> (for security configuration), and <code>hdfs-site.xml</code> (for HDFS configuration) file in <code>conf/</code>.</p>
<p>When working with Hive, one must instantiate <code>SparkSession</code> with Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions. Users who do not have an existing Hive deployment can still enable Hive support. When not configured by the <code>hive-site.xml</code>, the context automatically creates <code>metastore_db</code> in the current directory and creates a directory configured by <code>spark.sql.warehouse.dir</code>, which defaults to the directory <code>spark-warehouse</code> in the current directory that the Spark application is started. Note that the <code>hive.metastore.warehouse.dir</code> property in <code>hive-site.xml</code> is deprecated since Spark 2.0.0. Instead, use <code>spark.sql.warehouse.dir</code> to specify the default location of database in warehouse. You may need to grant write privilege to the user who starts the Spark application.</p>
<p>Spark SQL还支持读写存储在<a href="http://hive.apache.org/" target="_blank" rel="noopener">Apache Hive中的</a>数据。但是，由于Hive具有大量依赖关系，因此默认的Spark分发中不包含这些依赖关系。如果可以在类路径上找到Hive依赖项，Spark将自动加载它们。请注意，这些Hive依赖项也必须存在于所有工作节点上，因为它们将需要访问Hive序列化和反序列化库（SerDes）才能访问存储在Hive中的数据。</p>
<p>通过将<code>hive-site.xml</code>，<code>core-site.xml</code>（对于安全性配置）和<code>hdfs-site.xml</code>（对于HDFS配置）文件放置在中来配置Hive <code>conf/</code>。</p>
<p>使用Hive时，必须实例化<code>SparkSession</code>Hive支持，包括与永久性Hive元存储库的连接，对Hive Serdes的支持以及Hive用户定义的功能。没有现有Hive部署的用户仍可以启用Hive支持。如果未由配置<code>hive-site.xml</code>，则上下文会自动<code>metastore_db</code>在当前目录中创建并创建一个由配置<code>spark.sql.warehouse.dir</code>的目录<code>spark-warehouse</code>，该目录默认 为启动Spark应用程序的当前目录中的目录。请注意，自Spark 2.0.0起不推荐使用<code>hive.metastore.warehouse.dir</code>in 的属性<code>hive-site.xml</code>。而是使用<code>spark.sql.warehouse.dir</code>指定仓库中数据库的默认位置。您可能需要向启动Spark应用程序的用户授予写权限。</p>
<pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Row</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span>

<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Record</span>(<span class="hljs-params">key: <span class="hljs-type">Int</span>, value: <span class="hljs-type">String</span></span>)</span>
<span class="hljs-class"></span>
<span class="hljs-class"><span class="hljs-title">//</span> <span class="hljs-title">warehouseLocation</span> <span class="hljs-title">points</span> <span class="hljs-title">to</span> <span class="hljs-title">the</span> <span class="hljs-title">default</span> <span class="hljs-title">location</span> <span class="hljs-title">for</span> <span class="hljs-title">managed</span> <span class="hljs-title">databases</span> <span class="hljs-title">and</span> <span class="hljs-title">tables</span></span>
<span class="hljs-class"><span class="hljs-title">val</span> <span class="hljs-title">warehouseLocation</span> </span>= <span class="hljs-string">"spark-warehouse"</span>

<span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>
  .builder()
  .appName(<span class="hljs-string">"Spark Hive Example"</span>)
  .config(<span class="hljs-string">"spark.sql.warehouse.dir"</span>, warehouseLocation)
  .enableHiveSupport()
  .getOrCreate()

<span class="hljs-keyword">import</span> spark.implicits._
<span class="hljs-keyword">import</span> spark.sql

sql(<span class="hljs-string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)
sql(<span class="hljs-string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)

<span class="hljs-comment">// Queries are expressed in HiveQL</span>
sql(<span class="hljs-string">"SELECT * FROM src"</span>).show()
<span class="hljs-comment">// +---+-------+</span>
<span class="hljs-comment">// |key|  value|</span>
<span class="hljs-comment">// +---+-------+</span>
<span class="hljs-comment">// |238|val_238|</span>
<span class="hljs-comment">// | 86| val_86|</span>
<span class="hljs-comment">// |311|val_311|</span>
<span class="hljs-comment">// ...</span>

<span class="hljs-comment">// Aggregation queries are also supported.</span>
sql(<span class="hljs-string">"SELECT COUNT(*) FROM src"</span>).show()
<span class="hljs-comment">// +--------+</span>
<span class="hljs-comment">// |count(1)|</span>
<span class="hljs-comment">// +--------+</span>
<span class="hljs-comment">// |    500 |</span>
<span class="hljs-comment">// +--------+</span>

<span class="hljs-comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span>
<span class="hljs-keyword">val</span> sqlDF = sql(<span class="hljs-string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>)

<span class="hljs-comment">// The items in DaraFrames are of type Row, which allows you to access each column by ordinal.</span>
<span class="hljs-keyword">val</span> stringsDS = sqlDF.map &#123;
  <span class="hljs-keyword">case</span> <span class="hljs-type">Row</span>(key: <span class="hljs-type">Int</span>, value: <span class="hljs-type">String</span>) =&gt; <span class="hljs-string">s"Key: <span class="hljs-subst">$key</span>, Value: <span class="hljs-subst">$value</span>"</span>
&#125;
stringsDS.show()
<span class="hljs-comment">// +--------------------+</span>
<span class="hljs-comment">// |               value|</span>
<span class="hljs-comment">// +--------------------+</span>
<span class="hljs-comment">// |Key: 0, Value: val_0|</span>
<span class="hljs-comment">// |Key: 0, Value: val_0|</span>
<span class="hljs-comment">// |Key: 0, Value: val_0|</span>
<span class="hljs-comment">// ...</span>

<span class="hljs-comment">// You can also use DataFrames to create temporary views within a SparkSession.</span>
<span class="hljs-keyword">val</span> recordsDF = spark.createDataFrame((<span class="hljs-number">1</span> to <span class="hljs-number">100</span>).map(i =&gt; <span class="hljs-type">Record</span>(i, <span class="hljs-string">s"val_<span class="hljs-subst">$i</span>"</span>)))
recordsDF.createOrReplaceTempView(<span class="hljs-string">"records"</span>)

<span class="hljs-comment">// Queries can then join DataFrame data with data stored in Hive.</span>
sql(<span class="hljs-string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show()
<span class="hljs-comment">// +---+------+---+------+</span>
<span class="hljs-comment">// |key| value|key| value|</span>
<span class="hljs-comment">// +---+------+---+------+</span>
<span class="hljs-comment">// |  2| val_2|  2| val_2|</span>
<span class="hljs-comment">// |  4| val_4|  4| val_4|</span>
<span class="hljs-comment">// |  5| val_5|  5| val_5|</span>
<span class="hljs-comment">// ...</span></code></pre>

<h4 id="3-4-1-Interacting-with-Different-Versions-of-Hive-Metastore"><a href="#3-4-1-Interacting-with-Different-Versions-of-Hive-Metastore" class="headerlink" title="3.4.1 Interacting with Different Versions of Hive Metastore"></a>3.4.1 Interacting with Different Versions of Hive Metastore</h4><p>One of the most important pieces of Spark SQL’s Hive support is interaction with Hive metastore, which enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary build of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below. Note that independent of the version of Hive that is being used to talk to the metastore, internally Spark SQL will compile against Hive 1.2.1 and use those classes for internal execution (serdes, UDFs, UDAFs, etc).</p>
<p>与Hive metastore的交互是Spark SQL对Hive的最重要支持之一，它使Spark SQL能够访问Hive表的元数据。从Spark 1.4.0开始，使用以下描述的配置，可以使用Spark SQL的单个二进制版本来查询Hive元存储的不同版本。请注意，与用于与metastore进行通信的Hive版本无关，Spark SQL在内部将针对Hive 1.2.1进行编译，并将这些类用于内部执行（serdes，UDF，UDAF等）。</p>
<p>The following options can be used to configure the version of Hive that is used to retrieve metadata:</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.sql.hive.metastore.version</code></td>
<td align="left"><code>1.2.1</code></td>
<td align="left">Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>1.2.1</code>.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.hive.metastore.jars</code></td>
<td align="left"><code>builtin</code></td>
<td align="left">Location of the jars that should be used to instantiate the HiveMetastoreClient. This property can be one of three options:<code>builtin</code>Use Hive 1.2.1, which is bundled with the Spark assembly when <code>-Phive</code> is enabled. When this option is chosen, <code>spark.sql.hive.metastore.version</code> must be either <code>1.2.1</code> or not defined.<code>maven</code>Use Hive jars of specified version downloaded from Maven repositories. This configuration is not generally recommended for production deployments.A classpath in the standard format for the JVM. This classpath must include all of Hive and its dependencies, including the correct version of Hadoop. These jars only need to be present on the driver, but if you are running in yarn cluster mode then you must ensure they are packaged with your application.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.hive.metastore.sharedPrefixes</code></td>
<td align="left"><code>com.mysql.jdbc,org.postgresql</code>,<code>com.microsoft.sqlserver,oracle.jdbc</code></td>
<td align="left">A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.hive.metastore.barrierPrefixes</code></td>
<td align="left"><code>(empty)</code></td>
<td align="left">A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).</td>
</tr>
</tbody></table>
<h3 id="3-5-JDBC-To-Other-Databases"><a href="#3-5-JDBC-To-Other-Databases" class="headerlink" title="3.5 JDBC To Other Databases"></a>3.5 JDBC To Other Databases</h3><p>Spark SQL also includes a data source that can read data from other databases using JDBC. This functionality should be preferred over using <a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" target="_blank" rel="noopener">JdbcRDD</a>. This is because the results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources. The JDBC data source is also easier to use from Java or Python as it does not require the user to provide a ClassTag. (Note that this is different than the Spark SQL JDBC server, which allows other applications to run queries using Spark SQL).</p>
<p>To get started you will need to include the JDBC driver for you particular database on the spark classpath. For example, to connect to postgres from the Spark Shell you would run the following command:</p>
<p>Spark SQL还包括一个可以使用JDBC从其他数据库读取数据的数据源。与使用<a href="http://spark.apache.org/docs/2.1.2/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" target="_blank" rel="noopener">JdbcRDD相比，</a>应优先使用此功能。这是因为结果以DataFrame的形式返回，并且可以轻松地在Spark SQL中进行处理或与其他数据源合并。JDBC数据源也更易于从Java或Python使用，因为它不需要用户提供ClassTag。（请注意，这与Spark SQL JDBC服务器不同，后者允许其他应用程序使用Spark SQL运行查询）。</p>
<p>首先，您需要在spark类路径上包含特定数据库的JDBC驱动程序。例如，要从Spark Shell连接到postgres，您可以运行以下命令：</p>
<pre><code class="hljs lsl">bin/spark-shell --driver-class-path postgresql<span class="hljs-number">-9.4</span><span class="hljs-number">.1207</span>.jar --jars postgresql<span class="hljs-number">-9.4</span><span class="hljs-number">.1207</span>.jar</code></pre>

<p>Tables from the remote database can be loaded as a DataFrame or Spark SQL temporary view using the Data Sources API. Users can specify the JDBC connection properties in the data source options. <code>user</code> and <code>password</code> are normally provided as connection properties for logging into the data sources. In addition to the connection properties, Spark also supports the following case-insensitive options:</p>
<p>可以使用Data Sources API将远程数据库中的表作为DataFrame或Spark SQL临时视图加载。用户可以在数据源选项中指定JDBC连接属性。 <code>user</code>和<code>password</code>通常用于登录到数据源提供为连接属性。除连接属性外，Spark还支持以下不区分大小写的选项：</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>url</code></td>
<td align="left">The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., <code>jdbc:postgresql://localhost/test?user=fred&amp;password=secret</code></td>
</tr>
<tr>
<td align="left"><code>dbtable</code></td>
<td align="left">The JDBC table that should be read. Note that anything that is valid in a <code>FROM</code> clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.</td>
</tr>
<tr>
<td align="left"><code>driver</code></td>
<td align="left">The class name of the JDBC driver to use to connect to this URL.</td>
</tr>
<tr>
<td align="left"><code>partitionColumn, lowerBound, upperBound, numPartitions</code></td>
<td align="left">These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. <code>partitionColumn</code> must be a numeric column from the table in question. Notice that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.</td>
</tr>
<tr>
<td align="left"><code>fetchsize</code></td>
<td align="left">The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading.</td>
</tr>
<tr>
<td align="left"><code>batchsize</code></td>
<td align="left">The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to <code>1000</code>.</td>
</tr>
<tr>
<td align="left"><code>isolationLevel</code></td>
<td align="left">The transaction isolation level, which applies to current connection. It can be one of <code>NONE</code>, <code>READ_COMMITTED</code>, <code>READ_UNCOMMITTED</code>, <code>REPEATABLE_READ</code>, or <code>SERIALIZABLE</code>, corresponding to standard transaction isolation levels defined by JDBC’s Connection object, with default of <code>READ_UNCOMMITTED</code>. This option applies only to writing. Please refer the documentation in <code>java.sql.Connection</code>.</td>
</tr>
<tr>
<td align="left"><code>truncate</code></td>
<td align="left">This is a JDBC writer related option. When <code>SaveMode.Overwrite</code> is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to <code>false</code>. This option applies only to writing.</td>
</tr>
<tr>
<td align="left"><code>createTableOptions</code></td>
<td align="left">This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., <code>CREATE TABLE t (name string) ENGINE=InnoDB.</code>). This option applies only to writing.</td>
</tr>
</tbody></table>
<pre><code class="hljs scala"><span class="hljs-comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span>
<span class="hljs-comment">// Loading data from a JDBC source</span>
<span class="hljs-keyword">val</span> jdbcDF = spark.read
  .format(<span class="hljs-string">"jdbc"</span>)
  .option(<span class="hljs-string">"url"</span>, <span class="hljs-string">"jdbc:postgresql:dbserver"</span>)
  .option(<span class="hljs-string">"dbtable"</span>, <span class="hljs-string">"schema.tablename"</span>)
  .option(<span class="hljs-string">"user"</span>, <span class="hljs-string">"username"</span>)
  .option(<span class="hljs-string">"password"</span>, <span class="hljs-string">"password"</span>)
  .load()

<span class="hljs-keyword">val</span> connectionProperties = <span class="hljs-keyword">new</span> <span class="hljs-type">Properties</span>()
connectionProperties.put(<span class="hljs-string">"user"</span>, <span class="hljs-string">"username"</span>)
connectionProperties.put(<span class="hljs-string">"password"</span>, <span class="hljs-string">"password"</span>)
<span class="hljs-keyword">val</span> jdbcDF2 = spark.read
  .jdbc(<span class="hljs-string">"jdbc:postgresql:dbserver"</span>, <span class="hljs-string">"schema.tablename"</span>, connectionProperties)

<span class="hljs-comment">// Saving data to a JDBC source</span>
jdbcDF.write
  .format(<span class="hljs-string">"jdbc"</span>)
  .option(<span class="hljs-string">"url"</span>, <span class="hljs-string">"jdbc:postgresql:dbserver"</span>)
  .option(<span class="hljs-string">"dbtable"</span>, <span class="hljs-string">"schema.tablename"</span>)
  .option(<span class="hljs-string">"user"</span>, <span class="hljs-string">"username"</span>)
  .option(<span class="hljs-string">"password"</span>, <span class="hljs-string">"password"</span>)
  .save()

jdbcDF2.write
  .jdbc(<span class="hljs-string">"jdbc:postgresql:dbserver"</span>, <span class="hljs-string">"schema.tablename"</span>, connectionProperties)</code></pre>

<h3 id="3-6-Troubleshooting"><a href="#3-6-Troubleshooting" class="headerlink" title="3.6 Troubleshooting"></a>3.6 Troubleshooting</h3><ul>
<li><p>The JDBC driver class must be visible to the primordial class loader on the client session and on all executors. This is because Java’s DriverManager class does a security check that results in it ignoring all drivers not visible to the primordial class loader when one goes to open a connection. One convenient way to do this is to modify compute_classpath.sh on all worker nodes to include your driver JARs.</p>
</li>
<li><p>Some databases, such as H2, convert all names to upper case. You’ll need to use upper case to refer to those names in Spark SQL.</p>
</li>
<li><p>JDBC驱动程序类必须对客户机会话和所有执行程序上的原始类加载器可见。这是因为Java的DriverManager类进行了安全检查，导致它忽略了当打开连接时原始类加载器不可见的所有驱动程序。一种方便的方法是修改所有工作程序节点上的compute_classpath.sh以包括您的驱动程序JAR。</p>
</li>
<li><p>某些数据库（例如H2）会将所有名称都转换为大写。您需要使用大写字母在Spark SQL中引用这些名称。</p>
</li>
</ul>
<h2 id="4-Performance-Tuning"><a href="#4-Performance-Tuning" class="headerlink" title="4. Performance Tuning"></a>4. Performance Tuning</h2><p>For some workloads it is possible to improve performance by either caching data in memory, or by turning on some experimental options.</p>
<p>对于某些工作负载，可以通过在内存中缓存数据或打开某些实验选项来提高性能。</p>
<h3 id="4-1-Caching-Data-In-Memory"><a href="#4-1-Caching-Data-In-Memory" class="headerlink" title="4.1 Caching Data In Memory"></a>4.1 Caching Data In Memory</h3><p>Spark SQL can cache tables using an in-memory columnar format by calling <code>spark.catalog.cacheTable(&quot;tableName&quot;)</code> or <code>dataFrame.cache()</code>. Then Spark SQL will scan only required columns and will automatically tune compression to minimize memory usage and GC pressure. You can call <code>spark.catalog.uncacheTable(&quot;tableName&quot;)</code> to remove the table from memory.</p>
<p>Configuration of in-memory caching can be done using the <code>setConf</code> method on <code>SparkSession</code> or by running <code>SET key=value</code> commands using SQL.</p>
<p>Spark SQL可以通过调用<code>spark.catalog.cacheTable(&quot;tableName&quot;)</code>或使用内存列式格式缓存表<code>dataFrame.cache()</code>。然后，Spark SQL将仅扫描所需的列，并将自动调整压缩以最大程度地减少内存使用和GC压力。您可以调用<code>spark.catalog.uncacheTable(&quot;tableName&quot;)</code>从内存中删除表。</p>
<p>可以使用<code>setConf</code>on上的方法<code>SparkSession</code>或<code>SET key=value</code>使用SQL 运行 命令来完成内存中缓存的配置。</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.sql.inMemoryColumnarStorage.compressed</code></td>
<td align="left">true</td>
<td align="left">When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.inMemoryColumnarStorage.batchSize</code></td>
<td align="left">10000</td>
<td align="left">Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</td>
</tr>
</tbody></table>
<h3 id="4-2-Other-Configuration-Options"><a href="#4-2-Other-Configuration-Options" class="headerlink" title="4.2 Other Configuration Options"></a>4.2 Other Configuration Options</h3><p>The following options can also be used to tune the performance of query execution. It is possible that these options will be deprecated in future release as more optimizations are performed automatically.</p>
<p>以下选项也可以用于调整查询执行的性能。随着自动执行更多优化，这些选项可能会在将来的版本中被弃用。</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.sql.files.maxPartitionBytes</code></td>
<td align="left">134217728 (128 MB)</td>
<td align="left">The maximum number of bytes to pack into a single partition when reading files.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.files.openCostInBytes</code></td>
<td align="left">4194304 (4 MB)</td>
<td align="left">The estimated cost to open a file, measured by the number of bytes could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over estimated, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</td>
</tr>
<tr>
<td align="left"><code>spark.sql.broadcastTimeout</code></td>
<td align="left">300</td>
<td align="left">Timeout in seconds for the broadcast wait time in broadcast joins</td>
</tr>
<tr>
<td align="left"><code>spark.sql.autoBroadcastJoinThreshold</code></td>
<td align="left">10485760 (10 MB)</td>
<td align="left">Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code> has been run.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.shuffle.partitions</code></td>
<td align="left">200</td>
<td align="left">Configures the number of partitions to use when shuffling data for joins or aggregations.</td>
</tr>
</tbody></table>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Spark/">Spark</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Spark/">Spark</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2020/07/23/Tuning-Spark/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Spark系列（五）—— Tuning Spark</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/07/20/Spark-Streaming%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/">
                        <span class="hidden-mobile">Spark系列（三）-- Spark Streaming编程指南</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>





  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>




















</body>
</html>
